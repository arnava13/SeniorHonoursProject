2024-04-08 20:16:09.017368: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-08 20:16:09.017420: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-08 20:16:09.018592: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-08 20:16:10.021841: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-04-08 20:16:12.514975: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
using 1D layers and 4 channels
/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
loc = add_variable_fn(
/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
untransformed_scale = add_variable_fn(
Expected output dimension after layer: conv1d_flipout : 97
Expected output dimension after layer: conv1d_flipout_1 : 46
Expected output dimension after layer: conv1d_flipout_2 : 45
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1712608415.644275    4464 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
W0000 00:00:1712608431.377655    4464 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update
W0000 00:00:1712608446.419815    4466 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update
Creating directory models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC
-------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune False
one_vs_all False
c_0 ['lcdm']
c_1 ['dgp', 'ds', 'fR', 'rand', 'wcdm']
dataset_balanced False
include_last False
log_path models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC_log.txt
restore False
fname sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC
model_name custom
my_path None
DIR data/train
TEST_DIR data/test/
models_dir models/
save_ckpt True
out_path_overwrite False
curves_folder data/curve_files_sys/theory_error
save_processed_spectra False
cache_dir False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
norm_data_name /planck_ee2.txt
normalization stdcosmo
sample_pace 1
k_max 2.5
k_min 0.0
i_max None
i_min None
add_noise True
n_noisy_samples 10
add_shot False
add_sys True
add_cosvar True
sigma_sys None
sys_scaled None
sys_factor None
sys_max None
sigma_curves 0.05
sigma_curves_default 0.05
rescale_curves uniform
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable False
unfreeze False
lr 0.01
drop 0.5
n_epochs 100
val_size 0.15
test_size 0.0
batch_size 3000
patience 20
GPU True
TPU False
decay 0.95
BatchNorm True
padding valid
shuffle True
------------ CREATING DATA GENERATORS ------------
labels : ['dgp', 'ds_binC', 'fr', 'lcdm', 'rand', 'wcdm']
Labels encoding:
{'dgp': 0, 'ds_binC': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
n_labels : 6
dgp - 5000 training examples
ds_binC - 5000 training examples
fr - 5000 training examples
lcdm - 5000 training examples
rand - 5000 training examples
wcdm - 5000 training examples
N. of data files: 5000
get_all_indexes labels dict: {'dgp': 0, 'ds_binC': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
create_generators n_labels: 6
create_generators n_labels_eff: 6
create_generators len_c1: 1
Check for no duplicates in test: (0=ok):
0.0
Check for no duplicates in val: (0=ok):
0
N of indexes in training set: 4250
N of indexes in validation set: 750
N of indexes in test set: 0
Check - total per class: 5000
--create_generators, train indexes
batch_size: 3000
- Cut sample
bs: 3000
N_labels: 6
N_noise: 10
len_c1: 1
Train index length: 4250
--create_generators, validation indexes
- Cut sample
bs: 3000
N_labels: 6
N_noise: 10
len_c1: 1
Val index length: 750
len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 4250, 3000, 6, 10
--DataSet Train
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 399
Closest k to k_max is 2.504942
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (399, 1)
Final i_max used is 399
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds_binC', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 4250
n_indexes (n of file IDs read for each batch): 50
batch size: 3000
n_batches : 85
For each batch we read 50 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 3000 training examples
Input batch size: 3000
N of batches to cover all file IDs: 85
len(fname_list), batch_size, n_noisy_samples, n_batches: 25500, 3000, 10, 85
--DataSet Validation
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 399
Closest k to k_max is 2.504942
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (399, 1)
Final i_max used is 399
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds_binC', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 750
n_indexes (n of file IDs read for each batch): 50
batch size: 3000
n_batches : 15
For each batch we read 50 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 3000 training examples
Input batch size: 3000
N of batches to cover all file IDs: 15
len(fname_list), batch_size, n_noisy_samples, n_batches: 4500, 3000, 10, 15
------------ DONE ------------
------------ BUILDING MODEL ------------
Input shape (399, 4)
Model: "model"
_________________________________________________________________
Layer (type)                Output Shape              Param #
=================================================================
input_1 (InputLayer)        [(None, 399, 4)]          0
conv1d_flipout (Conv1DFlip  (None, 195, 8)            648
out)
max_pooling1d (MaxPooling1  (None, 97, 8)             0
D)
batch_normalization (Batch  (None, 97, 8)             32
Normalization)
conv1d_flipout_1 (Conv1DFl  (None, 47, 16)            1296
ipout)
max_pooling1d_1 (MaxPoolin  (None, 46, 16)            0
g1D)
batch_normalization_1 (Bat  (None, 46, 16)            64
chNormalization)
conv1d_flipout_2 (Conv1DFl  (None, 45, 32)            2080
ipout)
batch_normalization_2 (Bat  (None, 45, 32)            128
chNormalization)
global_average_pooling1d (  (None, 32)                0
GlobalAveragePooling1D)
dense_flipout (DenseFlipou  (None, 32)                2080
t)
batch_normalization_3 (Bat  (None, 32)                128
chNormalization)
dense_flipout_1 (DenseFlip  (None, 6)                 390
out)
=================================================================
Total params: 6846 (26.74 KB)
Trainable params: 6670 (26.05 KB)
Non-trainable params: 176 (704.00 Byte)
_________________________________________________________________
None
Found GPU at: /device:GPU:0
------------ TRAINING ------------
Features shape: (3000, 399, 4)
Labels shape: (3000, 6)
Initializing checkpoint from scratch.
Epoch 0
Validation loss decreased. Saved checkpoint for step 1: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-1
Time:  38.61s, ---- Loss: 1.1377, Acc.: 0.4533, Val. Loss: 3.4471, Val. Acc.: 0.2265
Epoch 1
Validation loss decreased. Saved checkpoint for step 2: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-2
Time:  1.75s, ---- Loss: 0.9663, Acc.: 0.5873, Val. Loss: 2.3006, Val. Acc.: 0.2378
Epoch 2
Loss did not decrease. Count = 1
Time:  1.42s, ---- Loss: 0.8756, Acc.: 0.6362, Val. Loss: 2.6297, Val. Acc.: 0.2782
Epoch 3
Loss did not decrease. Count = 2
Time:  1.43s, ---- Loss: 0.8107, Acc.: 0.6677, Val. Loss: 2.3413, Val. Acc.: 0.3265
Epoch 4
Validation loss decreased. Saved checkpoint for step 5: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-3
Time:  1.56s, ---- Loss: 0.7844, Acc.: 0.6845, Val. Loss: 1.7252, Val. Acc.: 0.4472
Epoch 5
Validation loss decreased. Saved checkpoint for step 6: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-4
Time:  1.55s, ---- Loss: 0.7445, Acc.: 0.6963, Val. Loss: 1.2185, Val. Acc.: 0.6028
Epoch 6
Validation loss decreased. Saved checkpoint for step 7: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-5
Time:  1.55s, ---- Loss: 0.7517, Acc.: 0.7033, Val. Loss: 1.1295, Val. Acc.: 0.6364
Epoch 7
Validation loss decreased. Saved checkpoint for step 8: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-6
Time:  1.56s, ---- Loss: 0.7350, Acc.: 0.7092, Val. Loss: 0.9967, Val. Acc.: 0.6852
Epoch 8
Validation loss decreased. Saved checkpoint for step 9: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-7
Time:  1.55s, ---- Loss: 0.7153, Acc.: 0.7141, Val. Loss: 0.9651, Val. Acc.: 0.6964
Epoch 9
Validation loss decreased. Saved checkpoint for step 10: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-8
Time:  1.53s, ---- Loss: 0.7076, Acc.: 0.7185, Val. Loss: 0.9580, Val. Acc.: 0.7004
Epoch 10
Loss did not decrease. Count = 1
Time:  1.40s, ---- Loss: 0.7016, Acc.: 0.7219, Val. Loss: 0.9675, Val. Acc.: 0.6984
Epoch 11
Loss did not decrease. Count = 2
Time:  1.42s, ---- Loss: 0.6826, Acc.: 0.7244, Val. Loss: 0.9645, Val. Acc.: 0.6968
Epoch 12
Loss did not decrease. Count = 3
Time:  1.42s, ---- Loss: 0.6778, Acc.: 0.7273, Val. Loss: 0.9803, Val. Acc.: 0.6936
Epoch 13
Loss did not decrease. Count = 4
Time:  1.42s, ---- Loss: 0.6718, Acc.: 0.7306, Val. Loss: 1.0253, Val. Acc.: 0.6825
Epoch 14
Loss did not decrease. Count = 5
Time:  1.40s, ---- Loss: 0.6669, Acc.: 0.7338, Val. Loss: 1.0337, Val. Acc.: 0.6803
Epoch 15
Loss did not decrease. Count = 6
Time:  1.41s, ---- Loss: 0.6658, Acc.: 0.7355, Val. Loss: 1.0730, Val. Acc.: 0.6729
Epoch 16
Loss did not decrease. Count = 7
Time:  1.40s, ---- Loss: 0.6554, Acc.: 0.7378, Val. Loss: 1.0228, Val. Acc.: 0.6893
Epoch 17
Loss did not decrease. Count = 8
Time:  1.40s, ---- Loss: 0.6438, Acc.: 0.7413, Val. Loss: 1.0115, Val. Acc.: 0.6922
Epoch 18
Loss did not decrease. Count = 9
Time:  1.40s, ---- Loss: 0.6428, Acc.: 0.7421, Val. Loss: 0.9760, Val. Acc.: 0.7004
Epoch 19
Loss did not decrease. Count = 10
Time:  1.41s, ---- Loss: 0.6415, Acc.: 0.7441, Val. Loss: 0.9591, Val. Acc.: 0.7070
Epoch 20
Validation loss decreased. Saved checkpoint for step 21: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-9
Time:  1.53s, ---- Loss: 0.6355, Acc.: 0.7457, Val. Loss: 0.9133, Val. Acc.: 0.7218
Epoch 21
Validation loss decreased. Saved checkpoint for step 22: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-10
Time:  1.53s, ---- Loss: 0.6252, Acc.: 0.7473, Val. Loss: 0.8860, Val. Acc.: 0.7318
Epoch 22
Validation loss decreased. Saved checkpoint for step 23: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-11
Time:  1.53s, ---- Loss: 0.6236, Acc.: 0.7491, Val. Loss: 0.8859, Val. Acc.: 0.7337
Epoch 23
Validation loss decreased. Saved checkpoint for step 24: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-12
Time:  1.54s, ---- Loss: 0.6208, Acc.: 0.7495, Val. Loss: 0.8746, Val. Acc.: 0.7371
Epoch 24
Validation loss decreased. Saved checkpoint for step 25: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-13
Time:  1.54s, ---- Loss: 0.6197, Acc.: 0.7517, Val. Loss: 0.8707, Val. Acc.: 0.7404
Epoch 25
Validation loss decreased. Saved checkpoint for step 26: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-14
Time:  1.54s, ---- Loss: 0.6136, Acc.: 0.7524, Val. Loss: 0.8623, Val. Acc.: 0.7446
Epoch 26
Validation loss decreased. Saved checkpoint for step 27: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-15
Time:  1.56s, ---- Loss: 0.6173, Acc.: 0.7535, Val. Loss: 0.8523, Val. Acc.: 0.7491
Epoch 27
Loss did not decrease. Count = 1
Time:  1.41s, ---- Loss: 0.6184, Acc.: 0.7542, Val. Loss: 0.8569, Val. Acc.: 0.7490
Epoch 28
Loss did not decrease. Count = 2
Time:  1.42s, ---- Loss: 0.6128, Acc.: 0.7549, Val. Loss: 0.8576, Val. Acc.: 0.7470
Epoch 29
Loss did not decrease. Count = 3
Time:  1.42s, ---- Loss: 0.6058, Acc.: 0.7558, Val. Loss: 0.8548, Val. Acc.: 0.7498
Epoch 30
Loss did not decrease. Count = 4
Time:  1.42s, ---- Loss: 0.6012, Acc.: 0.7569, Val. Loss: 0.8587, Val. Acc.: 0.7473
Epoch 31
Loss did not decrease. Count = 5
Time:  1.41s, ---- Loss: 0.6070, Acc.: 0.7570, Val. Loss: 0.8541, Val. Acc.: 0.7496
Epoch 32
Validation loss decreased. Saved checkpoint for step 33: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-16
Time:  1.52s, ---- Loss: 0.6022, Acc.: 0.7569, Val. Loss: 0.8462, Val. Acc.: 0.7532
Epoch 33
Validation loss decreased. Saved checkpoint for step 34: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-17
Time:  1.53s, ---- Loss: 0.6070, Acc.: 0.7576, Val. Loss: 0.8458, Val. Acc.: 0.7526
Epoch 34
Validation loss decreased. Saved checkpoint for step 35: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-18
Time:  1.54s, ---- Loss: 0.6093, Acc.: 0.7585, Val. Loss: 0.8427, Val. Acc.: 0.7552
Epoch 35
Loss did not decrease. Count = 1
Time:  1.40s, ---- Loss: 0.6050, Acc.: 0.7592, Val. Loss: 0.8429, Val. Acc.: 0.7538
Epoch 36
Validation loss decreased. Saved checkpoint for step 37: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-19
Time:  1.54s, ---- Loss: 0.5981, Acc.: 0.7606, Val. Loss: 0.8421, Val. Acc.: 0.7556
Epoch 37
Validation loss decreased. Saved checkpoint for step 38: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-20
Time:  1.54s, ---- Loss: 0.6054, Acc.: 0.7610, Val. Loss: 0.8399, Val. Acc.: 0.7564
Epoch 38
Validation loss decreased. Saved checkpoint for step 39: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-21
Time:  1.54s, ---- Loss: 0.5974, Acc.: 0.7614, Val. Loss: 0.8376, Val. Acc.: 0.7564
Epoch 39
Loss did not decrease. Count = 1
Time:  1.38s, ---- Loss: 0.6006, Acc.: 0.7616, Val. Loss: 0.8382, Val. Acc.: 0.7578
Epoch 40
Validation loss decreased. Saved checkpoint for step 41: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/tf_ckpts/ckpt-22
Time:  1.53s, ---- Loss: 0.6023, Acc.: 0.7621, Val. Loss: 0.8362, Val. Acc.: 0.7584
Epoch 41
Loss did not decrease. Count = 1
Time:  1.40s, ---- Loss: 0.5952, Acc.: 0.7627, Val. Loss: 0.8420, Val. Acc.: 0.7545
Epoch 42
Loss did not decrease. Count = 2
Time:  1.40s, ---- Loss: 0.5971, Acc.: 0.7633, Val. Loss: 0.8430, Val. Acc.: 0.7549
Epoch 43
Loss did not decrease. Count = 3
Time:  1.39s, ---- Loss: 0.6016, Acc.: 0.7639, Val. Loss: 0.8407, Val. Acc.: 0.7571
Epoch 44
Loss did not decrease. Count = 4
Time:  1.42s, ---- Loss: 0.5900, Acc.: 0.7640, Val. Loss: 0.8409, Val. Acc.: 0.7562
Epoch 45
Loss did not decrease. Count = 5
Time:  1.40s, ---- Loss: 0.5982, Acc.: 0.7643, Val. Loss: 0.8414, Val. Acc.: 0.7560
Epoch 46
Loss did not decrease. Count = 6
Time:  1.44s, ---- Loss: 0.5940, Acc.: 0.7642, Val. Loss: 0.8424, Val. Acc.: 0.7544
Epoch 47
Loss did not decrease. Count = 7
Time:  1.39s, ---- Loss: 0.5987, Acc.: 0.7651, Val. Loss: 0.8479, Val. Acc.: 0.7517
Epoch 48
Loss did not decrease. Count = 8
Time:  1.40s, ---- Loss: 0.5964, Acc.: 0.7644, Val. Loss: 0.8478, Val. Acc.: 0.7533
Epoch 49
Loss did not decrease. Count = 9
Time:  1.42s, ---- Loss: 0.5868, Acc.: 0.7649, Val. Loss: 0.8491, Val. Acc.: 0.7530
Epoch 50
Loss did not decrease. Count = 10
Time:  1.41s, ---- Loss: 0.5911, Acc.: 0.7657, Val. Loss: 0.8520, Val. Acc.: 0.7521
Epoch 51
Loss did not decrease. Count = 11
Time:  1.40s, ---- Loss: 0.5947, Acc.: 0.7657, Val. Loss: 0.8465, Val. Acc.: 0.7546
Epoch 52
Loss did not decrease. Count = 12
Time:  1.41s, ---- Loss: 0.5910, Acc.: 0.7660, Val. Loss: 0.8461, Val. Acc.: 0.7535
Epoch 53
Loss did not decrease. Count = 13
Time:  1.41s, ---- Loss: 0.5884, Acc.: 0.7662, Val. Loss: 0.8409, Val. Acc.: 0.7570
Epoch 54
Loss did not decrease. Count = 14
Time:  1.43s, ---- Loss: 0.5874, Acc.: 0.7666, Val. Loss: 0.8456, Val. Acc.: 0.7544
Epoch 55
Loss did not decrease. Count = 15
Time:  1.40s, ---- Loss: 0.5946, Acc.: 0.7666, Val. Loss: 0.8426, Val. Acc.: 0.7555
Epoch 56
Loss did not decrease. Count = 16
Time:  1.40s, ---- Loss: 0.5875, Acc.: 0.7663, Val. Loss: 0.8458, Val. Acc.: 0.7530
Epoch 57
Loss did not decrease. Count = 17
Time:  1.40s, ---- Loss: 0.5915, Acc.: 0.7670, Val. Loss: 0.8381, Val. Acc.: 0.7593
Epoch 58
Loss did not decrease. Count = 18
Time:  1.40s, ---- Loss: 0.5886, Acc.: 0.7668, Val. Loss: 0.8417, Val. Acc.: 0.7582
Epoch 59
Loss did not decrease. Count = 19
Time:  1.39s, ---- Loss: 0.5875, Acc.: 0.7676, Val. Loss: 0.8403, Val. Acc.: 0.7568
Epoch 60
Loss did not decrease. Count = 20
Max patience reached.
Saving at models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binC/hist.png
Done in 1163.08s
