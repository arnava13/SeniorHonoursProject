
 -------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune False
one_vs_all False
c_0 ['lcdm']
c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']
dataset_balanced False
include_last False
log_path 
restore False
fname fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_
model_name custom
my_path None
DIR data/train
TEST_DIR data/test
models_dir models/
save_ckpt True
out_path_overwrite False
curves_folder data/curve_files_sys/theory_error
save_processed_spectra False
cache_dir False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
norm_data_name /planck_ee2.txt
normalization stdcosmo
sample_pace 2
k_max 2.5
k_min 0.0
i_max None
i_min None
add_noise True
n_noisy_samples 10
add_shot False
add_sys True
add_cosvar True
sigma_sys None
sys_scaled None
sys_factor None
sys_max None
sigma_curves 0.05
sigma_curves_default 0.05
rescale_curves uniform
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable False
unfreeze False
lr 0.05
drop 0.5
n_epochs 100
val_size 0.15
test_size 0.0
batch_size 500
patience 20
GPU True
TPU False
decay 0.95
BatchNorm True
padding valid
shuffle True
group_lab_dict {'True': 'non_lcdm', 'dgp': 'non_lcdm', 'ds': 'non_lcdm', 'fR': 'non_lcdm', 'rand--save_ckpt': 'non_lcdm', 'wcdm': 'non_lcdm', 'lcdm': 'lcdm'}
save_indexes False
n_classes 5
------------ CREATING DATASETS ------------

labels : ['dgp', 'fr', 'lcdm', 'rand', 'wcdm']
Labels encoding: 
{'dgp': 0, 'fr': 1, 'lcdm': 2, 'rand': 3, 'wcdm': 4}
n_labels : 5
dgp - 1000 training examples
fr - 1000 training examples
lcdm - 1000 training examples
rand - 1000 training examples
wcdm - 1000 training examples

N. of data files: 1000
get_all_indexes labels dict: {'dgp': 0, 'fr': 1, 'lcdm': 2, 'rand': 3, 'wcdm': 4}
create_generators n_labels_eff: 5
create_generators len_c1: 1
--Train
batch_size: 500
- Cut sample
bs: 500
N_labels: 5
N_noise: 10
len_c1: 1
Indexes length: 1000
n_keep: 1000
Not sampling
New length: 1000
N batches: 100.0
 len_C1: 1
N indexes: 10.0
Ok.
N. of test files used: 1000
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 199
Closest k to k_max is 2.470504
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (199, 1)
Final i_max used is 199
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 5
N. n_classes in output: 5
LABELS: ['dgp', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 1000
n_indexes (n of file IDs read for each batch): 10
batch size: 500
n_batches : 100
For each batch we read 10 file IDs
For each file ID we have 5 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 500 training examples
Input batch size: 500
N of batches to cover all file IDs: 100
len(fname_list), batch_size, n_noisy_samples, n_batches: 5000, 500, 10, 100

 -------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune False
one_vs_all False
c_0 ['lcdm']
c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']
dataset_balanced False
include_last False
log_path 
restore False
fname fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_
model_name custom
my_path None
DIR data/train
TEST_DIR data/test
models_dir models/
save_ckpt True
out_path_overwrite False
curves_folder data/curve_files_sys/theory_error
save_processed_spectra False
cache_dir False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
norm_data_name /planck_ee2.txt
normalization stdcosmo
sample_pace 2
k_max 2.5
k_min 0.0
i_max None
i_min None
add_noise True
n_noisy_samples 10
add_shot False
add_sys True
add_cosvar True
sigma_sys None
sys_scaled None
sys_factor None
sys_max None
sigma_curves 0.05
sigma_curves_default 0.05
rescale_curves uniform
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable False
unfreeze False
lr 0.05
drop 0.5
n_epochs 100
val_size 0.15
test_size 0.0
batch_size 500
patience 20
GPU True
TPU False
decay 0.95
BatchNorm True
padding valid
shuffle True
group_lab_dict {'True': 'non_lcdm', 'dgp': 'non_lcdm', 'ds': 'non_lcdm', 'fR': 'non_lcdm', 'rand--save_ckpt': 'non_lcdm', 'wcdm': 'non_lcdm', 'lcdm': 'lcdm'}
save_indexes False
n_classes 5
------------ CREATING DATASETS ------------

labels : ['dgp', 'fr', 'lcdm', 'rand', 'wcdm']
Labels encoding: 
{'dgp': 0, 'fr': 1, 'lcdm': 2, 'rand': 3, 'wcdm': 4}
n_labels : 5
dgp - 1000 training examples
fr - 1000 training examples
lcdm - 1000 training examples
rand - 1000 training examples
wcdm - 1000 training examples

N. of data files: 1000
get_all_indexes labels dict: {'dgp': 0, 'fr': 1, 'lcdm': 2, 'rand': 3, 'wcdm': 4}
create_generators n_labels_eff: 5
create_generators len_c1: 1
--Train
batch_size: 500
- Cut sample
bs: 500
N_labels: 5
N_noise: 10
len_c1: 1
Indexes length: 1000
n_keep: 1000
Not sampling
New length: 1000
N batches: 100.0
 len_C1: 1
N indexes: 10.0
Ok.
N. of test files used: 1000
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 199
Closest k to k_max is 2.470504
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (199, 1)
Final i_max used is 199
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 5
N. n_classes in output: 5
LABELS: ['dgp', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 1000
n_indexes (n of file IDs read for each batch): 40
batch size: 2000
n_batches : 25
For each batch we read 40 file IDs
For each file ID we have 5 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 2000 training examples
Input batch size: 2000
N of batches to cover all file IDs: 25
len(fname_list), batch_size, n_noisy_samples, n_batches: 5000, 2000, 10, 25
------------ DONE ------------

Input shape (199, 4)
------------ BUILDING MODEL ------------

Model n_classes : 5 
Features shape: (199, 4)
Labels shape: (5,)
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 199, 4)]          0         
                                                                 
 conv1d_flipout (Conv1DFlip  (None, 95, 8)             648       
 out)                                                            
                                                                 
 max_pooling1d (MaxPooling1  (None, 47, 8)             0         
 D)                                                              
                                                                 
 batch_normalization (Batch  (None, 47, 8)             32        
 Normalization)                                                  
                                                                 
 conv1d_flipout_1 (Conv1DFl  (None, 22, 16)            1296      
 ipout)                                                          
                                                                 
 max_pooling1d_1 (MaxPoolin  (None, 21, 16)            0         
 g1D)                                                            
                                                                 
 batch_normalization_1 (Bat  (None, 21, 16)            64        
 chNormalization)                                                
                                                                 
 conv1d_flipout_2 (Conv1DFl  (None, 20, 32)            2080      
 ipout)                                                          
                                                                 
 batch_normalization_2 (Bat  (None, 20, 32)            128       
 chNormalization)                                                
                                                                 
 global_average_pooling1d (  (None, 32)                0         
 GlobalAveragePooling1D)                                         
                                                                 
 dense_flipout (DenseFlipou  (None, 32)                2080      
 t)                                                              
                                                                 
 batch_normalization_3 (Bat  (None, 32)                128       
 chNormalization)                                                
                                                                 
 dense_flipout_1 (DenseFlip  (None, 5)                 325       
 out)                                                            
                                                                 
=================================================================
Total params: 6781 (26.49 KB)
Trainable params: 6605 (25.80 KB)
Non-trainable params: 176 (704.00 Byte)
_________________________________________________________________
None
Computing loss for randomly initialized model...
Loss before loading weights/ 1.790875

------------ RESTORING CHECKPOINT ------------

Looking for ckpt in models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/
Restoring checkpoint from models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-39
Loss after loading weights/ 0.50147206

Threshold probability for classification: 0.5 
Accuracy on 0 batch using median of sampled probabilities: 0.91 %
Accuracy on 0 batch using median of sampled probabilities, not considering unclassified examples: 0.93285495 %
Accuracy on 1 batch using median of sampled probabilities: 0.9015 %
Accuracy on 1 batch using median of sampled probabilities, not considering unclassified examples: 0.92272264 %
Accuracy on 2 batch using median of sampled probabilities: 0.9075 %
Accuracy on 2 batch using median of sampled probabilities, not considering unclassified examples: 0.9283888 %
Accuracy on 3 batch using median of sampled probabilities: 0.906 %
Accuracy on 3 batch using median of sampled probabilities, not considering unclassified examples: 0.9235474 %
Accuracy on 4 batch using median of sampled probabilities: 0.9025 %
Accuracy on 4 batch using median of sampled probabilities, not considering unclassified examples: 0.9251666 %
Accuracy on 5 batch using median of sampled probabilities: 0.9015 %
Accuracy on 5 batch using median of sampled probabilities, not considering unclassified examples: 0.9213081 %
Accuracy on 6 batch using median of sampled probabilities: 0.9035 %
Accuracy on 6 batch using median of sampled probabilities, not considering unclassified examples: 0.9228805 %
Accuracy on 7 batch using median of sampled probabilities: 0.9095 %
Accuracy on 7 batch using median of sampled probabilities, not considering unclassified examples: 0.9294839 %
Accuracy on 8 batch using median of sampled probabilities: 0.903 %
Accuracy on 8 batch using median of sampled probabilities, not considering unclassified examples: 0.9275809 %
Accuracy on 9 batch using median of sampled probabilities: 0.915 %
Accuracy on 9 batch using median of sampled probabilities, not considering unclassified examples: 0.9346272 %
Accuracy on 10 batch using median of sampled probabilities: 0.909 %
Accuracy on 10 batch using median of sampled probabilities, not considering unclassified examples: 0.92849845 %
Accuracy on 11 batch using median of sampled probabilities: 0.904 %
Accuracy on 11 batch using median of sampled probabilities, not considering unclassified examples: 0.9248082 %
Accuracy on 12 batch using median of sampled probabilities: 0.92 %
Accuracy on 12 batch using median of sampled probabilities, not considering unclassified examples: 0.9411765 %
Accuracy on 13 batch using median of sampled probabilities: 0.9125 %
Accuracy on 13 batch using median of sampled probabilities, not considering unclassified examples: 0.9344598 %
Accuracy on 14 batch using median of sampled probabilities: 0.9165 %
Accuracy on 14 batch using median of sampled probabilities, not considering unclassified examples: 0.93568146 %
Accuracy on 15 batch using median of sampled probabilities: 0.9095 %
Accuracy on 15 batch using median of sampled probabilities, not considering unclassified examples: 0.93091094 %
Accuracy on 16 batch using median of sampled probabilities: 0.9025 %
Accuracy on 16 batch using median of sampled probabilities, not considering unclassified examples: 0.9270673 %
Accuracy on 17 batch using median of sampled probabilities: 0.906 %
Accuracy on 17 batch using median of sampled probabilities, not considering unclassified examples: 0.9249617 %
Accuracy on 18 batch using median of sampled probabilities: 0.9135 %
Accuracy on 18 batch using median of sampled probabilities, not considering unclassified examples: 0.9288256 %
Accuracy on 19 batch using median of sampled probabilities: 0.9015 %
Accuracy on 19 batch using median of sampled probabilities, not considering unclassified examples: 0.92319506 %
Accuracy on 20 batch using median of sampled probabilities: 0.901 %
Accuracy on 20 batch using median of sampled probabilities, not considering unclassified examples: 0.92695475 %
Accuracy on 21 batch using median of sampled probabilities: 0.9095 %
Accuracy on 21 batch using median of sampled probabilities, not considering unclassified examples: 0.9361812 %
Accuracy on 22 batch using median of sampled probabilities: 0.91 %
Accuracy on 22 batch using median of sampled probabilities, not considering unclassified examples: 0.93047035 %
Accuracy on 23 batch using median of sampled probabilities: 0.908 %
Accuracy on 23 batch using median of sampled probabilities, not considering unclassified examples: 0.92890024 %
Accuracy on 24 batch using median of sampled probabilities: 0.913 %
Accuracy on 24 batch using median of sampled probabilities, not considering unclassified examples: 0.93163264 %
-- Accuracy on test set using median of sampled probabilities: 0.90786016 % 

-- Accuracy on test set using median of sampled probabilities, not considering unclassified examples: 0.92889136 % 

Adding Not classified label
Saved confusion matrix at models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/cm_confusion_frozen_weights.pdf
Saved confusion matrix values at models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/cm_confusion_frozen_weights_values.txt
