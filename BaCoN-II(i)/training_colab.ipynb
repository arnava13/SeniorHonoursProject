{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2-LDl_8Nai6"
      },
      "source": [
        "# Classification of power spectra with BaCoN\n",
        "\n",
        "Arnav Agarwal\n",
        "\n",
        "Updated training notebook including minimum k_mode parameters, padding parameter, compatibility with TPU execution, github push/pull compatibility and minor improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErLy38KVuVZd"
      },
      "source": [
        "If you are saving models, you will need to use git push within the colab environment, or download the model files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flvg9eICNhqn"
      },
      "source": [
        "## Load the code\n",
        "\n",
        "This notebook is designed to run the training on google colab and testing on local machine (In ordeer to benefit from powerful Colab devices whilst avoidingn issues with colab's latex implementation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzX4nFm1BQqd",
        "outputId": "2e2c3892-0648-43ac-90cb-b15a69e381d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'SeniorHonoursProject'...\n",
            "remote: Enumerating objects: 293643, done.\u001b[K\n",
            "remote: Counting objects: 100% (16913/16913), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16535/16535), done.\u001b[K\n",
            "remote: Total 293643 (delta 378), reused 16911 (delta 376), pack-reused 276730\u001b[K\n",
            "Receiving objects: 100% (293643/293643), 3.55 GiB | 15.77 MiB/s, done.\n",
            "Resolving deltas: 100% (3570/3570), done.\n",
            "Updating files: 100% (32753/32753), done.\n",
            "/content/SeniorHonoursProject/BaCoN-II\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/arnava13/SeniorHonoursProject\n",
        "%cd SeniorHonoursProject/BaCoN-II(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVBtCPaWw60R"
      },
      "source": [
        "## Run training in shell\n",
        "only do this when not loading trained network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HamIuy_Qx2cf",
        "outputId": "1ffc46ba-371e-42ec-a880-becd17a913af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for interlap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for luigi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "lib5c version 0.6.1\n",
            "Requirement already satisfied: tensorflow==2.15.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-probability==0.23.0 in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (1.25.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (2.2.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (0.5.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (0.1.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q lib5c\n",
        "!lib5c -v\n",
        "!pip install tensorflow==2.15.0\n",
        "!pip install tensorflow-probability==0.23.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gv8_CyoHB0_Y"
      },
      "outputs": [],
      "source": [
        "import os;\n",
        "import subprocess\n",
        "import threading\n",
        "import argparse\n",
        "from test import *\n",
        "from utils import *\n",
        "from models import *\n",
        "from data_generator import *\n",
        "from train import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex2LIYxl6cjV"
      },
      "outputs": [],
      "source": [
        "from importer import load_model_for_test, my_predict, predict_bayes_label, predict_mean_proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oaw1mzZqTsnu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"font.family\"] = 'serif'\n",
        "plt.rcParams[\"mathtext.fontset\"] = \"cm\"\n",
        "latex_path = '/usr/local/texlive/2024/bin/universal-darwin/'\n",
        "if latex_path not in os.environ['PATH']:\n",
        "    os.environ['PATH'] += os.pathsep + latex_path\n",
        "plt.rcParams[\"text.usetex\"] = True\n",
        "plt.rcParams['text.latex.preamble'] = r'\\usepackage{siunitx}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZiOh8u5QowF"
      },
      "outputs": [],
      "source": [
        "def handle_output(stream, log_file):\n",
        "    for line in iter(stream.readline, b''):  # This iterates over the output line by line\n",
        "        my_bytes = line.strip()\n",
        "        if my_bytes:\n",
        "            output = f\"{my_bytes.decode('utf-8')}\"\n",
        "            print(output)\n",
        "            log_file.write(output + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXuIOzFXFQL9"
      },
      "source": [
        "### set parameters for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "w8B1FUswFQL9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Training options:\n",
        "\n",
        "# ------------------ change the training directory and model name here ------------------\n",
        "\n",
        "# training data directory path\n",
        "DIR='data/train'\n",
        "## (batch size, is a multiple of #classes * noise realisations, e.g. 5 classes, 10 noise --> must be multiple of 50)\n",
        "batch_size='10000'\n",
        "\n",
        "# training the network\n",
        "GPU='True' # Use GPUs in training?\n",
        "TPU='False' # Train with TPUs?\n",
        "# which kind of model\n",
        "bayesian='True' # Bayesian NN or traditional NN (weights single valued or distributions?)\n",
        "n_epochs='100' # How many epochs to train for?\n",
        "\n",
        "\n",
        "# ------------------ other parameters ------------------\n",
        "\n",
        "# Directories\n",
        "mypath=None # Parent directory\n",
        "\n",
        "# Edit this with base model directory\n",
        "mdir='models/'\n",
        "\n",
        "# normalisation file\n",
        "norm_data_name = '/planck_ee2.txt'\n",
        "\n",
        "# scale cut and resolution\n",
        "k_max='2.5' # max of k-modes to include?\n",
        "k_min='0.0' # min of k-modes to include?\n",
        "sample_pace='1'\n",
        "\n",
        "# should the processed curves be saved in an extra file (False recommended, only efficient for n_epochs = 1)\n",
        "save_processed_spectra='False'\n",
        "\n",
        "# fine-tune = only 'LCDM' and 'non-LCDM'\n",
        "fine_tune='False' # Model trained to distinguish between 'LCDM' and 'non-LCDM' classes or between all clases. i.e. binary or multi-classifier\n",
        "\n",
        "# Cache directory, set as False to cache on memory.\n",
        "cache_dir='False'\n",
        "\n",
        "# -------------------- noise model --------------------\n",
        "\n",
        "# noise model - Cosmic variance and shot noise are Euclid-like\n",
        "n_noisy_samples='10' # How many noise examples to train on?\n",
        "add_noise='True'# Add noise?\n",
        "add_shot='False'# Add shot noise term? -- default should be False\n",
        "add_sys='True'# Add systematic error term?\n",
        "add_cosvar='True'# Add cosmic variance term?\n",
        "\n",
        "# path to folder with theory error curves\n",
        "curves_folder = 'data/curve_files_sys/theory_error'; sigma_curves_default = '0.05'\n",
        "# change rescale distribution of theory error curves, uniform recommended\n",
        "rescale_curves = 'uniform' # or gaussian or None\n",
        "\n",
        "# sys error curves - relative scale of curve-amplitude\n",
        "sigma_curves='0.05'\n",
        "\n",
        "# ----------------- Model Name Generation -------------\n",
        "\n",
        "#How many classes?\n",
        "classnum = 'fivelabel'\n",
        "\n",
        "#Examples Per Class?\n",
        "examples_per_class = '20k'\n",
        "\n",
        "#LCDM * Filter or Equal Examples from each class * Filter for randoms?\n",
        "randstype = \"equalexamples-randoms\"\n",
        "#randstype = \"LCDM-randoms\"\n",
        "\n",
        "#Additional Notes?\n",
        "fname_notes = \"\"\n",
        "\n",
        "#Assemble fname\n",
        "fname = f\"{classnum}_{examples_per_class}_{randstype}_kmin-{k_min}_kmax-{k_max}_{fname_notes}\"\n",
        "\n",
        "# -------------------- additional training settings --------------------\n",
        "\n",
        "\n",
        "# Type of model\n",
        "model_name='custom' # Custom or dummy - dummy has presets as given in models.py\n",
        "\n",
        "#Test mode?\n",
        "test_mode='False' # Network trained on 1 batch of minimal size or not?\n",
        "seed='1312' # Initial seed for test mode batch\n",
        "# Saving or restoring training\n",
        "restore='False' # Restore training from a checkpoint?\n",
        "save_ckpt='True' # Save checkpoints?\n",
        "\n",
        "\n",
        "val_size='0.15' # Validation set % of data set\n",
        "\n",
        "add_FT_dense='False' #if True, adds an additional dense layer before the final 2D one\n",
        "\n",
        "patience='20' # terminate training after 'patience' epochs if no decrease in loss function\n",
        "lr='0.01' # learning rate\n",
        "decay='0.95' #decay rate: If None : Adam(lr),\n",
        "\n",
        "padding='valid'\n",
        "\n",
        "# -------------------- BNN parameters -----------------\n",
        "\n",
        "# Example image details\n",
        "im_depth='500' # Number in z direction (e.g. 500 wave modes for P(k) examples)\n",
        "im_width='1' # Number in y direction (1 is a 2D image : P(k,mu))\n",
        "im_channels='4'  # Number in x direction (e.g. 4 redshifts for P(k,z))\n",
        "swap_axes='True' # Do we swap depth and width axes? True if we have 1D image in 4 channels\n",
        "sort_labels='True' # Sorts labels in ascending/alphabetical order\n",
        "\n",
        "z1='0' # which z-bins to include? Assumes each channel is a new z bin.\n",
        "z2='1' # which z-bins to include? Assumes each channel is a new z bin.\n",
        "z3='2' # which z-bins to include? Assumes each channel is a new z bin.\n",
        "z4='3' # which z-bins to include? Assumes each channel is a new z bin.\n",
        "\n",
        "# Number of layers and kernel sizes\n",
        "k1='10'\n",
        "k2='5'\n",
        "k3='2'\n",
        " # The dimensionality of the output space (i.e. the number of filters in the convolution)\n",
        "f1='8'\n",
        "f2='16'\n",
        "f3='32'\n",
        " # Stride of each layer's kernel\n",
        "s1='2'\n",
        "s2='2'\n",
        "s3='1'\n",
        "# Pooling layer sizes\n",
        "p1='2'\n",
        "p2='2'\n",
        "p3='0'\n",
        "# Strides in Pooling layer\n",
        "sp1='2'\n",
        "sp2='1'\n",
        "sp3='0'\n",
        "\n",
        "n_dense='1' # Number of dense layers\n",
        "\n",
        "# labels of different cosmologies\n",
        "#c0_label = 'lcdm'\n",
        "#c1_label = 'fR dgp wcdm'\n",
        "\n",
        "log_path = mdir + fname + '_log'\n",
        "\n",
        "if fine_tune == \"True\":\n",
        "    log_path += '_FT'\n",
        "\n",
        "log_path += '.txt'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YN5PCcWXWgr"
      },
      "source": [
        "### run the training\n",
        "this may take a while to run, also expect some (harmless) error messages in the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdSzp6AFFQL-",
        "outputId": "0594d764-0281-471b-cc4f-0bbb4d0fc847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-09 22:31:15.534224: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-09 22:31:15.534267: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-09 22:31:15.536264: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-09 22:31:16.561364: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-04-09 22:31:18.977277: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "using 1D layers and 4 channels\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "loc = add_variable_fn(\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "untransformed_scale = add_variable_fn(\n",
            "Expected output dimension after layer: conv1d_flipout : 97\n",
            "Expected output dimension after layer: conv1d_flipout_1 : 46\n",
            "Expected output dimension after layer: conv1d_flipout_2 : 45\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1712705167.020390    5010 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "Directory models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_ not created\n",
            "-------- Parameters:\n",
            "bayesian True\n",
            "test_mode False\n",
            "n_test_idx 2\n",
            "seed 1312\n",
            "fine_tune False\n",
            "one_vs_all False\n",
            "c_0 ['lcdm']\n",
            "c_1 ['dgp', 'ds', 'fR', 'rand']\n",
            "dataset_balanced False\n",
            "include_last False\n",
            "log_path models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5__log.txt\n",
            "restore False\n",
            "fname fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_\n",
            "model_name custom\n",
            "my_path None\n",
            "DIR data/train\n",
            "TEST_DIR data/test/\n",
            "models_dir models/\n",
            "save_ckpt True\n",
            "out_path_overwrite False\n",
            "curves_folder data/curve_files_sys/theory_error\n",
            "save_processed_spectra False\n",
            "cache_dir False\n",
            "im_depth 500\n",
            "im_width 1\n",
            "im_channels 4\n",
            "swap_axes True\n",
            "sort_labels True\n",
            "norm_data_name /planck_ee2.txt\n",
            "normalization stdcosmo\n",
            "sample_pace 1\n",
            "k_max 2.5\n",
            "k_min 0.0\n",
            "i_max None\n",
            "i_min None\n",
            "add_noise True\n",
            "n_noisy_samples 10\n",
            "add_shot False\n",
            "add_sys True\n",
            "add_cosvar True\n",
            "sigma_sys None\n",
            "sys_scaled None\n",
            "sys_factor None\n",
            "sys_max None\n",
            "sigma_curves 0.05\n",
            "sigma_curves_default 0.05\n",
            "rescale_curves uniform\n",
            "z_bins [0, 1, 2, 3]\n",
            "n_dense 1\n",
            "filters [8, 16, 32]\n",
            "kernel_sizes [10, 5, 2]\n",
            "strides [2, 2, 1]\n",
            "pool_sizes [2, 2, 0]\n",
            "strides_pooling [2, 1, 0]\n",
            "add_FT_dense False\n",
            "trainable False\n",
            "unfreeze False\n",
            "lr 0.01\n",
            "drop 0.5\n",
            "n_epochs 100\n",
            "val_size 0.15\n",
            "test_size 0.0\n",
            "batch_size 10000\n",
            "patience 20\n",
            "GPU True\n",
            "TPU False\n",
            "decay 0.95\n",
            "BatchNorm True\n",
            "padding valid\n",
            "shuffle True\n",
            "------------ CREATING DATA GENERATORS ------------\n",
            "labels : ['dgp', 'ds', 'fr', 'lcdm', 'rand']\n",
            "Labels encoding:\n",
            "{'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4}\n",
            "n_labels : 5\n",
            "dgp - 20000 training examples\n",
            "ds - 20000 training examples\n",
            "fr - 20000 training examples\n",
            "lcdm - 20000 training examples\n",
            "rand - 20000 training examples\n",
            "N. of data files: 20000\n",
            "get_all_indexes labels dict: {'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4}\n",
            "create_generators n_labels: 5\n",
            "create_generators n_labels_eff: 5\n",
            "create_generators len_c1: 1\n",
            "Check for no duplicates in test: (0=ok):\n",
            "0.0\n",
            "Check for no duplicates in val: (0=ok):\n",
            "0\n",
            "N of indexes in training set: 17000\n",
            "N of indexes in validation set: 3000\n",
            "N of indexes in test set: 0\n",
            "Check - total per class: 20000\n",
            "--create_generators, train indexes\n",
            "batch_size: 10000\n",
            "- Cut sample\n",
            "bs: 10000\n",
            "N_labels: 5\n",
            "N_noise: 10\n",
            "len_c1: 1\n",
            "Train index length: 17000\n",
            "--create_generators, validation indexes\n",
            "- Cut sample\n",
            "bs: 10000\n",
            "N_labels: 5\n",
            "N_noise: 10\n",
            "len_c1: 1\n",
            "Val index length: 3000\n",
            "len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 17000, 10000, 5, 10\n",
            "--DataSet Train\n",
            "DataSet Initialization\n",
            "Using z bins [0, 1, 2, 3]\n",
            "Normalisation file is /planck_ee2.txt\n",
            "Specified k_max is 2.5\n",
            "Corresponding i_max is 399\n",
            "Closest k to k_max is 2.504942\n",
            "Specified k_min is 0.0\n",
            "Corresponding i_min is 0\n",
            "Closest k to k_min is 0.01\n",
            "New data dim: (399, 1)\n",
            "Final i_max used is 399\n",
            "Final i_min used is 0\n",
            "one_vs_all: False\n",
            "dataset_balanced: False\n",
            "base_case_dataset: True\n",
            "N. classes: 5\n",
            "N. n_classes in output: 5\n",
            "LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand']\n",
            "list_IDs length: 17000\n",
            "n_indexes (n of file IDs read for each batch): 200\n",
            "batch size: 10000\n",
            "n_batches : 85\n",
            "For each batch we read 200 file IDs\n",
            "For each file ID we have 5 labels\n",
            "For each ID, label we have 10 realizations of noise\n",
            "In total, for each batch we have 10000 training examples\n",
            "Input batch size: 10000\n",
            "N of batches to cover all file IDs: 85\n",
            "len(fname_list), batch_size, n_noisy_samples, n_batches: 85000, 10000, 10, 85\n",
            "--DataSet Validation\n",
            "DataSet Initialization\n",
            "Using z bins [0, 1, 2, 3]\n",
            "Normalisation file is /planck_ee2.txt\n",
            "Specified k_max is 2.5\n",
            "Corresponding i_max is 399\n",
            "Closest k to k_max is 2.504942\n",
            "Specified k_min is 0.0\n",
            "Corresponding i_min is 0\n",
            "Closest k to k_min is 0.01\n",
            "New data dim: (399, 1)\n",
            "Final i_max used is 399\n",
            "Final i_min used is 0\n",
            "one_vs_all: False\n",
            "dataset_balanced: False\n",
            "base_case_dataset: True\n",
            "N. classes: 5\n",
            "N. n_classes in output: 5\n",
            "LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand']\n",
            "list_IDs length: 3000\n",
            "n_indexes (n of file IDs read for each batch): 200\n",
            "batch size: 10000\n",
            "n_batches : 15\n",
            "For each batch we read 200 file IDs\n",
            "For each file ID we have 5 labels\n",
            "For each ID, label we have 10 realizations of noise\n",
            "In total, for each batch we have 10000 training examples\n",
            "Input batch size: 10000\n",
            "N of batches to cover all file IDs: 15\n",
            "len(fname_list), batch_size, n_noisy_samples, n_batches: 15000, 10000, 10, 15\n",
            "------------ DONE ------------\n",
            "------------ BUILDING MODEL ------------\n",
            "Input shape (399, 4)\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                Output Shape              Param #\n",
            "=================================================================\n",
            "input_1 (InputLayer)        [(None, 399, 4)]          0\n",
            "conv1d_flipout (Conv1DFlip  (None, 195, 8)            648\n",
            "out)\n",
            "max_pooling1d (MaxPooling1  (None, 97, 8)             0\n",
            "D)\n",
            "batch_normalization (Batch  (None, 97, 8)             32\n",
            "Normalization)\n",
            "conv1d_flipout_1 (Conv1DFl  (None, 47, 16)            1296\n",
            "ipout)\n",
            "max_pooling1d_1 (MaxPoolin  (None, 46, 16)            0\n",
            "g1D)\n",
            "batch_normalization_1 (Bat  (None, 46, 16)            64\n",
            "chNormalization)\n",
            "conv1d_flipout_2 (Conv1DFl  (None, 45, 32)            2080\n",
            "ipout)\n",
            "batch_normalization_2 (Bat  (None, 45, 32)            128\n",
            "chNormalization)\n",
            "global_average_pooling1d (  (None, 32)                0\n",
            "GlobalAveragePooling1D)\n",
            "dense_flipout (DenseFlipou  (None, 32)                2080\n",
            "t)\n",
            "batch_normalization_3 (Bat  (None, 32)                128\n",
            "chNormalization)\n",
            "dense_flipout_1 (DenseFlip  (None, 5)                 325\n",
            "out)\n",
            "=================================================================\n",
            "Total params: 6781 (26.49 KB)\n",
            "Trainable params: 6605 (25.80 KB)\n",
            "Non-trainable params: 176 (704.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Found GPU at: /device:GPU:0\n",
            "------------ TRAINING ------------\n",
            "Features shape: (10000, 399, 4)\n",
            "Labels shape: (10000, 5)\n",
            "Initializing checkpoint from scratch.\n",
            "Epoch 0\n",
            "Validation loss decreased. Saved checkpoint for step 1: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-1\n",
            "Time:  45.57s, ---- Loss: 0.7934, Acc.: 0.5715, Val. Loss: 1.6714, Val. Acc.: 0.3762\n",
            "Epoch 1\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.34s, ---- Loss: 0.6097, Acc.: 0.7335, Val. Loss: 2.0015, Val. Acc.: 0.3804\n",
            "Epoch 2\n",
            "Loss did not decrease. Count = 2\n",
            "Time:  2.34s, ---- Loss: 0.5626, Acc.: 0.7759, Val. Loss: 2.4356, Val. Acc.: 0.4250\n",
            "Epoch 3\n",
            "Loss did not decrease. Count = 3\n",
            "Time:  2.33s, ---- Loss: 0.5200, Acc.: 0.7955, Val. Loss: 2.1892, Val. Acc.: 0.4878\n",
            "Epoch 4\n",
            "Validation loss decreased. Saved checkpoint for step 5: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-2\n",
            "Time:  2.46s, ---- Loss: 0.4957, Acc.: 0.8069, Val. Loss: 1.5085, Val. Acc.: 0.5608\n",
            "Epoch 5\n",
            "Validation loss decreased. Saved checkpoint for step 6: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-3\n",
            "Time:  2.45s, ---- Loss: 0.4703, Acc.: 0.8152, Val. Loss: 1.0709, Val. Acc.: 0.6487\n",
            "Epoch 6\n",
            "Validation loss decreased. Saved checkpoint for step 7: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-4\n",
            "Time:  2.46s, ---- Loss: 0.4529, Acc.: 0.8236, Val. Loss: 0.9210, Val. Acc.: 0.7000\n",
            "Epoch 7\n",
            "Validation loss decreased. Saved checkpoint for step 8: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-5\n",
            "Time:  2.45s, ---- Loss: 0.4377, Acc.: 0.8301, Val. Loss: 0.7780, Val. Acc.: 0.7514\n",
            "Epoch 8\n",
            "Validation loss decreased. Saved checkpoint for step 9: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-6\n",
            "Time:  2.47s, ---- Loss: 0.4266, Acc.: 0.8357, Val. Loss: 0.6724, Val. Acc.: 0.7759\n",
            "Epoch 9\n",
            "Validation loss decreased. Saved checkpoint for step 10: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-7\n",
            "Time:  2.45s, ---- Loss: 0.4242, Acc.: 0.8405, Val. Loss: 0.6235, Val. Acc.: 0.7891\n",
            "Epoch 10\n",
            "Validation loss decreased. Saved checkpoint for step 11: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-8\n",
            "Time:  2.45s, ---- Loss: 0.4102, Acc.: 0.8449, Val. Loss: 0.6033, Val. Acc.: 0.7950\n",
            "Epoch 11\n",
            "Validation loss decreased. Saved checkpoint for step 12: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-9\n",
            "Time:  2.45s, ---- Loss: 0.4048, Acc.: 0.8481, Val. Loss: 0.5567, Val. Acc.: 0.8091\n",
            "Epoch 12\n",
            "Validation loss decreased. Saved checkpoint for step 13: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-10\n",
            "Time:  2.45s, ---- Loss: 0.3993, Acc.: 0.8519, Val. Loss: 0.5512, Val. Acc.: 0.8148\n",
            "Epoch 13\n",
            "Validation loss decreased. Saved checkpoint for step 14: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-11\n",
            "Time:  2.47s, ---- Loss: 0.3854, Acc.: 0.8543, Val. Loss: 0.5447, Val. Acc.: 0.8179\n",
            "Epoch 14\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.33s, ---- Loss: 0.3800, Acc.: 0.8568, Val. Loss: 0.5535, Val. Acc.: 0.8147\n",
            "Epoch 15\n",
            "Validation loss decreased. Saved checkpoint for step 16: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-12\n",
            "Time:  2.44s, ---- Loss: 0.3768, Acc.: 0.8593, Val. Loss: 0.5318, Val. Acc.: 0.8249\n",
            "Epoch 16\n",
            "Validation loss decreased. Saved checkpoint for step 17: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-13\n",
            "Time:  2.45s, ---- Loss: 0.3750, Acc.: 0.8607, Val. Loss: 0.5132, Val. Acc.: 0.8315\n",
            "Epoch 17\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.33s, ---- Loss: 0.3688, Acc.: 0.8628, Val. Loss: 0.5134, Val. Acc.: 0.8326\n",
            "Epoch 18\n",
            "Validation loss decreased. Saved checkpoint for step 19: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-14\n",
            "Time:  2.47s, ---- Loss: 0.3664, Acc.: 0.8642, Val. Loss: 0.4944, Val. Acc.: 0.8383\n",
            "Epoch 19\n",
            "Validation loss decreased. Saved checkpoint for step 20: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-15\n",
            "Time:  2.45s, ---- Loss: 0.3627, Acc.: 0.8652, Val. Loss: 0.4942, Val. Acc.: 0.8391\n",
            "Epoch 20\n",
            "Validation loss decreased. Saved checkpoint for step 21: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-16\n",
            "Time:  2.46s, ---- Loss: 0.3582, Acc.: 0.8669, Val. Loss: 0.4578, Val. Acc.: 0.8523\n",
            "Epoch 21\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.33s, ---- Loss: 0.3570, Acc.: 0.8682, Val. Loss: 0.4760, Val. Acc.: 0.8462\n",
            "Epoch 22\n",
            "Loss did not decrease. Count = 2\n",
            "Time:  2.34s, ---- Loss: 0.3566, Acc.: 0.8693, Val. Loss: 0.4629, Val. Acc.: 0.8510\n",
            "Epoch 23\n",
            "Loss did not decrease. Count = 3\n",
            "Time:  2.34s, ---- Loss: 0.3487, Acc.: 0.8706, Val. Loss: 0.4598, Val. Acc.: 0.8517\n",
            "Epoch 24\n",
            "Validation loss decreased. Saved checkpoint for step 25: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-17\n",
            "Time:  2.44s, ---- Loss: 0.3492, Acc.: 0.8716, Val. Loss: 0.4415, Val. Acc.: 0.8611\n",
            "Epoch 25\n",
            "Validation loss decreased. Saved checkpoint for step 26: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-18\n",
            "Time:  2.46s, ---- Loss: 0.3485, Acc.: 0.8728, Val. Loss: 0.4339, Val. Acc.: 0.8640\n",
            "Epoch 26\n",
            "Validation loss decreased. Saved checkpoint for step 27: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-19\n",
            "Time:  2.45s, ---- Loss: 0.3421, Acc.: 0.8737, Val. Loss: 0.4314, Val. Acc.: 0.8647\n",
            "Epoch 27\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.35s, ---- Loss: 0.3448, Acc.: 0.8745, Val. Loss: 0.4380, Val. Acc.: 0.8629\n",
            "Epoch 28\n",
            "Validation loss decreased. Saved checkpoint for step 29: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-20\n",
            "Time:  2.46s, ---- Loss: 0.3391, Acc.: 0.8753, Val. Loss: 0.4305, Val. Acc.: 0.8648\n",
            "Epoch 29\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.35s, ---- Loss: 0.3403, Acc.: 0.8762, Val. Loss: 0.4316, Val. Acc.: 0.8638\n",
            "Epoch 30\n",
            "Validation loss decreased. Saved checkpoint for step 31: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-21\n",
            "Time:  2.45s, ---- Loss: 0.3385, Acc.: 0.8768, Val. Loss: 0.4275, Val. Acc.: 0.8658\n",
            "Epoch 31\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.33s, ---- Loss: 0.3395, Acc.: 0.8772, Val. Loss: 0.4286, Val. Acc.: 0.8650\n",
            "Epoch 32\n",
            "Validation loss decreased. Saved checkpoint for step 33: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-22\n",
            "Time:  2.48s, ---- Loss: 0.3329, Acc.: 0.8779, Val. Loss: 0.4194, Val. Acc.: 0.8699\n",
            "Epoch 33\n",
            "Validation loss decreased. Saved checkpoint for step 34: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-23\n",
            "Time:  2.46s, ---- Loss: 0.3314, Acc.: 0.8784, Val. Loss: 0.4154, Val. Acc.: 0.8727\n",
            "Epoch 34\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.32s, ---- Loss: 0.3340, Acc.: 0.8788, Val. Loss: 0.4218, Val. Acc.: 0.8701\n",
            "Epoch 35\n",
            "Loss did not decrease. Count = 2\n",
            "Time:  2.32s, ---- Loss: 0.3304, Acc.: 0.8794, Val. Loss: 0.4167, Val. Acc.: 0.8731\n",
            "Epoch 36\n",
            "Loss did not decrease. Count = 3\n",
            "Time:  2.33s, ---- Loss: 0.3336, Acc.: 0.8795, Val. Loss: 0.4224, Val. Acc.: 0.8707\n",
            "Epoch 37\n",
            "Loss did not decrease. Count = 4\n",
            "Time:  2.34s, ---- Loss: 0.3270, Acc.: 0.8803, Val. Loss: 0.4198, Val. Acc.: 0.8714\n",
            "Epoch 38\n",
            "Loss did not decrease. Count = 5\n",
            "Time:  2.33s, ---- Loss: 0.3297, Acc.: 0.8806, Val. Loss: 0.4158, Val. Acc.: 0.8735\n",
            "Epoch 39\n",
            "Validation loss decreased. Saved checkpoint for step 40: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-24\n",
            "Time:  2.46s, ---- Loss: 0.3283, Acc.: 0.8807, Val. Loss: 0.4135, Val. Acc.: 0.8738\n",
            "Epoch 40\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.33s, ---- Loss: 0.3277, Acc.: 0.8813, Val. Loss: 0.4152, Val. Acc.: 0.8733\n",
            "Epoch 41\n",
            "Validation loss decreased. Saved checkpoint for step 42: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-25\n",
            "Time:  2.47s, ---- Loss: 0.3271, Acc.: 0.8817, Val. Loss: 0.4123, Val. Acc.: 0.8751\n",
            "Epoch 42\n",
            "Validation loss decreased. Saved checkpoint for step 43: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-26\n",
            "Time:  2.46s, ---- Loss: 0.3261, Acc.: 0.8821, Val. Loss: 0.4090, Val. Acc.: 0.8761\n",
            "Epoch 43\n",
            "Validation loss decreased. Saved checkpoint for step 44: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-27\n",
            "Time:  2.44s, ---- Loss: 0.3204, Acc.: 0.8820, Val. Loss: 0.4052, Val. Acc.: 0.8778\n",
            "Epoch 44\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.32s, ---- Loss: 0.3263, Acc.: 0.8823, Val. Loss: 0.4086, Val. Acc.: 0.8764\n",
            "Epoch 45\n",
            "Loss did not decrease. Count = 2\n",
            "Time:  2.33s, ---- Loss: 0.3221, Acc.: 0.8829, Val. Loss: 0.4062, Val. Acc.: 0.8777\n",
            "Epoch 46\n",
            "Loss did not decrease. Count = 3\n",
            "Time:  2.33s, ---- Loss: 0.3263, Acc.: 0.8830, Val. Loss: 0.4053, Val. Acc.: 0.8786\n",
            "Epoch 47\n",
            "Validation loss decreased. Saved checkpoint for step 48: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-28\n",
            "Time:  2.45s, ---- Loss: 0.3220, Acc.: 0.8834, Val. Loss: 0.4040, Val. Acc.: 0.8797\n",
            "Epoch 48\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.33s, ---- Loss: 0.3197, Acc.: 0.8834, Val. Loss: 0.4075, Val. Acc.: 0.8778\n",
            "Epoch 49\n",
            "Loss did not decrease. Count = 2\n",
            "Time:  2.33s, ---- Loss: 0.3205, Acc.: 0.8838, Val. Loss: 0.4185, Val. Acc.: 0.8733\n",
            "Epoch 50\n",
            "Loss did not decrease. Count = 3\n",
            "Time:  2.33s, ---- Loss: 0.3220, Acc.: 0.8839, Val. Loss: 0.4176, Val. Acc.: 0.8741\n",
            "Epoch 51\n",
            "Loss did not decrease. Count = 4\n",
            "Time:  2.35s, ---- Loss: 0.3189, Acc.: 0.8840, Val. Loss: 0.4054, Val. Acc.: 0.8788\n",
            "Epoch 52\n",
            "Validation loss decreased. Saved checkpoint for step 53: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-29\n",
            "Time:  2.45s, ---- Loss: 0.3210, Acc.: 0.8840, Val. Loss: 0.3990, Val. Acc.: 0.8816\n",
            "Epoch 53\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.33s, ---- Loss: 0.3193, Acc.: 0.8845, Val. Loss: 0.4018, Val. Acc.: 0.8799\n",
            "Epoch 54\n",
            "Loss did not decrease. Count = 2\n",
            "Time:  2.33s, ---- Loss: 0.3203, Acc.: 0.8845, Val. Loss: 0.4089, Val. Acc.: 0.8775\n",
            "Epoch 55\n",
            "Loss did not decrease. Count = 3\n",
            "Time:  2.33s, ---- Loss: 0.3190, Acc.: 0.8847, Val. Loss: 0.4096, Val. Acc.: 0.8771\n",
            "Epoch 56\n",
            "Loss did not decrease. Count = 4\n",
            "Time:  2.37s, ---- Loss: 0.3161, Acc.: 0.8849, Val. Loss: 0.4012, Val. Acc.: 0.8810\n",
            "Epoch 57\n",
            "Loss did not decrease. Count = 5\n",
            "Time:  2.33s, ---- Loss: 0.3186, Acc.: 0.8851, Val. Loss: 0.4017, Val. Acc.: 0.8802\n",
            "Epoch 58\n",
            "Loss did not decrease. Count = 6\n",
            "Time:  2.33s, ---- Loss: 0.3156, Acc.: 0.8852, Val. Loss: 0.3991, Val. Acc.: 0.8814\n",
            "Epoch 59\n",
            "Loss did not decrease. Count = 7\n",
            "Time:  2.33s, ---- Loss: 0.3196, Acc.: 0.8852, Val. Loss: 0.4016, Val. Acc.: 0.8799\n",
            "Epoch 60\n",
            "Loss did not decrease. Count = 8\n",
            "Time:  2.34s, ---- Loss: 0.3174, Acc.: 0.8853, Val. Loss: 0.4048, Val. Acc.: 0.8795\n",
            "Epoch 61\n",
            "Validation loss decreased. Saved checkpoint for step 62: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-30\n",
            "Time:  2.47s, ---- Loss: 0.3166, Acc.: 0.8856, Val. Loss: 0.3975, Val. Acc.: 0.8825\n",
            "Epoch 62\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.33s, ---- Loss: 0.3146, Acc.: 0.8856, Val. Loss: 0.3983, Val. Acc.: 0.8820\n",
            "Epoch 63\n",
            "Validation loss decreased. Saved checkpoint for step 64: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-31\n",
            "Time:  2.45s, ---- Loss: 0.3132, Acc.: 0.8858, Val. Loss: 0.3964, Val. Acc.: 0.8822\n",
            "Epoch 64\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.34s, ---- Loss: 0.3161, Acc.: 0.8861, Val. Loss: 0.3974, Val. Acc.: 0.8821\n",
            "Epoch 65\n",
            "Validation loss decreased. Saved checkpoint for step 66: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-32\n",
            "Time:  2.47s, ---- Loss: 0.3178, Acc.: 0.8859, Val. Loss: 0.3953, Val. Acc.: 0.8832\n",
            "Epoch 66\n",
            "Validation loss decreased. Saved checkpoint for step 67: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-33\n",
            "Time:  2.46s, ---- Loss: 0.3158, Acc.: 0.8864, Val. Loss: 0.3945, Val. Acc.: 0.8830\n",
            "Epoch 67\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.33s, ---- Loss: 0.3163, Acc.: 0.8862, Val. Loss: 0.3958, Val. Acc.: 0.8829\n",
            "Epoch 68\n",
            "Loss did not decrease. Count = 2\n",
            "Time:  2.33s, ---- Loss: 0.3162, Acc.: 0.8860, Val. Loss: 0.3957, Val. Acc.: 0.8832\n",
            "Epoch 69\n",
            "Loss did not decrease. Count = 3\n",
            "Time:  2.34s, ---- Loss: 0.3134, Acc.: 0.8862, Val. Loss: 0.3962, Val. Acc.: 0.8825\n",
            "Epoch 70\n",
            "Validation loss decreased. Saved checkpoint for step 71: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-34\n",
            "Time:  2.46s, ---- Loss: 0.3162, Acc.: 0.8862, Val. Loss: 0.3927, Val. Acc.: 0.8841\n",
            "Epoch 71\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.34s, ---- Loss: 0.3130, Acc.: 0.8861, Val. Loss: 0.3935, Val. Acc.: 0.8842\n",
            "Epoch 72\n",
            "Validation loss decreased. Saved checkpoint for step 73: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-35\n",
            "Time:  2.45s, ---- Loss: 0.3135, Acc.: 0.8861, Val. Loss: 0.3923, Val. Acc.: 0.8842\n",
            "Epoch 73\n",
            "Validation loss decreased. Saved checkpoint for step 74: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-36\n",
            "Time:  2.46s, ---- Loss: 0.3188, Acc.: 0.8866, Val. Loss: 0.3921, Val. Acc.: 0.8845\n",
            "Epoch 74\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.32s, ---- Loss: 0.3140, Acc.: 0.8865, Val. Loss: 0.3953, Val. Acc.: 0.8836\n",
            "Epoch 75\n",
            "Loss did not decrease. Count = 2\n",
            "Time:  2.34s, ---- Loss: 0.3160, Acc.: 0.8866, Val. Loss: 0.3937, Val. Acc.: 0.8841\n",
            "Epoch 76\n",
            "Validation loss decreased. Saved checkpoint for step 77: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-37\n",
            "Time:  2.45s, ---- Loss: 0.3147, Acc.: 0.8868, Val. Loss: 0.3918, Val. Acc.: 0.8849\n",
            "Epoch 77\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.34s, ---- Loss: 0.3134, Acc.: 0.8869, Val. Loss: 0.3925, Val. Acc.: 0.8847\n",
            "Epoch 78\n",
            "Validation loss decreased. Saved checkpoint for step 79: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-38\n",
            "Time:  2.45s, ---- Loss: 0.3135, Acc.: 0.8867, Val. Loss: 0.3912, Val. Acc.: 0.8850\n",
            "Epoch 79\n",
            "Validation loss decreased. Saved checkpoint for step 80: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-39\n",
            "Time:  2.46s, ---- Loss: 0.3127, Acc.: 0.8869, Val. Loss: 0.3911, Val. Acc.: 0.8852\n",
            "Epoch 80\n",
            "Validation loss decreased. Saved checkpoint for step 81: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-40\n",
            "Time:  2.48s, ---- Loss: 0.3145, Acc.: 0.8867, Val. Loss: 0.3907, Val. Acc.: 0.8850\n",
            "Epoch 81\n",
            "Validation loss decreased. Saved checkpoint for step 82: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-41\n",
            "Time:  2.47s, ---- Loss: 0.3120, Acc.: 0.8870, Val. Loss: 0.3906, Val. Acc.: 0.8851\n",
            "Epoch 82\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.33s, ---- Loss: 0.3134, Acc.: 0.8871, Val. Loss: 0.3915, Val. Acc.: 0.8855\n",
            "Epoch 83\n",
            "Validation loss decreased. Saved checkpoint for step 84: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-42\n",
            "Time:  2.45s, ---- Loss: 0.3142, Acc.: 0.8869, Val. Loss: 0.3903, Val. Acc.: 0.8855\n",
            "Epoch 84\n",
            "Validation loss decreased. Saved checkpoint for step 85: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-43\n",
            "Time:  2.45s, ---- Loss: 0.3123, Acc.: 0.8868, Val. Loss: 0.3897, Val. Acc.: 0.8861\n",
            "Epoch 85\n",
            "Validation loss decreased. Saved checkpoint for step 86: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-44\n",
            "Time:  2.47s, ---- Loss: 0.3117, Acc.: 0.8871, Val. Loss: 0.3896, Val. Acc.: 0.8857\n",
            "Epoch 86\n",
            "Validation loss decreased. Saved checkpoint for step 87: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-45\n",
            "Time:  2.45s, ---- Loss: 0.3147, Acc.: 0.8871, Val. Loss: 0.3892, Val. Acc.: 0.8859\n",
            "Epoch 87\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.33s, ---- Loss: 0.3134, Acc.: 0.8872, Val. Loss: 0.3895, Val. Acc.: 0.8859\n",
            "Epoch 88\n",
            "Loss did not decrease. Count = 2\n",
            "Time:  2.33s, ---- Loss: 0.3122, Acc.: 0.8872, Val. Loss: 0.3893, Val. Acc.: 0.8859\n",
            "Epoch 89\n",
            "Loss did not decrease. Count = 3\n",
            "Time:  2.33s, ---- Loss: 0.3129, Acc.: 0.8873, Val. Loss: 0.3894, Val. Acc.: 0.8859\n",
            "Epoch 90\n",
            "Loss did not decrease. Count = 4\n",
            "Time:  2.34s, ---- Loss: 0.3132, Acc.: 0.8872, Val. Loss: 0.3895, Val. Acc.: 0.8856\n",
            "Epoch 91\n",
            "Validation loss decreased. Saved checkpoint for step 92: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-46\n",
            "Time:  2.46s, ---- Loss: 0.3136, Acc.: 0.8874, Val. Loss: 0.3889, Val. Acc.: 0.8856\n",
            "Epoch 92\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.33s, ---- Loss: 0.3123, Acc.: 0.8873, Val. Loss: 0.3891, Val. Acc.: 0.8858\n",
            "Epoch 93\n",
            "Validation loss decreased. Saved checkpoint for step 94: models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-47\n",
            "Time:  2.45s, ---- Loss: 0.3111, Acc.: 0.8872, Val. Loss: 0.3889, Val. Acc.: 0.8858\n",
            "Epoch 94\n",
            "Loss did not decrease. Count = 1\n",
            "Time:  2.35s, ---- Loss: 0.3146, Acc.: 0.8874, Val. Loss: 0.3891, Val. Acc.: 0.8862\n",
            "Epoch 95\n",
            "Loss did not decrease. Count = 2\n",
            "Time:  2.33s, ---- Loss: 0.3139, Acc.: 0.8874, Val. Loss: 0.3894, Val. Acc.: 0.8858\n",
            "Epoch 96\n",
            "Loss did not decrease. Count = 3\n",
            "Time:  2.34s, ---- Loss: 0.3130, Acc.: 0.8878, Val. Loss: 0.3894, Val. Acc.: 0.8856\n",
            "Epoch 97\n",
            "Loss did not decrease. Count = 4\n",
            "Time:  2.33s, ---- Loss: 0.3119, Acc.: 0.8875, Val. Loss: 0.3892, Val. Acc.: 0.8859\n",
            "Epoch 98\n",
            "Loss did not decrease. Count = 5\n",
            "Time:  2.33s, ---- Loss: 0.3125, Acc.: 0.8876, Val. Loss: 0.3889, Val. Acc.: 0.8859\n",
            "Epoch 99\n",
            "Loss did not decrease. Count = 6\n",
            "Time:  2.34s, ---- Loss: 0.3123, Acc.: 0.8873, Val. Loss: 0.3889, Val. Acc.: 0.8861\n",
            "Saving at models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/hist.png\n",
            "Done in 3561.18s\n"
          ]
        }
      ],
      "source": [
        "# -------------- training loads parameters entered above  --------------\n",
        "proc = subprocess.Popen([\"python3\", \"train.py\", \"--test_mode\" , test_mode, \"--seed\", seed, \\\n",
        "                     \"--bayesian\", bayesian, \"--model_name\", model_name, \\\n",
        "                     \"--fine_tune\", fine_tune, \"--log_path\", log_path,\\\n",
        "                     \"--restore\", restore, \\\n",
        "                     \"--models_dir\", mdir, \\\n",
        "                     \"--cache_dir\", cache_dir, \\\n",
        "                     \"--fname\", fname, \\\n",
        "                     \"--DIR\", DIR, \\\n",
        "                     '--norm_data_name', norm_data_name, \\\n",
        "                     '--curves_folder', curves_folder,\\\n",
        "                     \"--c_0\", 'lcdm', \"--c_1\", 'fR', 'dgp', 'ds', 'rand', \\\n",
        "                     \"--save_ckpt\", save_ckpt, \\\n",
        "                     \"--im_depth\", im_depth, \"--im_width\", im_width, \"--im_channels\", im_channels, \\\n",
        "                     \"--swap_axes\", swap_axes, \\\n",
        "                     \"--sort_labels\", sort_labels, \\\n",
        "                     \"--add_noise\", add_noise, \"--add_shot\", add_shot, \"--add_sys\", add_sys,\"--add_cosvar\", add_cosvar, \\\n",
        "                     \"--sigma_curves\", sigma_curves, \\\n",
        "                     \"--sigma_curves_default\", sigma_curves_default, \\\n",
        "                     \"--save_processed_spectra\", save_processed_spectra, \\\n",
        "                     \"--sample_pace\", sample_pace,\\\n",
        "                     \"--n_noisy_samples\", n_noisy_samples, \\\n",
        "                     \"--rescale_curves\", rescale_curves, \\\n",
        "                     \"--val_size\", val_size, \\\n",
        "                     \"--z_bins\", z1,z2,z3,z4, \\\n",
        "                     \"--filters\", f1,f2,f3, \"--kernel_sizes\", k1,k2,k3, \"--strides\", s1,s2,s3, \"--pool_sizes\", p1,p2,p3, \"--strides_pooling\", sp1,sp2,sp3, \\\n",
        "                     \"--k_max\", k_max,\\\n",
        "                     \"--k_min\", k_min,\\\n",
        "                     \"--n_dense\", n_dense,\\\n",
        "                     \"--add_FT_dense\", add_FT_dense, \\\n",
        "                     \"--n_epochs\", n_epochs, \"--patience\", patience, \"--batch_size\", batch_size, \"--lr\", lr, \\\n",
        "                     \"--decay\", decay, \\\n",
        "                     \"--GPU\", GPU, \\\n",
        "                     \"--TPU\", TPU, \\\n",
        "                     \"--padding\", padding],\\\n",
        "                     stdout=subprocess.PIPE, \\\n",
        "                     stderr=subprocess.PIPE)\n",
        "\n",
        "with open(log_path, \"w\") as log_file:\n",
        "\n",
        "    stdout_thread = threading.Thread(target=handle_output, args=(proc.stdout, log_file))\n",
        "    stderr_thread = threading.Thread(target=handle_output, args=(proc.stderr, log_file))\n",
        "\n",
        "    stdout_thread.start()\n",
        "    stderr_thread.start()\n",
        "\n",
        "    stdout_thread.join()\n",
        "    stderr_thread.join()\n",
        "\n",
        "    proc.wait()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ."
      ],
      "metadata": {
        "id": "nbmvH6jH_uUk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Trained 5label model with no wcdm\""
      ],
      "metadata": {
        "id": "hipEDF7P_tul",
        "outputId": "1ef0a25d-a5de-49a0-a1cd-6fcd96bdbab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 0480c1d1c1] Trained 5label model with no wcdm\n",
            " 19 files changed, 23316 insertions(+)\n",
            " rewrite BaCoN-II(i)/__pycache__/data_generator.cpython-310.pyc (93%)\n",
            " rewrite BaCoN-II(i)/__pycache__/train.cpython-310.pyc (86%)\n",
            " create mode 100644 BaCoN-II(i)/models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5__log.txt\n",
            " create mode 100644 BaCoN-II(i)/models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/hist.png\n",
            " create mode 100644 BaCoN-II(i)/models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/checkpoint\n",
            " create mode 100644 BaCoN-II(i)/models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-46.data-00000-of-00001\n",
            " create mode 100644 BaCoN-II(i)/models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-46.index\n",
            " create mode 100644 BaCoN-II(i)/models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-47.data-00000-of-00001\n",
            " create mode 100644 BaCoN-II(i)/models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-47.index\n",
            " create mode 100644 BaCoN-II(i)/models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/history_accuracy.txt\n",
            " create mode 100644 BaCoN-II(i)/models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/history_loss.txt\n",
            " create mode 100644 BaCoN-II(i)/models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/history_val_accuracy.txt\n",
            " create mode 100644 BaCoN-II(i)/models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/history_val_loss.txt\n",
            " create mode 100644 BaCoN-II(i)/models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/idxs_train.txt\n",
            " create mode 100644 BaCoN-II(i)/models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/idxs_val.txt\n",
            " create mode 100644 BaCoN-II(i)/models/fivelabel_20k_equalexamples-randoms_kmin-0.0_kmax-2.5__log.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push"
      ],
      "metadata": {
        "id": "6CPAk-iZ_tb7",
        "outputId": "b2138638-e749-4f1b-e23b-aa5a813f905b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enumerating objects: 31, done.\n",
            "Counting objects:   3% (1/31)\rCounting objects:   6% (2/31)\rCounting objects:   9% (3/31)\rCounting objects:  12% (4/31)\rCounting objects:  16% (5/31)\rCounting objects:  19% (6/31)\rCounting objects:  22% (7/31)\rCounting objects:  25% (8/31)\rCounting objects:  29% (9/31)\rCounting objects:  32% (10/31)\rCounting objects:  35% (11/31)\rCounting objects:  38% (12/31)\rCounting objects:  41% (13/31)\rCounting objects:  45% (14/31)\rCounting objects:  48% (15/31)\rCounting objects:  51% (16/31)\rCounting objects:  54% (17/31)\rCounting objects:  58% (18/31)\rCounting objects:  61% (19/31)\rCounting objects:  64% (20/31)\rCounting objects:  67% (21/31)\rCounting objects:  70% (22/31)\rCounting objects:  74% (23/31)\rCounting objects:  77% (24/31)\rCounting objects:  80% (25/31)\rCounting objects:  83% (26/31)\rCounting objects:  87% (27/31)\rCounting objects:  90% (28/31)\rCounting objects:  93% (29/31)\rCounting objects:  96% (30/31)\rCounting objects: 100% (31/31)\rCounting objects: 100% (31/31), done.\n",
            "Delta compression using up to 12 threads\n",
            "Compressing objects:   3% (1/26)\rCompressing objects:   7% (2/26)\rCompressing objects:  11% (3/26)\rCompressing objects:  15% (4/26)\rCompressing objects:  19% (5/26)\rCompressing objects:  23% (6/26)\rCompressing objects:  26% (7/26)\rCompressing objects:  30% (8/26)\rCompressing objects:  34% (9/26)\rCompressing objects:  38% (10/26)\rCompressing objects:  42% (11/26)\rCompressing objects:  46% (12/26)\rCompressing objects:  50% (13/26)\rCompressing objects:  53% (14/26)\rCompressing objects:  57% (15/26)\rCompressing objects:  61% (16/26)\rCompressing objects:  65% (17/26)\rCompressing objects:  69% (18/26)\rCompressing objects:  73% (19/26)\rCompressing objects:  76% (20/26)\rCompressing objects:  80% (21/26)\rCompressing objects:  84% (22/26)\rCompressing objects:  88% (23/26)\rCompressing objects:  92% (24/26)\rCompressing objects:  96% (25/26)\rCompressing objects: 100% (26/26)\rCompressing objects: 100% (26/26), done.\n",
            "Writing objects:   3% (1/26)\rWriting objects:   7% (2/26)\rWriting objects:  11% (3/26)\rWriting objects:  15% (4/26)\rWriting objects:  19% (5/26)\rWriting objects:  23% (6/26)\rWriting objects:  26% (7/26)\rWriting objects:  30% (8/26)\rWriting objects:  34% (9/26)\rWriting objects:  38% (10/26)\rWriting objects:  42% (11/26)\rWriting objects:  46% (12/26)\rWriting objects:  50% (13/26)\rWriting objects:  53% (14/26)\rWriting objects:  57% (15/26)\rWriting objects:  61% (16/26)\rWriting objects:  69% (18/26)\rWriting objects:  73% (19/26)\rWriting objects:  76% (20/26)\rWriting objects:  80% (21/26)\rWriting objects:  84% (22/26)\rWriting objects:  88% (23/26)\rWriting objects:  92% (24/26)\rWriting objects:  96% (25/26)\rWriting objects: 100% (26/26)\rWriting objects: 100% (26/26), 286.56 KiB | 11.46 MiB/s, done.\n",
            "Total 26 (delta 6), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (6/6), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/arnava13/SeniorHonoursProject\n",
            "   d60b9c5f89..0480c1d1c1  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6v9Ql7T87Me"
      },
      "outputs": [],
      "source": [
        "ls models/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3sLuZopQ8Dy"
      },
      "source": [
        "## Run the test of the model\n",
        "- insert the path+name of the log file of the model\n",
        "- change the test directory\n",
        "- change the custom name that will be used for the confusion matrix\n",
        "\n",
        "The pdf file with the confusion matrix is saved in the model folder.\n",
        "\n",
        "#### I have been running this locally, whereas the train part is run on colab. I simply push the output models and I save the notebook before switching to local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtOfbiLLBpJ_",
        "outputId": "dff4d9c8-3b65-4f9a-f76f-256a674b463f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/arnav/.pyenv/versions/3.11.8/bin:/opt/homebrew/opt/llvm/bin:/Users/arnav/.pyenv/plugins/pyenv-virtualenv/shims:/Users/arnav/.pyenv/shims:/Users/arnav/.pyenv/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/usr/local/texlive/2024/bin/universal-darwin/\n"
          ]
        }
      ],
      "source": [
        "print(os.environ['PATH'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c98CGa3KJlyf"
      },
      "outputs": [],
      "source": [
        "!python test.py --log_path='models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binB/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binB_log.txt' \\\n",
        "  --TEST_DIR='data/test' --cm_name_custom='confusion'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ux1dgMxJXn"
      },
      "source": [
        "## Load modules for classification of a single spectrum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYGq6yP8Nvoq"
      },
      "source": [
        "### Functions to load trained models and data\n",
        "just execute the following cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gazkA01-CMCj"
      },
      "outputs": [],
      "source": [
        "def read_pre_trained_flags(args_dict):\n",
        "  '''\n",
        "  Read model options from logfile and return them in the form of FLAGS (the corresponding class is in utils)\n",
        "  '''\n",
        "  args = DummyFlags(args_dict)\n",
        "  FLAGS = get_flags(args.log_path)\n",
        "  if args.models_dir is not None:\n",
        "        print('Reading model from the directory %s' %args.models_dir)\n",
        "        FLAGS.models_dir = args.models_dir\n",
        "  return FLAGS\n",
        "\n",
        "\n",
        "def get_pre_trained_model(fname, n_classes, input_shape):\n",
        "  '''\n",
        "  returns pre-trained model.\n",
        "  Input:  - fname: string, folder where the trained model is stored\n",
        "          - n_classes: integer, number of output classes of the model\n",
        "          - input_shape:  tuple, input dimensions of the model.\n",
        "                          For pre-trained model in the paper use (100, 4)\n",
        "\n",
        "  '''\n",
        "\n",
        "  log_path = os.path.join('models', fname, fname+'_log.txt' )\n",
        "  models_dir = os.path.join('models', fname)\n",
        "\n",
        "  args_dict= {'log_path':log_path, 'models_dir': models_dir, }\n",
        "  FLAGS=read_pre_trained_flags(args_dict)\n",
        "\n",
        "  print('Input shape %s' %str(input_shape))\n",
        "\n",
        "\n",
        "  model_loaded =  load_model_for_test(FLAGS, input_shape, n_classes=n_classes,\n",
        "                                        generator=None, new_fname='')\n",
        "\n",
        "\n",
        "  return model_loaded\n",
        "\n",
        "\n",
        "\n",
        "def load_X(fname, sample_pace, i_max, norm_data,\n",
        "           add_noise=True, normalise=True, use_sample_pace=False, include_k=True):\n",
        "  '''\n",
        "  Generates one power spectrum X ready for classification.\n",
        "  Options:  fname: path to data\n",
        "            sample_pace: read one point every sample_pace. Useful to reduce data dimension\n",
        "            i_max: Index corresponding to the max k used\n",
        "            norm_data: normalization spectrum\n",
        "\n",
        "            add_noise: whether to add noise or not\n",
        "            normalise: normalise by Planck or not\n",
        "            use_sample_pace\n",
        "            include_k: return k, X or only X\n",
        "\n",
        "\n",
        "\n",
        "  '''\n",
        "\n",
        "  all = np.loadtxt(fname)\n",
        "  print('loaded data shape: %s' %str(all.shape))\n",
        "  if include_k:\n",
        "    k, X = all[:, 0], all[:, 1:]\n",
        "    print('k, X  shape: %s, %s' %(str(k.shape), str(X.shape)))\n",
        "  else:\n",
        "    X = all\n",
        "    k=np.arange(X.shape[0])\n",
        "    print('X  shape : %s' %( str(X.shape)))\n",
        "  if use_sample_pace:\n",
        "    k, X = k[::sample_pace], X[::sample_pace]\n",
        "  print('X  shape after sample pace:  %s' %( str(X.shape)))\n",
        "  k, X  = k[:i_max], X[:i_max]\n",
        "  print('X  shape after k max: %s' %( str(X.shape)))\n",
        "  if add_noise:\n",
        "    noise = np.random.normal(loc=0, scale=generate_noise(k,X, add_sys=True,add_shot=True,sigma_sys=5 ))\n",
        "    X = X+noise\n",
        "  if normalise:\n",
        "    planck = norm_data\n",
        "    print('planck data shape: %s' %str(planck.shape))\n",
        "    X = X/planck-1\n",
        "  if include_k:\n",
        "    return k, X\n",
        "  else:\n",
        "    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho00eqUjDVPp"
      },
      "source": [
        "## Code for getting confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ciV9qDIDXY7"
      },
      "outputs": [],
      "source": [
        "def sample_from_data(model,  my_Xs_norm,  my_min=np.zeros(5), my_max=np.ones(5),\n",
        "                     n_samples=50000, num_monte_carlo=100, th_prob=0.5,\n",
        "                     verbose=True):\n",
        "  '''\n",
        "  Given some normalized features my_Xs_norm and the model, compute mu and Sigma as in the paper eq. 2.8-2.9\n",
        "  and return samples from the probability distribuion eq. 2.10\n",
        "  '''\n",
        "  Al_error, Ep_error, mean = get_mu_Sigma(model, my_Xs_norm , num_monte_carlo=num_monte_carlo, th_prob=th_prob, verbose=verbose)\n",
        "  Sigma = (Al_error+ Ep_error).numpy()\n",
        "  MM = mean.numpy()\n",
        "  if verbose:\n",
        "    print('Sigma')\n",
        "    print(np.round(Sigma, 3))\n",
        "  samples, z_samples = sample_probas(Sigma, MM, my_min=my_min,my_max=my_max, n_samples=n_samples, verbose=verbose)\n",
        "\n",
        "  return samples, z_samples, Sigma, MM\n",
        "\n",
        "\n",
        "\n",
        "def get_mu_Sigma(model, my_Xs_norm, num_monte_carlo=100, th_prob=0.5, verbose=True):\n",
        "  ''''\n",
        "  Given some normalized features my_Xs_norm and the model, compute mu and Sigma as in the paper eq. 2.8-2.9\n",
        "  '''\n",
        "  if verbose:\n",
        "    print('Computing mu and sigma...')\n",
        "  sampled_probas, _, _ = my_predict(tf.expand_dims(my_Xs_norm, axis=0), model, num_monte_carlo=num_monte_carlo, th_prob=th_prob)\n",
        "  mean = tf.reduce_mean(sampled_probas[:,0,:], axis=0)\n",
        "  _, Al_error, Ep_error = get_err_variances(sampled_probas[:,0,:])\n",
        "  return Al_error, Ep_error, mean\n",
        "\n",
        "\n",
        "def sample_probas(Sigma, MM, my_min=np.zeros(5), my_max=np.ones(5), n_samples=50000, verbose=True):\n",
        "  '''\n",
        "  Given mu and Sigma as in the paper eq. 2.8-2.9, computes B, U as described in appendix B of the paper.\n",
        "  Then calls function to sample the prob. distribution (eq. 2.10)\n",
        "  '''\n",
        "  if verbose:\n",
        "    print('Sample probas call')\n",
        "  evals, evecs = np.linalg.eig(Sigma)\n",
        "  B = evecs\n",
        "  U = B.T @ Sigma @  B\n",
        "  if verbose:\n",
        "    print('U')\n",
        "    print(np.round(U, 3))\n",
        "  n_dims=U.shape[0]-1\n",
        "  n_dims=U.shape[0]-1\n",
        "  ind_min = np.argmin(np.diag(U))\n",
        "  if verbose:\n",
        "    print('Index of min eigenvalue is: %s' %ind_min)\n",
        "    print('Min eigenvalue is: %s' %np.min(np.diag(U)))\n",
        "  UU = np.array([np.sqrt(U[i,i])  for i in range(U.shape[0]) if i!=ind_min ] )\n",
        "  if verbose:\n",
        "    print('U has %s elements ' %str(UU.shape[0]))\n",
        "  samples, z_samples = get_samples(UU, B, MM, my_min=my_min, my_max=my_max, ind_min=ind_min, n_dims=n_dims, n_samples=n_samples, verbose=verbose)\n",
        "  return samples, z_samples\n",
        "\n",
        "\n",
        "def get_samples(UU, B, MM, my_min=np.zeros(5), my_max=np.ones(5), ind_min=-1, n_dims=4, n_samples=50000, verbose=True):\n",
        "  '''\n",
        "  samples eq. 2.10 as described in appendix B\n",
        "  '''\n",
        "  if verbose:\n",
        "    print('Sampling...')\n",
        "  samples = np.zeros((0, n_dims+1))\n",
        "  z_samples = np.zeros((0, n_dims))\n",
        "  while samples.shape[0] < n_samples:\n",
        "    s = np.random.multivariate_normal( np.zeros(n_dims), np.diag(UU**2), size=(n_samples,))\n",
        "    accepted = s[(np.min(X_val_batch(s, B, MM, null_ind=ind_min)-my_min, axis=1) >= 0) & (np.max(X_val_batch(s, B, MM, null_ind=ind_min) - my_max, axis=1) <= 0)]\n",
        "    samples = np.concatenate((samples, X_val_batch(accepted, B, MM, null_ind=ind_min)), axis=0)\n",
        "    z_samples = np.concatenate((z_samples, accepted), axis=0)\n",
        "  samples = samples[:n_samples, :]\n",
        "  z_samples = z_samples[:n_samples, :]\n",
        "  if verbose:\n",
        "    print('Done.')\n",
        "  return samples, z_samples\n",
        "\n",
        "\n",
        "\n",
        "def X_val_batch(Z, B, MM, null_ind):\n",
        "  return np.array([ X_val(Z[i], B, MM, null_ind=null_ind,) for i in range(Z.shape[0])])\n",
        "\n",
        "def Z(X, B, MM):\n",
        "  '''  X must be 5-d '''\n",
        "  return B.T @ (X-MM)\n",
        "\n",
        "def X_val(Z, B, MM, null_ind=-1, verbose=False):\n",
        "  '''  Z must be 4-d , MM must be 5-d '''\n",
        "  if verbose:\n",
        "    print(Z)\n",
        "  Z = np.insert(Z, null_ind, 0)\n",
        "  if verbose:\n",
        "    print(Z)\n",
        "  return B @ Z+ MM\n",
        "\n",
        "\n",
        "def get_err_variances(prob_k, AlEp_split=True):\n",
        "  '''\n",
        "  Computes aleatoric and epistemic uncertainty from MC samples from the net. weights\n",
        "  '''\n",
        "  num_samples = prob_k.shape[0]\n",
        "  prob_mean = tf.reduce_mean(prob_k, axis=0)\n",
        "\n",
        "  diag_probs = tf.stack([np.diag(prob_k[i,:]) for i in range(num_samples)], axis=0)  # Diagonal matrix whose diagonal elements are the output of the network for the ith sample\n",
        "  outer_products_Al = tf.stack([tf.tensordot(prob_k[i,:], prob_k[i,:], axes =0) for i in range(num_samples)], axis=0) # Outer self-product of the predictive vectors for the ith sample used in computing Al uncertainty\n",
        "  outer_products_Ep = tf.stack([tf.tensordot(prob_k[i,:] - prob_mean, prob_k[i,:] - prob_mean, axes =0) for i in range(num_samples)], axis=0) # Outer self-product between the difference of the predictive vector for ith sampl and average prediction\n",
        "\n",
        "  Al_error = tf.reduce_mean(diag_probs - outer_products_Al, axis=0) # Average over all the samples to compute aleotoric uncertainty\n",
        "  Ep_error = tf.reduce_mean(outer_products_Ep, axis=0) # Average over all the samples to compute epistemic uncertainty\n",
        "\n",
        "  tot_error = Al_error + Ep_error # Combine to compute total uncertainty\n",
        "  confidences = tf.linalg.diag_part(tot_error).numpy()\n",
        "\n",
        "  if AlEp_split:\n",
        "    return confidences, Al_error, Ep_error\n",
        "  else:\n",
        "    return confidences\n",
        "\n",
        "\n",
        "\n",
        "def get_P_from_samples(samples, th_prob=0.5 ):\n",
        "  '''\n",
        "  Given samples from eq. 2.10, computes the probabilities as in eq. 2.11\n",
        "  '''\n",
        "  Ntot = samples.shape[0] #-unclassified\n",
        "  n_dims  = samples.shape[-1]\n",
        "  all_preds = np.array([predict_bayes_label(sample, th_prob=th_prob) for sample in samples])\n",
        "  unclassified=all_preds[all_preds==99].shape[0]\n",
        "\n",
        "  n_in_class = np.array([all_preds[all_preds==k].shape[0] for k in range(n_dims)])\n",
        "\n",
        "  #print('Ntot: %s  ' %Ntot)\n",
        "  p_uncl=unclassified/Ntot\n",
        "  #print('%s unclassified examples ' %unclassified)\n",
        "  #print('P(unclassified)=%s ' %p_uncl)\n",
        "  pp=n_in_class/Ntot\n",
        "  #print(pp)\n",
        "  #print(p_uncl)\n",
        "\n",
        "  return np.append(pp, p_uncl)\n",
        "\n",
        "\n",
        "def get_all_probas(X, model_loaded,\n",
        "                   n_classes=5,\n",
        "                   num_monte_carlo=100,\n",
        "                   th_prob=0.5,\n",
        "                   n_samples=10000,\n",
        "                   verbose=True):\n",
        "    '''\n",
        "    Given features X, predicts the labels with MC samples from the net,\n",
        "    computes mu and sigma, samples from the prob distribution\n",
        "    and computes P.\n",
        "    Returns result in the form of a dictionary\n",
        "    '''\n",
        "\n",
        "    # Gaussian approx\n",
        "    samples, z_samples, Sigma, MM = sample_from_data(model_loaded,\n",
        "                                                 my_Xs_norm=X,\n",
        "                                                 my_min=np.zeros(n_classes), my_max=np.ones(n_classes),\n",
        "                                                 n_samples=n_samples,\n",
        "                                                 num_monte_carlo=num_monte_carlo,\n",
        "                                                 th_prob=th_prob, verbose=verbose)\n",
        "\n",
        "    P = get_P_from_samples(samples, th_prob=th_prob)\n",
        "    if verbose:\n",
        "      print('P: %s ' %str(P))\n",
        "      print('P sum to %s' %P.sum())\n",
        "    if verbose:\n",
        "      print('mu  sum to %s' %MM.sum())\n",
        "\n",
        "\n",
        "    # Result\n",
        "    res = {'samples':samples, 'P':P, 'z_samples': z_samples, 'Sigma': Sigma, 'MM': MM,}\n",
        "    return res\n",
        "\n",
        "\n",
        "def plot_hist_1D(samples, MM, th_prob=0.5,\n",
        "                  P=None, inv_labels_dict=None,):\n",
        "  '''\n",
        "  Plots histogram of gauss samples for each class\n",
        "  '''\n",
        "\n",
        "  n_dims = samples.shape[-1]-1\n",
        "  fig, axs = plt.subplots(1, n_dims+1, sharey=True, sharex=False,figsize=(25,5))\n",
        "\n",
        "  t_str=''\n",
        "  for k in range(n_dims+1):\n",
        "\n",
        "    axs[k].hist(samples.T[k], bins=15, color = \"lightgray\",lw=0, density=True, label='Gauss samples')\n",
        "    axs[k].text(0.1, 0.9, '$\\mu = $' + '$'+ str(MM[k])+'$', fontsize = 16 , transform=axs[k].transAxes);\n",
        "    if P is  not None:\n",
        "      axs[k].text(0.1, 0.8, '$ P = $' + '$'+ str(P[k])+'$', fontsize = 16 , transform=axs[k].transAxes);\n",
        "      if k==0:\n",
        "        t_str+= 'P unclassified: %s \\n' %str(np.round(P[-1], 2) )\n",
        "    axs[k].set_xlim(0,1)\n",
        "    axs[k].set_ylim(0,10)\n",
        "\n",
        "    if inv_labels_dict is not None:\n",
        "      axs[k].set_title(inv_labels_dict[k])\n",
        "\n",
        "\n",
        "    axs[k].legend(loc='lower right')\n",
        "\n",
        "  axs[0].set_ylabel('P')\n",
        "  fig.suptitle(t_str)\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKo2vnj_Nzo0"
      },
      "source": [
        "## Load models\n",
        "\n",
        "Here we load the pre trained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YdVmL6YItLH"
      },
      "outputs": [],
      "source": [
        "# Encoding of the labels for the two networks. Can be found in the log file of the training.\n",
        "# Here it is easier to define it by hand.\n",
        "\n",
        "inv_labels_dict_6={'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}\n",
        "inv_labels_dict_2={0:'lcdm', 1:'non-LCDM'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9pyiC969KoE"
      },
      "source": [
        "change the model name (first parameter) in the following function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHuW_fpxJmvc"
      },
      "outputs": [],
      "source": [
        "model_6 = get_pre_trained_model('ds_ee2_1k_equalweight_randoms_kmax_5', 6, (100, 4) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WV6117ZOqdF"
      },
      "source": [
        "## Loading a trained network and applying it to a single file\n",
        "\n",
        "Template for loading spectra. Suppose you saved the spectrum in\n",
        "'data/example_spectra/my_ex.txt' . The spectrum is in 4 redshift bins and with 100 points between 0.01 - 2.5 in k . We normalize by the reference planck spectrum and add gaussian noise, then pass it to the five-label network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I0W-Yd0YERw"
      },
      "source": [
        "Load normalization data first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FirJ3GM5YDgl"
      },
      "outputs": [],
      "source": [
        "sample_pace=4 # the planck data were generated with 500 points up to k=10 . We need 1 every 4 points, and to cut to k_max=5.0\n",
        "i_max=200 #for kmax = 5\n",
        "\n",
        "planck = np.loadtxt('data/normalisation/planck_ee2.txt')[:, 1:][::sample_pace][:i_max]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S0HfNLeLsiv"
      },
      "outputs": [],
      "source": [
        "k, X_ds = load_X('data/ds_ee2_test_1k/ds/976.txt',\n",
        "                 sample_pace=sample_pace, i_max=200,\n",
        "                 norm_data=planck,\n",
        "                 add_noise=False, normalise=True, use_sample_pace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_01rSvQTjrT"
      },
      "source": [
        "Let's look at the input features as seen by the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhtjvSiCTiux"
      },
      "outputs": [],
      "source": [
        "plt.plot(k, X_ds);\n",
        "plt.xscale('log');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZgUlHtZrvG7"
      },
      "source": [
        "## Classify your spectra and compute confidence\n",
        "### (for Bayesian networks only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKNL6te3ruT0"
      },
      "outputs": [],
      "source": [
        "res_ds = get_all_probas(X_ds,\n",
        "                        model_5,\n",
        "                        n_classes=6,\n",
        "                        num_monte_carlo=10,\n",
        "                        th_prob=0.5,\n",
        "                        n_samples=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVXx5ejLsG-i"
      },
      "outputs": [],
      "source": [
        "print('P-non-LCDM=%s' %(np.delete(res_ds['P'], [2]).sum()))\n",
        "plot_hist_1D(res_ds['samples'],\n",
        "             res_ds['MM'],\n",
        "             th_prob=0.5,\n",
        "             P=res_ds['P'],\n",
        "             inv_labels_dict=inv_labels_dict_5)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}