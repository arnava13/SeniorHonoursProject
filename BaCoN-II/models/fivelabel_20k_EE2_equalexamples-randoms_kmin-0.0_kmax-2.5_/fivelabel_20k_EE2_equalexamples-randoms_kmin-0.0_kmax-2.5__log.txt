
 -------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune False
one_vs_all False
c_0 ['lcdm']
c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']
dataset_balanced False
include_last False
log_path models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5__log.txt
restore False
fname fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_
model_name custom
my_path None
DIR data/train
TEST_DIR data/test
models_dir models/
save_ckpt True
out_path_overwrite False
curves_folder data/curve_files_sys/theory_error
save_processed_spectra False
cache_dir False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
norm_data_name /planck_ee2.txt
normalization stdcosmo
sample_pace 4
k_max 2.5
k_min 0.0
i_max None
i_min None
add_noise True
n_noisy_samples 10
add_shot False
add_sys True
add_cosvar True
sigma_sys None
sys_scaled None
sys_factor None
sys_max None
sigma_curves 0.05
sigma_curves_default 0.05
rescale_curves uniform
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable False
unfreeze False
lr 0.05
drop 0.5
n_epochs 50
val_size 0.15
test_size 0.0
batch_size 2000
patience 20
GPU True
TPU False
decay 0.9
BatchNorm True
padding valid
shuffle True

------------ CREATING DATA GENERATORS ------------
labels : ['dgp', 'fr', 'lcdm', 'rand', 'wcdm']
Labels encoding: 
{'dgp': 0, 'fr': 1, 'lcdm': 2, 'rand': 3, 'wcdm': 4}
n_labels : 5
dgp - 20000 training examples
fr - 20000 training examples
lcdm - 20000 training examples
rand - 20000 training examples
wcdm - 20000 training examples

N. of data files: 20000
get_all_indexes labels dict: {'dgp': 0, 'fr': 1, 'lcdm': 2, 'rand': 3, 'wcdm': 4}
create_generators n_labels: 5
create_generators n_labels_eff: 5
create_generators len_c1: 1
Check for no duplicates in test: (0=ok):
0.0
Check for no duplicates in val: (0=ok):
0
N of indexes in training set: 17000
N of indexes in validation set: 3000
N of indexes in test set: 0
Check - total per class: 20000
--create_generators, train indexes
batch_size: 2000
- Cut sample
bs: 2000
N_labels: 5
N_noise: 10
len_c1: 1
Train index length: 17000
--create_generators, validation indexes
- Cut sample
bs: 2000
N_labels: 5
N_noise: 10
len_c1: 1
Val index length: 3000
len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 17000, 2000, 5, 10

--DataSet Train
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 100
Closest k to k_max is 2.539859
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (100, 1)
Final i_max used is 100
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 5
N. n_classes in output: 5
LABELS: ['dgp', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 17000
n_indexes (n of file IDs read for each batch): 40
batch size: 2000
n_batches : 425
For each batch we read 40 file IDs
For each file ID we have 5 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 2000 training examples
Input batch size: 2000
N of batches to cover all file IDs: 425
len(fname_list), batch_size, n_noisy_samples, n_batches: 85000, 2000, 10, 425

--DataSet Validation
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 100
Closest k to k_max is 2.539859
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (100, 1)
Final i_max used is 100
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 5
N. n_classes in output: 5
LABELS: ['dgp', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 3000
n_indexes (n of file IDs read for each batch): 40
batch size: 2000
n_batches : 75
For each batch we read 40 file IDs
For each file ID we have 5 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 2000 training examples
Input batch size: 2000
N of batches to cover all file IDs: 75
len(fname_list), batch_size, n_noisy_samples, n_batches: 15000, 2000, 10, 75
------------ DONE ------------

------------ BUILDING MODEL ------------
Input shape (100, 4)
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 100, 4)]          0         
                                                                 
 conv1d_flipout (Conv1DFlip  (None, 46, 8)             648       
 out)                                                            
                                                                 
 max_pooling1d (MaxPooling1  (None, 23, 8)             0         
 D)                                                              
                                                                 
 batch_normalization (Batch  (None, 23, 8)             32        
 Normalization)                                                  
                                                                 
 conv1d_flipout_1 (Conv1DFl  (None, 10, 16)            1296      
 ipout)                                                          
                                                                 
 max_pooling1d_1 (MaxPoolin  (None, 9, 16)             0         
 g1D)                                                            
                                                                 
 batch_normalization_1 (Bat  (None, 9, 16)             64        
 chNormalization)                                                
                                                                 
 conv1d_flipout_2 (Conv1DFl  (None, 8, 32)             2080      
 ipout)                                                          
                                                                 
 batch_normalization_2 (Bat  (None, 8, 32)             128       
 chNormalization)                                                
                                                                 
 global_average_pooling1d (  (None, 32)                0         
 GlobalAveragePooling1D)                                         
                                                                 
 dense_flipout (DenseFlipou  (None, 32)                2080      
 t)                                                              
                                                                 
 batch_normalization_3 (Bat  (None, 32)                128       
 chNormalization)                                                
                                                                 
 dense_flipout_1 (DenseFlip  (None, 5)                 325       
 out)                                                            
                                                                 
=================================================================
Total params: 6781 (26.49 KB)
Trainable params: 6605 (25.80 KB)
Non-trainable params: 176 (704.00 Byte)
_________________________________________________________________
None
Found GPU at: /device:GPU:0
------------ TRAINING ------------

Features shape: (2000, 100, 4)
Labels shape: (2000, 5)
Initializing checkpoint from scratch.
Epoch 0
Validation loss decreased. Saved checkpoint for step 1: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-1
Time:  34.20s, ---- Loss: 0.4328, Acc.: 0.7544, Val. Loss: 0.9865, Val. Acc.: 0.6812

Epoch 1
Validation loss decreased. Saved checkpoint for step 2: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-2
Time:  2.78s, ---- Loss: 0.3908, Acc.: 0.8624, Val. Loss: 0.8615, Val. Acc.: 0.7354

Epoch 2
Validation loss decreased. Saved checkpoint for step 3: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-3
Time:  2.74s, ---- Loss: 0.3488, Acc.: 0.8793, Val. Loss: 0.6831, Val. Acc.: 0.7792

Epoch 3
Validation loss decreased. Saved checkpoint for step 4: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-4
Time:  2.95s, ---- Loss: 0.3278, Acc.: 0.8873, Val. Loss: 0.6103, Val. Acc.: 0.7839

Epoch 4
Validation loss decreased. Saved checkpoint for step 5: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-5
Time:  2.85s, ---- Loss: 0.3112, Acc.: 0.8909, Val. Loss: 0.4951, Val. Acc.: 0.8251

Epoch 5
Validation loss decreased. Saved checkpoint for step 6: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-6
Time:  2.78s, ---- Loss: 0.2996, Acc.: 0.8931, Val. Loss: 0.4787, Val. Acc.: 0.8326

Epoch 6
Validation loss decreased. Saved checkpoint for step 7: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-7
Time:  2.73s, ---- Loss: 0.2964, Acc.: 0.8956, Val. Loss: 0.4781, Val. Acc.: 0.8384

Epoch 7
Validation loss decreased. Saved checkpoint for step 8: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-8
Time:  2.79s, ---- Loss: 0.2941, Acc.: 0.8979, Val. Loss: 0.4423, Val. Acc.: 0.8498

Epoch 8
Loss did not decrease. Count = 1
Time:  2.69s, ---- Loss: 0.2985, Acc.: 0.8993, Val. Loss: 0.4885, Val. Acc.: 0.8321

Epoch 9
Loss did not decrease. Count = 2
Time:  2.62s, ---- Loss: 0.2876, Acc.: 0.9009, Val. Loss: 0.4555, Val. Acc.: 0.8476

Epoch 10
Validation loss decreased. Saved checkpoint for step 11: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-9
Time:  2.75s, ---- Loss: 0.2854, Acc.: 0.9024, Val. Loss: 0.4117, Val. Acc.: 0.8628

Epoch 11
Loss did not decrease. Count = 1
Time:  2.70s, ---- Loss: 0.2803, Acc.: 0.9033, Val. Loss: 0.4199, Val. Acc.: 0.8599

Epoch 12
Loss did not decrease. Count = 2
Time:  2.65s, ---- Loss: 0.2799, Acc.: 0.9042, Val. Loss: 0.4175, Val. Acc.: 0.8646

Epoch 13
Validation loss decreased. Saved checkpoint for step 14: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-10
Time:  2.76s, ---- Loss: 0.2734, Acc.: 0.9050, Val. Loss: 0.3886, Val. Acc.: 0.8745

Epoch 14
Loss did not decrease. Count = 1
Time:  2.61s, ---- Loss: 0.2704, Acc.: 0.9057, Val. Loss: 0.4017, Val. Acc.: 0.8711

Epoch 15
Validation loss decreased. Saved checkpoint for step 16: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-11
Time:  2.77s, ---- Loss: 0.2724, Acc.: 0.9063, Val. Loss: 0.3605, Val. Acc.: 0.8878

Epoch 16
Validation loss decreased. Saved checkpoint for step 17: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-12
Time:  2.78s, ---- Loss: 0.2683, Acc.: 0.9069, Val. Loss: 0.3483, Val. Acc.: 0.8932

Epoch 17
Validation loss decreased. Saved checkpoint for step 18: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-13
Time:  2.77s, ---- Loss: 0.2634, Acc.: 0.9075, Val. Loss: 0.3383, Val. Acc.: 0.8971

Epoch 18
Validation loss decreased. Saved checkpoint for step 19: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-14
Time:  2.73s, ---- Loss: 0.2610, Acc.: 0.9081, Val. Loss: 0.3331, Val. Acc.: 0.8994

Epoch 19
Validation loss decreased. Saved checkpoint for step 20: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-15
Time:  2.77s, ---- Loss: 0.2608, Acc.: 0.9083, Val. Loss: 0.3272, Val. Acc.: 0.9021

Epoch 20
Validation loss decreased. Saved checkpoint for step 21: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-16
Time:  2.78s, ---- Loss: 0.2565, Acc.: 0.9089, Val. Loss: 0.3231, Val. Acc.: 0.9030

Epoch 21
Loss did not decrease. Count = 1
Time:  2.66s, ---- Loss: 0.2574, Acc.: 0.9092, Val. Loss: 0.3273, Val. Acc.: 0.9018

Epoch 22
Validation loss decreased. Saved checkpoint for step 23: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-17
Time:  2.73s, ---- Loss: 0.2536, Acc.: 0.9098, Val. Loss: 0.3227, Val. Acc.: 0.9037

Epoch 23
Loss did not decrease. Count = 1
Time:  2.65s, ---- Loss: 0.2565, Acc.: 0.9101, Val. Loss: 0.3294, Val. Acc.: 0.9012

Epoch 24
Loss did not decrease. Count = 2
Time:  2.74s, ---- Loss: 0.2535, Acc.: 0.9102, Val. Loss: 0.3274, Val. Acc.: 0.9022

Epoch 25
Validation loss decreased. Saved checkpoint for step 26: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-18
Time:  2.81s, ---- Loss: 0.2555, Acc.: 0.9106, Val. Loss: 0.3201, Val. Acc.: 0.9050

Epoch 26
Validation loss decreased. Saved checkpoint for step 27: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-19
Time:  2.77s, ---- Loss: 0.2555, Acc.: 0.9109, Val. Loss: 0.3182, Val. Acc.: 0.9059

Epoch 27
Loss did not decrease. Count = 1
Time:  2.62s, ---- Loss: 0.2547, Acc.: 0.9112, Val. Loss: 0.3217, Val. Acc.: 0.9044

Epoch 28
Loss did not decrease. Count = 2
Time:  2.74s, ---- Loss: 0.2513, Acc.: 0.9113, Val. Loss: 0.3182, Val. Acc.: 0.9060

Epoch 29
Validation loss decreased. Saved checkpoint for step 30: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-20
Time:  2.83s, ---- Loss: 0.2536, Acc.: 0.9115, Val. Loss: 0.3164, Val. Acc.: 0.9067

Epoch 30
Validation loss decreased. Saved checkpoint for step 31: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-21
Time:  2.83s, ---- Loss: 0.2504, Acc.: 0.9118, Val. Loss: 0.3156, Val. Acc.: 0.9068

Epoch 31
Validation loss decreased. Saved checkpoint for step 32: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-22
Time:  2.84s, ---- Loss: 0.2532, Acc.: 0.9118, Val. Loss: 0.3133, Val. Acc.: 0.9078

Epoch 32
Validation loss decreased. Saved checkpoint for step 33: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-23
Time:  2.96s, ---- Loss: 0.2498, Acc.: 0.9120, Val. Loss: 0.3123, Val. Acc.: 0.9086

Epoch 33
Loss did not decrease. Count = 1
Time:  2.77s, ---- Loss: 0.2502, Acc.: 0.9120, Val. Loss: 0.3123, Val. Acc.: 0.9082

Epoch 34
Validation loss decreased. Saved checkpoint for step 35: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-24
Time:  2.85s, ---- Loss: 0.2504, Acc.: 0.9122, Val. Loss: 0.3122, Val. Acc.: 0.9084

Epoch 35
Validation loss decreased. Saved checkpoint for step 36: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-25
Time:  2.76s, ---- Loss: 0.2497, Acc.: 0.9124, Val. Loss: 0.3114, Val. Acc.: 0.9083

Epoch 36
Loss did not decrease. Count = 1
Time:  2.67s, ---- Loss: 0.2523, Acc.: 0.9123, Val. Loss: 0.3116, Val. Acc.: 0.9081

Epoch 37
Loss did not decrease. Count = 2
Time:  2.67s, ---- Loss: 0.2515, Acc.: 0.9127, Val. Loss: 0.3117, Val. Acc.: 0.9082

Epoch 38
Validation loss decreased. Saved checkpoint for step 39: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-26
Time:  2.82s, ---- Loss: 0.2522, Acc.: 0.9126, Val. Loss: 0.3113, Val. Acc.: 0.9083

Epoch 39
Validation loss decreased. Saved checkpoint for step 40: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-27
Time:  2.90s, ---- Loss: 0.2487, Acc.: 0.9127, Val. Loss: 0.3111, Val. Acc.: 0.9086

Epoch 40
Loss did not decrease. Count = 1
Time:  2.92s, ---- Loss: 0.2483, Acc.: 0.9128, Val. Loss: 0.3121, Val. Acc.: 0.9080

Epoch 41
Loss did not decrease. Count = 2
Time:  2.75s, ---- Loss: 0.2499, Acc.: 0.9127, Val. Loss: 0.3114, Val. Acc.: 0.9084

Epoch 42
Loss did not decrease. Count = 3
Time:  2.72s, ---- Loss: 0.2503, Acc.: 0.9129, Val. Loss: 0.3119, Val. Acc.: 0.9087

Epoch 43
Loss did not decrease. Count = 4
Time:  2.69s, ---- Loss: 0.2499, Acc.: 0.9131, Val. Loss: 0.3114, Val. Acc.: 0.9086

Epoch 44
Validation loss decreased. Saved checkpoint for step 45: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-28
Time:  2.82s, ---- Loss: 0.2506, Acc.: 0.9129, Val. Loss: 0.3109, Val. Acc.: 0.9088

Epoch 45
Validation loss decreased. Saved checkpoint for step 46: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-29
Time:  2.75s, ---- Loss: 0.2488, Acc.: 0.9130, Val. Loss: 0.3109, Val. Acc.: 0.9091

Epoch 46
Validation loss decreased. Saved checkpoint for step 47: models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-30
Time:  2.75s, ---- Loss: 0.2524, Acc.: 0.9130, Val. Loss: 0.3107, Val. Acc.: 0.9087

Epoch 47
Loss did not decrease. Count = 1
Time:  2.66s, ---- Loss: 0.2488, Acc.: 0.9131, Val. Loss: 0.3108, Val. Acc.: 0.9090

Epoch 48
Loss did not decrease. Count = 2
Time:  2.65s, ---- Loss: 0.2515, Acc.: 0.9133, Val. Loss: 0.3108, Val. Acc.: 0.9088

Epoch 49
Loss did not decrease. Count = 3
Time:  2.66s, ---- Loss: 0.2514, Acc.: 0.9131, Val. Loss: 0.3108, Val. Acc.: 0.9087

Saving at models/fivelabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/hist.png
Done in 3337.51s
