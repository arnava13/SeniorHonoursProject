
 -------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune False
one_vs_all False
c_0 ['lcdm']
c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']
dataset_balanced False
include_last False
log_path models/ds_ee2_1k_equalweight_randoms_kmax_5_log.txt
restore False
fname ds_ee2_1k_equalweight_randoms_kmax_5
model_name custom
my_path None
DIR data/ds_ee2_train_1k
TEST_DIR data/test_data/
models_dir models/
save_ckpt True
out_path_overwrite False
curves_folder data/curve_files_sys/theory_error
save_processed_spectra False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
norm_data_name /planck_ee2.txt
normalization stdcosmo
sample_pace 4
k_max 5.0
i_max None
add_noise True
n_noisy_samples 10
add_shot False
add_sys True
add_cosvar True
sigma_sys None
sys_scaled None
sys_factor None
sys_max None
sigma_curves 0.05
sigma_curves_default 0.05
rescale_curves uniform
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable False
unfreeze False
lr 0.01
drop 0.5
n_epochs 50
val_size 0.15
test_size 0.0
batch_size 3000
patience 50
GPU True
decay 0.95
BatchNorm True

------------ CREATING DATA GENERATORS ------------
labels : ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
Labels encoding: 
{'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
n_labels : 6
dgp - 1000 training examples
ds - 1000 training examples
fr - 1000 training examples
lcdm - 1000 training examples
rand - 1000 training examples
wcdm - 1000 training examples

N. of data files: 1000
get_all_indexes labels dict: {'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
create_generators n_labels: 6
create_generators n_labels_eff: 6
create_generators len_c1: 1
Check for no duplicates in test: (0=ok):
0.0
Check for no duplicates in val: (0=ok):
0
N of files in training set: 850
N of files in validation set: 150
N of files in test set: 0
Check - total: 1000
--create_generators, train indexes
batch_size: 3000
- Cut sample
bs: 3000
N_labels: 6
N_noise: 10
len_c1: 1
Train index length: 850
--create_generators, validation indexes
- Cut sample
bs: 3000
N_labels: 6
N_noise: 10
len_c1: 1
Val index length: 150
len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 850, 3000, 6, 10

--DataGenerator Train
Data Generator Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 5.0
Corresponding i_max is 112
Closest k to k_max is 4.936132
New data dim: (112, 1)
Final i_max used is 112
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 850
n_indexes (n of file IDs read for each batch): 50
batch size: 3000
n_batches : 17
For each batch we read 50 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 3000 training examples
Input batch size: 3000
N of batches to cover all file IDs: 17

--DataGenerator Validation
Data Generator Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 5.0
Corresponding i_max is 112
Closest k to k_max is 4.936132
New data dim: (112, 1)
Final i_max used is 112
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 150
n_indexes (n of file IDs read for each batch): 50
batch size: 3000
n_batches : 3
For each batch we read 50 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 3000 training examples
Input batch size: 3000
N of batches to cover all file IDs: 3
------------ DONE ------------

------------ BUILDING MODEL ------------
Input shape (112, 4)
using 1D layers and 4 channels
Expected output dimension of layer conv1d_flipout: 52.0
Expected output dimension of layer max_pooling1d: 26.0
Expected output dimension of layer conv1d_flipout_1: 11.5
Expected output dimension of layer max_pooling1d_1: 10.5
Expected output dimension of layer conv1d_flipout_2: 9.5
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 112, 4)]          0         
                                                                 
 conv1d_flipout (Conv1DFlip  (None, 52, 8)             648       
 out)                                                            
                                                                 
 max_pooling1d (MaxPooling1  (None, 26, 8)             0         
 D)                                                              
                                                                 
 batch_normalization (Batch  (None, 26, 8)             32        
 Normalization)                                                  
                                                                 
 conv1d_flipout_1 (Conv1DFl  (None, 11, 16)            1296      
 ipout)                                                          
                                                                 
 max_pooling1d_1 (MaxPoolin  (None, 10, 16)            0         
 g1D)                                                            
                                                                 
 batch_normalization_1 (Bat  (None, 10, 16)            64        
 chNormalization)                                                
                                                                 
 conv1d_flipout_2 (Conv1DFl  (None, 9, 32)             2080      
 ipout)                                                          
                                                                 
 batch_normalization_2 (Bat  (None, 9, 32)             128       
 chNormalization)                                                
                                                                 
 global_average_pooling1d (  (None, 32)                0         
 GlobalAveragePooling1D)                                         
                                                                 
 dense_flipout (DenseFlipou  (None, 32)                2080      
 t)                                                              
                                                                 
 batch_normalization_3 (Bat  (None, 32)                128       
 chNormalization)                                                
                                                                 
 dense_flipout_1 (DenseFlip  (None, 6)                 390       
 out)                                                            
                                                                 
=================================================================
Total params: 6846 (26.74 KB)
Trainable params: 6670 (26.05 KB)
Non-trainable params: 176 (704.00 Byte)
_________________________________________________________________
None
Found GPU at: /device:GPU:0
------------ TRAINING ------------

Features shape: (3000, 112, 4)
Labels shape: (3000, 6)
Initializing checkpoint from scratch.
Epoch 0
Validation loss decreased. Saved checkpoint for step 1: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-1
Time:  87.03s, ---- Loss: 1.7528, Acc.: 0.2543, Val. Loss: 2.6896, Val. Acc.: 0.1722

Epoch 1
Loss did not decrease. Count = 1
Time:  78.80s, ---- Loss: 1.4807, Acc.: 0.3926, Val. Loss: 2.7202, Val. Acc.: 0.1766

Epoch 2
Loss did not decrease. Count = 2
Time:  78.29s, ---- Loss: 1.3407, Acc.: 0.5011, Val. Loss: 2.9207, Val. Acc.: 0.1724

Epoch 3
Loss did not decrease. Count = 3
Time:  78.69s, ---- Loss: 1.2714, Acc.: 0.5370, Val. Loss: 2.7673, Val. Acc.: 0.1770

Epoch 4
Validation loss decreased. Saved checkpoint for step 5: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-2
Time:  78.31s, ---- Loss: 1.2491, Acc.: 0.5565, Val. Loss: 2.6091, Val. Acc.: 0.2117

Epoch 5
Validation loss decreased. Saved checkpoint for step 6: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-3
Time:  78.81s, ---- Loss: 1.2255, Acc.: 0.5678, Val. Loss: 2.5802, Val. Acc.: 0.2307

Epoch 6
Loss did not decrease. Count = 1
Time:  78.46s, ---- Loss: 1.2186, Acc.: 0.5769, Val. Loss: 2.6528, Val. Acc.: 0.2263

Epoch 7
Loss did not decrease. Count = 2
Time:  78.38s, ---- Loss: 1.2085, Acc.: 0.5855, Val. Loss: 2.6613, Val. Acc.: 0.2600

Epoch 8
Loss did not decrease. Count = 3
Time:  78.51s, ---- Loss: 1.1876, Acc.: 0.5899, Val. Loss: 2.6446, Val. Acc.: 0.2713

Epoch 9
Validation loss decreased. Saved checkpoint for step 10: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-4
Time:  78.22s, ---- Loss: 1.1829, Acc.: 0.5931, Val. Loss: 2.5708, Val. Acc.: 0.3210

Epoch 10
Loss did not decrease. Count = 1
Time:  78.39s, ---- Loss: 1.1554, Acc.: 0.5968, Val. Loss: 2.5861, Val. Acc.: 0.3508

Epoch 11
Loss did not decrease. Count = 2
Time:  78.30s, ---- Loss: 1.1310, Acc.: 0.6027, Val. Loss: 2.5846, Val. Acc.: 0.3441

Epoch 12
Loss did not decrease. Count = 3
Time:  78.15s, ---- Loss: 1.1447, Acc.: 0.6055, Val. Loss: 2.5982, Val. Acc.: 0.3591

Epoch 13
Validation loss decreased. Saved checkpoint for step 14: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-5
Time:  77.94s, ---- Loss: 1.1188, Acc.: 0.6072, Val. Loss: 2.5044, Val. Acc.: 0.3797

Epoch 14
Validation loss decreased. Saved checkpoint for step 15: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-6
Time:  78.49s, ---- Loss: 1.0942, Acc.: 0.6097, Val. Loss: 2.3823, Val. Acc.: 0.4098

Epoch 15
Validation loss decreased. Saved checkpoint for step 16: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-7
Time:  77.92s, ---- Loss: 1.0763, Acc.: 0.6167, Val. Loss: 2.3532, Val. Acc.: 0.4204

Epoch 16
Validation loss decreased. Saved checkpoint for step 17: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-8
Time:  78.68s, ---- Loss: 1.1048, Acc.: 0.6180, Val. Loss: 2.3121, Val. Acc.: 0.4509

Epoch 17
Validation loss decreased. Saved checkpoint for step 18: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-9
Time:  78.15s, ---- Loss: 1.0721, Acc.: 0.6213, Val. Loss: 2.2384, Val. Acc.: 0.4681

Epoch 18
Validation loss decreased. Saved checkpoint for step 19: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-10
Time:  77.65s, ---- Loss: 1.0669, Acc.: 0.6211, Val. Loss: 2.2196, Val. Acc.: 0.4877

Epoch 19
Validation loss decreased. Saved checkpoint for step 20: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-11
Time:  77.85s, ---- Loss: 1.0430, Acc.: 0.6274, Val. Loss: 2.1509, Val. Acc.: 0.5073

Epoch 20
Validation loss decreased. Saved checkpoint for step 21: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-12
Time:  77.76s, ---- Loss: 1.0687, Acc.: 0.6306, Val. Loss: 2.1001, Val. Acc.: 0.5242

Epoch 21
Validation loss decreased. Saved checkpoint for step 22: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-13
Time:  77.38s, ---- Loss: 1.0803, Acc.: 0.6315, Val. Loss: 2.0462, Val. Acc.: 0.5464

Epoch 22
Validation loss decreased. Saved checkpoint for step 23: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-14
Time:  77.11s, ---- Loss: 1.0469, Acc.: 0.6371, Val. Loss: 2.0013, Val. Acc.: 0.5601

Epoch 23
Validation loss decreased. Saved checkpoint for step 24: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-15
Time:  77.46s, ---- Loss: 1.0642, Acc.: 0.6357, Val. Loss: 1.9667, Val. Acc.: 0.5788

Epoch 24
Validation loss decreased. Saved checkpoint for step 25: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-16
Time:  76.90s, ---- Loss: 1.0552, Acc.: 0.6396, Val. Loss: 1.9509, Val. Acc.: 0.5847

Epoch 25
Validation loss decreased. Saved checkpoint for step 26: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-17
Time:  76.14s, ---- Loss: 1.0404, Acc.: 0.6416, Val. Loss: 1.9269, Val. Acc.: 0.5836

Epoch 26
Validation loss decreased. Saved checkpoint for step 27: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-18
Time:  77.62s, ---- Loss: 1.0286, Acc.: 0.6415, Val. Loss: 1.9189, Val. Acc.: 0.5898

Epoch 27
Loss did not decrease. Count = 1
Time:  77.61s, ---- Loss: 1.0478, Acc.: 0.6488, Val. Loss: 1.9406, Val. Acc.: 0.5729

Epoch 28
Validation loss decreased. Saved checkpoint for step 29: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-19
Time:  77.31s, ---- Loss: 1.0283, Acc.: 0.6501, Val. Loss: 1.8964, Val. Acc.: 0.5922

Epoch 29
Validation loss decreased. Saved checkpoint for step 30: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-20
Time:  77.41s, ---- Loss: 1.0225, Acc.: 0.6492, Val. Loss: 1.8899, Val. Acc.: 0.5962

Epoch 30
Validation loss decreased. Saved checkpoint for step 31: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-21
Time:  77.04s, ---- Loss: 1.0092, Acc.: 0.6527, Val. Loss: 1.8655, Val. Acc.: 0.5987

Epoch 31
Validation loss decreased. Saved checkpoint for step 32: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-22
Time:  77.45s, ---- Loss: 1.0291, Acc.: 0.6531, Val. Loss: 1.8476, Val. Acc.: 0.6074

Epoch 32
Validation loss decreased. Saved checkpoint for step 33: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-23
Time:  76.96s, ---- Loss: 1.0044, Acc.: 0.6562, Val. Loss: 1.8456, Val. Acc.: 0.6012

Epoch 33
Validation loss decreased. Saved checkpoint for step 34: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-24
Time:  77.94s, ---- Loss: 1.0366, Acc.: 0.6580, Val. Loss: 1.8353, Val. Acc.: 0.6033

Epoch 34
Loss did not decrease. Count = 1
Time:  76.71s, ---- Loss: 0.9960, Acc.: 0.6587, Val. Loss: 1.8625, Val. Acc.: 0.5918

Epoch 35
Loss did not decrease. Count = 2
Time:  76.48s, ---- Loss: 0.9958, Acc.: 0.6573, Val. Loss: 1.8558, Val. Acc.: 0.5878

Epoch 36
Loss did not decrease. Count = 3
Time:  76.33s, ---- Loss: 0.9834, Acc.: 0.6611, Val. Loss: 1.8668, Val. Acc.: 0.5912

Epoch 37
Validation loss decreased. Saved checkpoint for step 38: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-25
Time:  75.55s, ---- Loss: 1.0009, Acc.: 0.6614, Val. Loss: 1.8219, Val. Acc.: 0.6074

Epoch 38
Loss did not decrease. Count = 1
Time:  75.56s, ---- Loss: 0.9839, Acc.: 0.6619, Val. Loss: 1.8296, Val. Acc.: 0.5984

Epoch 39
Validation loss decreased. Saved checkpoint for step 40: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-26
Time:  76.79s, ---- Loss: 0.9996, Acc.: 0.6640, Val. Loss: 1.8103, Val. Acc.: 0.6084

Epoch 40
Loss did not decrease. Count = 1
Time:  76.26s, ---- Loss: 0.9711, Acc.: 0.6634, Val. Loss: 1.8154, Val. Acc.: 0.6079

Epoch 41
Validation loss decreased. Saved checkpoint for step 42: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-27
Time:  76.43s, ---- Loss: 0.9918, Acc.: 0.6647, Val. Loss: 1.7868, Val. Acc.: 0.6226

Epoch 42
Validation loss decreased. Saved checkpoint for step 43: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-28
Time:  76.47s, ---- Loss: 0.9882, Acc.: 0.6653, Val. Loss: 1.7780, Val. Acc.: 0.6264

Epoch 43
Loss did not decrease. Count = 1
Time:  76.85s, ---- Loss: 0.9544, Acc.: 0.6707, Val. Loss: 1.7831, Val. Acc.: 0.6287

Epoch 44
Validation loss decreased. Saved checkpoint for step 45: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-29
Time:  76.09s, ---- Loss: 0.9642, Acc.: 0.6671, Val. Loss: 1.7533, Val. Acc.: 0.6369

Epoch 45
Validation loss decreased. Saved checkpoint for step 46: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-30
Time:  76.54s, ---- Loss: 0.9896, Acc.: 0.6655, Val. Loss: 1.7450, Val. Acc.: 0.6412

Epoch 46
Loss did not decrease. Count = 1
Time:  76.15s, ---- Loss: 0.9762, Acc.: 0.6688, Val. Loss: 1.7605, Val. Acc.: 0.6322

Epoch 47
Loss did not decrease. Count = 2
Time:  76.48s, ---- Loss: 0.9500, Acc.: 0.6716, Val. Loss: 1.7539, Val. Acc.: 0.6380

Epoch 48
Loss did not decrease. Count = 3
Time:  76.19s, ---- Loss: 0.9826, Acc.: 0.6707, Val. Loss: 1.7600, Val. Acc.: 0.6343

Epoch 49
Loss did not decrease. Count = 4
Time:  76.74s, ---- Loss: 0.9611, Acc.: 0.6695, Val. Loss: 1.7571, Val. Acc.: 0.6391

Saving at models/ds_ee2_1k_equalweight_randoms_kmax_5/hist.png
Done in 3888.34s
