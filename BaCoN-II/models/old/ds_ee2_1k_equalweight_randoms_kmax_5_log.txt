err: 2024-02-21 15:53:55.831068: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
err: 2024-02-21 15:53:55.831137: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
err: 2024-02-21 15:53:55.832377: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
err: 2024-02-21 15:53:56.808944: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
err: /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
err: loc = add_variable_fn(
err: 2024-02-21 15:53:59.309683: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
err: /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
err: untransformed_scale = add_variable_fn(
err: WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
err: I0000 00:00:1708530857.187287   18048 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
out: Directory models/ds_ee2_1k_equalweight_randoms_kmax_5 not created
out: Logger creating log file: models/ds_ee2_1k_equalweight_randoms_kmax_5/ds_ee2_1k_equalweight_randoms_kmax_5_log.txt
out: -------- Parameters:
out: bayesian True
out: test_mode False
out: n_test_idx 2
out: seed 1312
out: fine_tune False
out: one_vs_all False
out: c_0 ['lcdm']
out: c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']
out: dataset_balanced False
out: include_last False
out: log_path models/ds_ee2_1k_equalweight_randoms_kmax_5_log.txt
out: restore False
out: fname ds_ee2_1k_equalweight_randoms_kmax_5
out: model_name custom
out: my_path None
out: DIR data/ds_ee2_train_1k
out: TEST_DIR data/test_data/
out: models_dir models/
out: save_ckpt True
out: out_path_overwrite False
out: curves_folder data/curve_files_sys/theory_error
out: save_processed_spectra False
out: im_depth 500
out: im_width 1
out: im_channels 4
out: swap_axes True
out: sort_labels True
out: norm_data_name /planck_ee2.txt
out: normalization stdcosmo
out: sample_pace 4
out: k_max 5.0
out: i_max None
out: add_noise True
out: n_noisy_samples 10
out: add_shot False
out: add_sys True
out: add_cosvar True
out: sigma_sys None
out: sys_scaled None
out: sys_factor None
out: sys_max None
out: sigma_curves 0.05
out: sigma_curves_default 0.05
out: rescale_curves uniform
out: z_bins [0, 1, 2, 3]
out: n_dense 1
out: filters [8, 16, 32]
out: kernel_sizes [10, 5, 2]
out: strides [2, 2, 1]
out: pool_sizes [2, 2, 0]
out: strides_pooling [2, 1, 0]
out: add_FT_dense False
out: trainable False
out: unfreeze False
out: lr 0.01
out: drop 0.5
out: n_epochs 50
out: val_size 0.15
out: test_size 0.0
out: batch_size 3000
out: patience 50
out: GPU True
out: decay 0.95
out: BatchNorm True
out: ------------ CREATING DATA GENERATORS ------------
out: labels : ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
out: Labels encoding:
out: {'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
out: n_labels : 6
out: dgp - 1000 training examples
out: ds - 1000 training examples
out: fr - 1000 training examples
out: lcdm - 1000 training examples
out: rand - 1000 training examples
out: wcdm - 1000 training examples
out: N. of data files: 1000
out: get_all_indexes labels dict: {'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
out: create_generators n_labels: 6
out: create_generators n_labels_eff: 6
out: create_generators len_c1: 1
out: Check for no duplicates in test: (0=ok):
out: 0.0
out: Check for no duplicates in val: (0=ok):
out: 0
out: N of files in training set: 850
out: N of files in validation set: 150
out: N of files in test set: 0
out: Check - total: 1000
out: --create_generators, train indexes
out: batch_size: 3000
out: - Cut sample
out: bs: 3000
out: N_labels: 6
out: N_noise: 10
out: len_c1: 1
out: Train index length: 850
out: --create_generators, validation indexes
out: - Cut sample
out: bs: 3000
out: N_labels: 6
out: N_noise: 10
out: len_c1: 1
out: Val index length: 150
out: len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 850, 3000, 6, 10
out: --DataGenerator Train
out: Data Generator Initialization
out: Using z bins [0, 1, 2, 3]
out: Normalisation file is /planck_ee2.txt
out: Specified k_max is 5.0
out: Corresponding i_max is 112
out: Closest k to k_max is 4.936132
out: New data dim: (112, 1)
out: Final i_max used is 112
out: one_vs_all: False
out: dataset_balanced: False
out: base_case_dataset: True
out: N. classes: 6
out: N. n_classes in output: 6
out: LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
out: list_IDs length: 850
out: n_indexes (n of file IDs read for each batch): 50
out: batch size: 3000
out: n_batches : 17
out: For each batch we read 50 file IDs
out: For each file ID we have 6 labels
out: For each ID, label we have 10 realizations of noise
out: In total, for each batch we have 3000 training examples
out: Input batch size: 3000
out: N of batches to cover all file IDs: 17
out: --DataGenerator Validation
out: Data Generator Initialization
out: Using z bins [0, 1, 2, 3]
out: Normalisation file is /planck_ee2.txt
out: Specified k_max is 5.0
out: Corresponding i_max is 112
out: Closest k to k_max is 4.936132
out: New data dim: (112, 1)
out: Final i_max used is 112
out: one_vs_all: False
out: dataset_balanced: False
out: base_case_dataset: True
out: N. classes: 6
out: N. n_classes in output: 6
out: LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
out: list_IDs length: 150
out: n_indexes (n of file IDs read for each batch): 50
out: batch size: 3000
out: n_batches : 3
out: For each batch we read 50 file IDs
out: For each file ID we have 6 labels
out: For each ID, label we have 10 realizations of noise
out: In total, for each batch we have 3000 training examples
out: Input batch size: 3000
out: N of batches to cover all file IDs: 3
out: ------------ DONE ------------
out: ------------ BUILDING MODEL ------------
out: Input shape (112, 4)
out: using 1D layers and 4 channels
out: Expected output dimension of layer conv1d_flipout: 52.0
out: Expected output dimension of layer max_pooling1d: 26.0
out: Expected output dimension of layer conv1d_flipout_1: 11.5
out: Expected output dimension of layer max_pooling1d_1: 10.5
out: Expected output dimension of layer conv1d_flipout_2: 9.5
out: Model: "model"
out: _________________________________________________________________
out: Layer (type)                Output Shape              Param #
out: =================================================================
out: input_1 (InputLayer)        [(None, 112, 4)]          0
out: conv1d_flipout (Conv1DFlip  (None, 52, 8)             648
out: out)
out: max_pooling1d (MaxPooling1  (None, 26, 8)             0
out: D)
out: batch_normalization (Batch  (None, 26, 8)             32
out: Normalization)
out: conv1d_flipout_1 (Conv1DFl  (None, 11, 16)            1296
out: ipout)
out: max_pooling1d_1 (MaxPoolin  (None, 10, 16)            0
out: g1D)
out: batch_normalization_1 (Bat  (None, 10, 16)            64
out: chNormalization)
out: conv1d_flipout_2 (Conv1DFl  (None, 9, 32)             2080
out: ipout)
out: batch_normalization_2 (Bat  (None, 9, 32)             128
out: chNormalization)
out: global_average_pooling1d (  (None, 32)                0
out: GlobalAveragePooling1D)
out: dense_flipout (DenseFlipou  (None, 32)                2080
out: t)
out: batch_normalization_3 (Bat  (None, 32)                128
out: chNormalization)
out: dense_flipout_1 (DenseFlip  (None, 6)                 390
out: out)
out: =================================================================
out: Total params: 6846 (26.74 KB)
out: Trainable params: 6670 (26.05 KB)
out: Non-trainable params: 176 (704.00 Byte)
out: _________________________________________________________________
out: None
out: Found GPU at: /device:GPU:0
out: ------------ TRAINING ------------
out: Features shape: (3000, 112, 4)
out: Labels shape: (3000, 6)
out: Initializing checkpoint from scratch.
out: Epoch 0
out: Validation loss decreased. Saved checkpoint for step 1: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-1
out: Time:  87.03s, ---- Loss: 1.7528, Acc.: 0.2543, Val. Loss: 2.6896, Val. Acc.: 0.1722
out: Epoch 1
out: Loss did not decrease. Count = 1
out: Time:  78.80s, ---- Loss: 1.4807, Acc.: 0.3926, Val. Loss: 2.7202, Val. Acc.: 0.1766
out: Epoch 2
out: Loss did not decrease. Count = 2
out: Time:  78.29s, ---- Loss: 1.3407, Acc.: 0.5011, Val. Loss: 2.9207, Val. Acc.: 0.1724
out: Epoch 3
out: Loss did not decrease. Count = 3
out: Time:  78.69s, ---- Loss: 1.2714, Acc.: 0.5370, Val. Loss: 2.7673, Val. Acc.: 0.1770
out: Epoch 4
out: Validation loss decreased. Saved checkpoint for step 5: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-2
out: Time:  78.31s, ---- Loss: 1.2491, Acc.: 0.5565, Val. Loss: 2.6091, Val. Acc.: 0.2117
out: Epoch 5
out: Validation loss decreased. Saved checkpoint for step 6: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-3
out: Time:  78.81s, ---- Loss: 1.2255, Acc.: 0.5678, Val. Loss: 2.5802, Val. Acc.: 0.2307
out: Epoch 6
out: Loss did not decrease. Count = 1
out: Time:  78.46s, ---- Loss: 1.2186, Acc.: 0.5769, Val. Loss: 2.6528, Val. Acc.: 0.2263
out: Epoch 7
out: Loss did not decrease. Count = 2
out: Time:  78.38s, ---- Loss: 1.2085, Acc.: 0.5855, Val. Loss: 2.6613, Val. Acc.: 0.2600
out: Epoch 8
out: Loss did not decrease. Count = 3
out: Time:  78.51s, ---- Loss: 1.1876, Acc.: 0.5899, Val. Loss: 2.6446, Val. Acc.: 0.2713
out: Epoch 9
out: Validation loss decreased. Saved checkpoint for step 10: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-4
out: Time:  78.22s, ---- Loss: 1.1829, Acc.: 0.5931, Val. Loss: 2.5708, Val. Acc.: 0.3210
out: Epoch 10
out: Loss did not decrease. Count = 1
out: Time:  78.39s, ---- Loss: 1.1554, Acc.: 0.5968, Val. Loss: 2.5861, Val. Acc.: 0.3508
out: Epoch 11
out: Loss did not decrease. Count = 2
out: Time:  78.30s, ---- Loss: 1.1310, Acc.: 0.6027, Val. Loss: 2.5846, Val. Acc.: 0.3441
out: Epoch 12
out: Loss did not decrease. Count = 3
out: Time:  78.15s, ---- Loss: 1.1447, Acc.: 0.6055, Val. Loss: 2.5982, Val. Acc.: 0.3591
out: Epoch 13
out: Validation loss decreased. Saved checkpoint for step 14: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-5
out: Time:  77.94s, ---- Loss: 1.1188, Acc.: 0.6072, Val. Loss: 2.5044, Val. Acc.: 0.3797
out: Epoch 14
out: Validation loss decreased. Saved checkpoint for step 15: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-6
out: Time:  78.49s, ---- Loss: 1.0942, Acc.: 0.6097, Val. Loss: 2.3823, Val. Acc.: 0.4098
out: Epoch 15
out: Validation loss decreased. Saved checkpoint for step 16: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-7
out: Time:  77.92s, ---- Loss: 1.0763, Acc.: 0.6167, Val. Loss: 2.3532, Val. Acc.: 0.4204
out: Epoch 16
out: Validation loss decreased. Saved checkpoint for step 17: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-8
out: Time:  78.68s, ---- Loss: 1.1048, Acc.: 0.6180, Val. Loss: 2.3121, Val. Acc.: 0.4509
out: Epoch 17
out: Validation loss decreased. Saved checkpoint for step 18: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-9
out: Time:  78.15s, ---- Loss: 1.0721, Acc.: 0.6213, Val. Loss: 2.2384, Val. Acc.: 0.4681
out: Epoch 18
out: Validation loss decreased. Saved checkpoint for step 19: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-10
out: Time:  77.65s, ---- Loss: 1.0669, Acc.: 0.6211, Val. Loss: 2.2196, Val. Acc.: 0.4877
out: Epoch 19
out: Validation loss decreased. Saved checkpoint for step 20: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-11
out: Time:  77.85s, ---- Loss: 1.0430, Acc.: 0.6274, Val. Loss: 2.1509, Val. Acc.: 0.5073
out: Epoch 20
out: Validation loss decreased. Saved checkpoint for step 21: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-12
out: Time:  77.76s, ---- Loss: 1.0687, Acc.: 0.6306, Val. Loss: 2.1001, Val. Acc.: 0.5242
out: Epoch 21
out: Validation loss decreased. Saved checkpoint for step 22: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-13
out: Time:  77.38s, ---- Loss: 1.0803, Acc.: 0.6315, Val. Loss: 2.0462, Val. Acc.: 0.5464
out: Epoch 22
out: Validation loss decreased. Saved checkpoint for step 23: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-14
out: Time:  77.11s, ---- Loss: 1.0469, Acc.: 0.6371, Val. Loss: 2.0013, Val. Acc.: 0.5601
out: Epoch 23
out: Validation loss decreased. Saved checkpoint for step 24: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-15
out: Time:  77.46s, ---- Loss: 1.0642, Acc.: 0.6357, Val. Loss: 1.9667, Val. Acc.: 0.5788
out: Epoch 24
out: Validation loss decreased. Saved checkpoint for step 25: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-16
out: Time:  76.90s, ---- Loss: 1.0552, Acc.: 0.6396, Val. Loss: 1.9509, Val. Acc.: 0.5847
out: Epoch 25
out: Validation loss decreased. Saved checkpoint for step 26: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-17
out: Time:  76.14s, ---- Loss: 1.0404, Acc.: 0.6416, Val. Loss: 1.9269, Val. Acc.: 0.5836
out: Epoch 26
out: Validation loss decreased. Saved checkpoint for step 27: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-18
out: Time:  77.62s, ---- Loss: 1.0286, Acc.: 0.6415, Val. Loss: 1.9189, Val. Acc.: 0.5898
out: Epoch 27
out: Loss did not decrease. Count = 1
out: Time:  77.61s, ---- Loss: 1.0478, Acc.: 0.6488, Val. Loss: 1.9406, Val. Acc.: 0.5729
out: Epoch 28
out: Validation loss decreased. Saved checkpoint for step 29: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-19
out: Time:  77.31s, ---- Loss: 1.0283, Acc.: 0.6501, Val. Loss: 1.8964, Val. Acc.: 0.5922
out: Epoch 29
out: Validation loss decreased. Saved checkpoint for step 30: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-20
out: Time:  77.41s, ---- Loss: 1.0225, Acc.: 0.6492, Val. Loss: 1.8899, Val. Acc.: 0.5962
out: Epoch 30
out: Validation loss decreased. Saved checkpoint for step 31: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-21
out: Time:  77.04s, ---- Loss: 1.0092, Acc.: 0.6527, Val. Loss: 1.8655, Val. Acc.: 0.5987
out: Epoch 31
out: Validation loss decreased. Saved checkpoint for step 32: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-22
out: Time:  77.45s, ---- Loss: 1.0291, Acc.: 0.6531, Val. Loss: 1.8476, Val. Acc.: 0.6074
out: Epoch 32
out: Validation loss decreased. Saved checkpoint for step 33: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-23
out: Time:  76.96s, ---- Loss: 1.0044, Acc.: 0.6562, Val. Loss: 1.8456, Val. Acc.: 0.6012
out: Epoch 33
out: Validation loss decreased. Saved checkpoint for step 34: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-24
out: Time:  77.94s, ---- Loss: 1.0366, Acc.: 0.6580, Val. Loss: 1.8353, Val. Acc.: 0.6033
out: Epoch 34
out: Loss did not decrease. Count = 1
out: Time:  76.71s, ---- Loss: 0.9960, Acc.: 0.6587, Val. Loss: 1.8625, Val. Acc.: 0.5918
out: Epoch 35
out: Loss did not decrease. Count = 2
out: Time:  76.48s, ---- Loss: 0.9958, Acc.: 0.6573, Val. Loss: 1.8558, Val. Acc.: 0.5878
out: Epoch 36
out: Loss did not decrease. Count = 3
out: Time:  76.33s, ---- Loss: 0.9834, Acc.: 0.6611, Val. Loss: 1.8668, Val. Acc.: 0.5912
out: Epoch 37
out: Validation loss decreased. Saved checkpoint for step 38: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-25
out: Time:  75.55s, ---- Loss: 1.0009, Acc.: 0.6614, Val. Loss: 1.8219, Val. Acc.: 0.6074
out: Epoch 38
out: Loss did not decrease. Count = 1
out: Time:  75.56s, ---- Loss: 0.9839, Acc.: 0.6619, Val. Loss: 1.8296, Val. Acc.: 0.5984
out: Epoch 39
out: Validation loss decreased. Saved checkpoint for step 40: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-26
out: Time:  76.79s, ---- Loss: 0.9996, Acc.: 0.6640, Val. Loss: 1.8103, Val. Acc.: 0.6084
out: Epoch 40
out: Loss did not decrease. Count = 1
out: Time:  76.26s, ---- Loss: 0.9711, Acc.: 0.6634, Val. Loss: 1.8154, Val. Acc.: 0.6079
out: Epoch 41
out: Validation loss decreased. Saved checkpoint for step 42: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-27
out: Time:  76.43s, ---- Loss: 0.9918, Acc.: 0.6647, Val. Loss: 1.7868, Val. Acc.: 0.6226
out: Epoch 42
out: Validation loss decreased. Saved checkpoint for step 43: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-28
out: Time:  76.47s, ---- Loss: 0.9882, Acc.: 0.6653, Val. Loss: 1.7780, Val. Acc.: 0.6264
out: Epoch 43
out: Loss did not decrease. Count = 1
out: Time:  76.85s, ---- Loss: 0.9544, Acc.: 0.6707, Val. Loss: 1.7831, Val. Acc.: 0.6287
out: Epoch 44
out: Validation loss decreased. Saved checkpoint for step 45: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-29
out: Time:  76.09s, ---- Loss: 0.9642, Acc.: 0.6671, Val. Loss: 1.7533, Val. Acc.: 0.6369
out: Epoch 45
out: Validation loss decreased. Saved checkpoint for step 46: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-30
out: Time:  76.54s, ---- Loss: 0.9896, Acc.: 0.6655, Val. Loss: 1.7450, Val. Acc.: 0.6412
out: Epoch 46
out: Loss did not decrease. Count = 1
out: Time:  76.15s, ---- Loss: 0.9762, Acc.: 0.6688, Val. Loss: 1.7605, Val. Acc.: 0.6322
out: Epoch 47
out: Loss did not decrease. Count = 2
out: Time:  76.48s, ---- Loss: 0.9500, Acc.: 0.6716, Val. Loss: 1.7539, Val. Acc.: 0.6380
out: Epoch 48
out: Loss did not decrease. Count = 3
out: Time:  76.19s, ---- Loss: 0.9826, Acc.: 0.6707, Val. Loss: 1.7600, Val. Acc.: 0.6343
out: Epoch 49
out: Loss did not decrease. Count = 4
out: Time:  76.74s, ---- Loss: 0.9611, Acc.: 0.6695, Val. Loss: 1.7571, Val. Acc.: 0.6391
out: Saving at models/ds_ee2_1k_equalweight_randoms_kmax_5/hist.png
out: Done in 3888.34s
