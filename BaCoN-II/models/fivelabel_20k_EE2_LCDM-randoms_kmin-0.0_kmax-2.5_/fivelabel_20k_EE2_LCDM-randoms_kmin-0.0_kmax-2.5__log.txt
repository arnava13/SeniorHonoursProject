
 -------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune False
one_vs_all False
c_0 ['lcdm']  c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']
dataset_balanced False
include_last False
log_path models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5__log.txt
restore False
fname fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_
model_name custom
my_path None
DIR data/train
TEST_DIR data/test_data/
models_dir models/
save_ckpt True
out_path_overwrite False
curves_folder data/curve_files_sys/theory_error
save_processed_spectra False
cache_dir False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
norm_data_name /planck_ee2.txt
normalization stdcosmo
sample_pace 4
k_max 2.5
k_min 0.0
i_max None
i_min None
add_noise True
n_noisy_samples 10
add_shot False
add_sys True
add_cosvar True
sigma_sys None
sys_scaled None
sys_factor None
sys_max None
sigma_curves 0.05
sigma_curves_default 0.05
rescale_curves uniform
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable False
unfreeze False
lr 0.01
drop 0.5
n_epochs 50
val_size 0.15
test_size 0.0
batch_size 12500
patience 20
GPU True
TPU False
decay 0.95
BatchNorm True
padding valid
shuffle False

------------ CREATING DATA GENERATORS ------------
labels : ['dgp', 'fr', 'lcdm', 'rand', 'wcdm']
Labels encoding: 
{'dgp': 0, 'fr': 1, 'lcdm': 2, 'rand': 3, 'wcdm': 4}
n_labels : 5
dgp - 20000 training examples
fr - 20000 training examples
lcdm - 20000 training examples
rand - 20000 training examples
wcdm - 20000 training examples

N. of data files: 20000
get_all_indexes labels dict: {'dgp': 0, 'fr': 1, 'lcdm': 2, 'rand': 3, 'wcdm': 4}
create_generators n_labels: 5
create_generators n_labels_eff: 5
create_generators len_c1: 1
Check for no duplicates in test: (0=ok):
0.0
Check for no duplicates in val: (0=ok):
0
N of indexes in training set: 17000
N of indexes in validation set: 3000
N of indexes in test set: 0
Check - total per class: 20000
--create_generators, train indexes
batch_size: 12500
- Cut sample
bs: 12500
N_labels: 5
N_noise: 10
len_c1: 1
Train index length: 17000
--create_generators, validation indexes
- Cut sample
bs: 12500
N_labels: 5
N_noise: 10
len_c1: 1
Val index length: 3000
len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 17000, 12500, 5, 10

--DataSet Train
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 100
Closest k to k_max is 2.539859
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (100, 1)
Final i_max used is 100
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 5
N. n_classes in output: 5
LABELS: ['dgp', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 17000
n_indexes (n of file IDs read for each batch): 250
batch size: 12500
n_batches : 68
For each batch we read 250 file IDs
For each file ID we have 5 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 12500 training examples
Input batch size: 12500
N of batches to cover all file IDs: 68
len(fname_list), batch_size, n_noisy_samples, n_batches: 85000, 12500, 10, 68

--DataSet Validation
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 100
Closest k to k_max is 2.539859
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (100, 1)
Final i_max used is 100
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 5
N. n_classes in output: 5
LABELS: ['dgp', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 3000
n_indexes (n of file IDs read for each batch): 250
batch size: 12500
n_batches : 12
For each batch we read 250 file IDs
For each file ID we have 5 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 12500 training examples
Input batch size: 12500
N of batches to cover all file IDs: 12
len(fname_list), batch_size, n_noisy_samples, n_batches: 15000, 12500, 10, 12
------------ DONE ------------

------------ BUILDING MODEL ------------
Input shape (100, 4)
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 100, 4)]          0         
                                                                 
 conv1d_flipout (Conv1DFlip  (None, 46, 8)             648       
 out)                                                            
                                                                 
 max_pooling1d (MaxPooling1  (None, 23, 8)             0         
 D)                                                              
                                                                 
 batch_normalization (Batch  (None, 23, 8)             32        
 Normalization)                                                  
                                                                 
 conv1d_flipout_1 (Conv1DFl  (None, 10, 16)            1296      
 ipout)                                                          
                                                                 
 max_pooling1d_1 (MaxPoolin  (None, 9, 16)             0         
 g1D)                                                            
                                                                 
 batch_normalization_1 (Bat  (None, 9, 16)             64        
 chNormalization)                                                
                                                                 
 conv1d_flipout_2 (Conv1DFl  (None, 8, 32)             2080      
 ipout)                                                          
                                                                 
 batch_normalization_2 (Bat  (None, 8, 32)             128       
 chNormalization)                                                
                                                                 
 global_average_pooling1d (  (None, 32)                0         
 GlobalAveragePooling1D)                                         
                                                                 
 dense_flipout (DenseFlipou  (None, 32)                2080      
 t)                                                              
                                                                 
 batch_normalization_3 (Bat  (None, 32)                128       
 chNormalization)                                                
                                                                 
 dense_flipout_1 (DenseFlip  (None, 5)                 325       
 out)                                                            
                                                                 
=================================================================
Total params: 6781 (26.49 KB)
Trainable params: 6605 (25.80 KB)
Non-trainable params: 176 (704.00 Byte)
_________________________________________________________________
None
Found GPU at: /device:GPU:0
------------ TRAINING ------------

Features shape: (12500, 100, 4)
Labels shape: (12500, 5)
Initializing checkpoint from scratch.
Epoch 0
Validation loss decreased. Saved checkpoint for step 1: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-1
Time:  36.38s, ---- Loss: 0.7332, Acc.: 0.5689, Val. Loss: 2.0008, Val. Acc.: 0.2563

Epoch 1
Validation loss decreased. Saved checkpoint for step 2: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-2
Time:  0.92s, ---- Loss: 0.5617, Acc.: 0.7571, Val. Loss: 1.9520, Val. Acc.: 0.3026

Epoch 2
Validation loss decreased. Saved checkpoint for step 3: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-3
Time:  0.90s, ---- Loss: 0.4593, Acc.: 0.8050, Val. Loss: 1.5080, Val. Acc.: 0.4901

Epoch 3
Validation loss decreased. Saved checkpoint for step 4: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-4
Time:  0.90s, ---- Loss: 0.4240, Acc.: 0.8288, Val. Loss: 1.2948, Val. Acc.: 0.5691

Epoch 4
Validation loss decreased. Saved checkpoint for step 5: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-5
Time:  0.89s, ---- Loss: 0.3865, Acc.: 0.8435, Val. Loss: 1.1312, Val. Acc.: 0.6150

Epoch 5
Loss did not decrease. Count = 1
Time:  0.77s, ---- Loss: 0.3565, Acc.: 0.8553, Val. Loss: 1.1610, Val. Acc.: 0.6304

Epoch 6
Validation loss decreased. Saved checkpoint for step 7: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-6
Time:  0.88s, ---- Loss: 0.3423, Acc.: 0.8639, Val. Loss: 1.0705, Val. Acc.: 0.6603

Epoch 7
Validation loss decreased. Saved checkpoint for step 8: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-7
Time:  0.90s, ---- Loss: 0.3328, Acc.: 0.8703, Val. Loss: 1.0176, Val. Acc.: 0.6728

Epoch 8
Validation loss decreased. Saved checkpoint for step 9: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-8
Time:  0.90s, ---- Loss: 0.3181, Acc.: 0.8759, Val. Loss: 0.8765, Val. Acc.: 0.7056

Epoch 9
Validation loss decreased. Saved checkpoint for step 10: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-9
Time:  0.90s, ---- Loss: 0.3110, Acc.: 0.8799, Val. Loss: 0.8557, Val. Acc.: 0.7152

Epoch 10
Validation loss decreased. Saved checkpoint for step 11: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-10
Time:  0.90s, ---- Loss: 0.2952, Acc.: 0.8831, Val. Loss: 0.8110, Val. Acc.: 0.7274

Epoch 11
Validation loss decreased. Saved checkpoint for step 12: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-11
Time:  0.90s, ---- Loss: 0.2927, Acc.: 0.8864, Val. Loss: 0.7451, Val. Acc.: 0.7416

Epoch 12
Validation loss decreased. Saved checkpoint for step 13: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-12
Time:  0.90s, ---- Loss: 0.2889, Acc.: 0.8885, Val. Loss: 0.7354, Val. Acc.: 0.7450

Epoch 13
Validation loss decreased. Saved checkpoint for step 14: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-13
Time:  0.90s, ---- Loss: 0.2863, Acc.: 0.8901, Val. Loss: 0.6026, Val. Acc.: 0.7889

Epoch 14
Validation loss decreased. Saved checkpoint for step 15: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-14
Time:  0.90s, ---- Loss: 0.2787, Acc.: 0.8920, Val. Loss: 0.5578, Val. Acc.: 0.8048

Epoch 15
Validation loss decreased. Saved checkpoint for step 16: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-15
Time:  0.89s, ---- Loss: 0.2771, Acc.: 0.8933, Val. Loss: 0.5180, Val. Acc.: 0.8204

Epoch 16
Validation loss decreased. Saved checkpoint for step 17: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-16
Time:  0.90s, ---- Loss: 0.2754, Acc.: 0.8946, Val. Loss: 0.5135, Val. Acc.: 0.8235

Epoch 17
Validation loss decreased. Saved checkpoint for step 18: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-17
Time:  0.89s, ---- Loss: 0.2717, Acc.: 0.8957, Val. Loss: 0.4656, Val. Acc.: 0.8411

Epoch 18
Validation loss decreased. Saved checkpoint for step 19: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-18
Time:  0.91s, ---- Loss: 0.2693, Acc.: 0.8965, Val. Loss: 0.4383, Val. Acc.: 0.8533

Epoch 19
Validation loss decreased. Saved checkpoint for step 20: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-19
Time:  0.91s, ---- Loss: 0.2668, Acc.: 0.8977, Val. Loss: 0.4101, Val. Acc.: 0.8669

Epoch 20
Validation loss decreased. Saved checkpoint for step 21: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-20
Time:  0.90s, ---- Loss: 0.2672, Acc.: 0.8982, Val. Loss: 0.3953, Val. Acc.: 0.8734

Epoch 21
Validation loss decreased. Saved checkpoint for step 22: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-21
Time:  0.90s, ---- Loss: 0.2624, Acc.: 0.8995, Val. Loss: 0.3896, Val. Acc.: 0.8754

Epoch 22
Validation loss decreased. Saved checkpoint for step 23: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-22
Time:  0.90s, ---- Loss: 0.2652, Acc.: 0.8998, Val. Loss: 0.3809, Val. Acc.: 0.8800

Epoch 23
Loss did not decrease. Count = 1
Time:  0.77s, ---- Loss: 0.2636, Acc.: 0.9006, Val. Loss: 0.3809, Val. Acc.: 0.8803

Epoch 24
Validation loss decreased. Saved checkpoint for step 25: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-23
Time:  0.90s, ---- Loss: 0.2594, Acc.: 0.9012, Val. Loss: 0.3756, Val. Acc.: 0.8831

Epoch 25
Validation loss decreased. Saved checkpoint for step 26: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-24
Time:  0.92s, ---- Loss: 0.2580, Acc.: 0.9017, Val. Loss: 0.3695, Val. Acc.: 0.8855

Epoch 26
Validation loss decreased. Saved checkpoint for step 27: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-25
Time:  0.89s, ---- Loss: 0.2571, Acc.: 0.9023, Val. Loss: 0.3666, Val. Acc.: 0.8865

Epoch 27
Loss did not decrease. Count = 1
Time:  0.76s, ---- Loss: 0.2555, Acc.: 0.9025, Val. Loss: 0.3709, Val. Acc.: 0.8847

Epoch 28
Validation loss decreased. Saved checkpoint for step 29: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-26
Time:  0.88s, ---- Loss: 0.2538, Acc.: 0.9030, Val. Loss: 0.3651, Val. Acc.: 0.8874

Epoch 29
Validation loss decreased. Saved checkpoint for step 30: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-27
Time:  0.89s, ---- Loss: 0.2575, Acc.: 0.9033, Val. Loss: 0.3605, Val. Acc.: 0.8895

Epoch 30
Loss did not decrease. Count = 1
Time:  0.77s, ---- Loss: 0.2562, Acc.: 0.9039, Val. Loss: 0.3629, Val. Acc.: 0.8886

Epoch 31
Validation loss decreased. Saved checkpoint for step 32: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-28
Time:  0.90s, ---- Loss: 0.2520, Acc.: 0.9041, Val. Loss: 0.3572, Val. Acc.: 0.8913

Epoch 32
Validation loss decreased. Saved checkpoint for step 33: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-29
Time:  0.90s, ---- Loss: 0.2578, Acc.: 0.9042, Val. Loss: 0.3546, Val. Acc.: 0.8923

Epoch 33
Validation loss decreased. Saved checkpoint for step 34: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-30
Time:  0.89s, ---- Loss: 0.2553, Acc.: 0.9046, Val. Loss: 0.3533, Val. Acc.: 0.8933

Epoch 34
Validation loss decreased. Saved checkpoint for step 35: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-31
Time:  0.90s, ---- Loss: 0.2496, Acc.: 0.9048, Val. Loss: 0.3521, Val. Acc.: 0.8942

Epoch 35
Validation loss decreased. Saved checkpoint for step 36: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-32
Time:  0.90s, ---- Loss: 0.2528, Acc.: 0.9050, Val. Loss: 0.3496, Val. Acc.: 0.8952

Epoch 36
Loss did not decrease. Count = 1
Time:  0.77s, ---- Loss: 0.2519, Acc.: 0.9054, Val. Loss: 0.3525, Val. Acc.: 0.8937

Epoch 37
Loss did not decrease. Count = 2
Time:  0.77s, ---- Loss: 0.2519, Acc.: 0.9056, Val. Loss: 0.3521, Val. Acc.: 0.8942

Epoch 38
Validation loss decreased. Saved checkpoint for step 39: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-33
Time:  0.90s, ---- Loss: 0.2505, Acc.: 0.9058, Val. Loss: 0.3470, Val. Acc.: 0.8961

Epoch 39
Loss did not decrease. Count = 1
Time:  0.78s, ---- Loss: 0.2510, Acc.: 0.9061, Val. Loss: 0.3486, Val. Acc.: 0.8959

Epoch 40
Validation loss decreased. Saved checkpoint for step 41: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-34
Time:  0.88s, ---- Loss: 0.2483, Acc.: 0.9062, Val. Loss: 0.3469, Val. Acc.: 0.8962

Epoch 41
Validation loss decreased. Saved checkpoint for step 42: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-35
Time:  0.90s, ---- Loss: 0.2491, Acc.: 0.9063, Val. Loss: 0.3458, Val. Acc.: 0.8966

Epoch 42
Validation loss decreased. Saved checkpoint for step 43: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-36
Time:  0.89s, ---- Loss: 0.2471, Acc.: 0.9066, Val. Loss: 0.3443, Val. Acc.: 0.8982

Epoch 43
Loss did not decrease. Count = 1
Time:  0.78s, ---- Loss: 0.2493, Acc.: 0.9066, Val. Loss: 0.3446, Val. Acc.: 0.8977

Epoch 44
Validation loss decreased. Saved checkpoint for step 45: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-37
Time:  0.89s, ---- Loss: 0.2481, Acc.: 0.9067, Val. Loss: 0.3442, Val. Acc.: 0.8978

Epoch 45
Validation loss decreased. Saved checkpoint for step 46: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-38
Time:  0.89s, ---- Loss: 0.2484, Acc.: 0.9070, Val. Loss: 0.3438, Val. Acc.: 0.8982

Epoch 46
Validation loss decreased. Saved checkpoint for step 47: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-39
Time:  0.90s, ---- Loss: 0.2465, Acc.: 0.9072, Val. Loss: 0.3419, Val. Acc.: 0.8986

Epoch 47
Validation loss decreased. Saved checkpoint for step 48: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-40
Time:  0.89s, ---- Loss: 0.2469, Acc.: 0.9073, Val. Loss: 0.3398, Val. Acc.: 0.8996

Epoch 48
Loss did not decrease. Count = 1
Time:  0.77s, ---- Loss: 0.2483, Acc.: 0.9074, Val. Loss: 0.3398, Val. Acc.: 0.9000

Epoch 49
Validation loss decreased. Saved checkpoint for step 50: models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-41
Time:  0.89s, ---- Loss: 0.2458, Acc.: 0.9074, Val. Loss: 0.3390, Val. Acc.: 0.9001

Saving at models/fivelabel_20k_EE2_LCDM-randoms_kmin-0.0_kmax-2.5_/hist.png
Done in 7020.21s
