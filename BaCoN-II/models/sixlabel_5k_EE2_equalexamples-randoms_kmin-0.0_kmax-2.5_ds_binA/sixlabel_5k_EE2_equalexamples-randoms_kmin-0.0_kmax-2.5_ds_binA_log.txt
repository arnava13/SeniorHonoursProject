
 -------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune False
one_vs_all False
c_0 ['lcdm']
c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']
dataset_balanced False
include_last False
log_path models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA_log.txt
restore False
fname sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA
model_name custom
my_path None
DIR data/train
TEST_DIR data/test/
models_dir models/
save_ckpt True
out_path_overwrite False
curves_folder data/curve_files_sys/theory_error
save_processed_spectra False
cache_dir False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
norm_data_name /planck_ee2.txt
normalization stdcosmo
sample_pace 1
k_max 2.5
k_min 0.0
i_max None
i_min None
add_noise True
n_noisy_samples 10
add_shot False
add_sys True
add_cosvar True
sigma_sys None
sys_scaled None
sys_factor None
sys_max None
sigma_curves 0.05
sigma_curves_default 0.05
rescale_curves uniform
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable False
unfreeze False
lr 0.01
drop 0.5
n_epochs 50
val_size 0.15
test_size 0.0
batch_size 3000
patience 20
GPU True
TPU False
decay 0.95
BatchNorm True
padding valid
shuffle True

------------ CREATING DATA GENERATORS ------------
labels : ['dgp', 'ds_binA', 'fr', 'lcdm', 'rand', 'wcdm']
Labels encoding: 
{'dgp': 0, 'ds_binA': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
n_labels : 6
dgp - 5000 training examples
ds_binA - 5000 training examples
fr - 5000 training examples
lcdm - 5000 training examples
rand - 5000 training examples
wcdm - 5000 training examples

N. of data files: 5000
get_all_indexes labels dict: {'dgp': 0, 'ds_binA': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
create_generators n_labels: 6
create_generators n_labels_eff: 6
create_generators len_c1: 1
Check for no duplicates in test: (0=ok):
0.0
Check for no duplicates in val: (0=ok):
0
N of indexes in training set: 4250
N of indexes in validation set: 750
N of indexes in test set: 0
Check - total per class: 5000
--create_generators, train indexes
batch_size: 3000
- Cut sample
bs: 3000
N_labels: 6
N_noise: 10
len_c1: 1
Train index length: 4250
--create_generators, validation indexes
- Cut sample
bs: 3000
N_labels: 6
N_noise: 10
len_c1: 1
Val index length: 750
len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 4250, 3000, 6, 10

--DataSet Train
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 399
Closest k to k_max is 2.504942
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (399, 1)
Final i_max used is 399
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds_binA', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 4250
n_indexes (n of file IDs read for each batch): 50
batch size: 3000
n_batches : 85
For each batch we read 50 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 3000 training examples
Input batch size: 3000
N of batches to cover all file IDs: 85
len(fname_list), batch_size, n_noisy_samples, n_batches: 25500, 3000, 10, 85

--DataSet Validation
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 399
Closest k to k_max is 2.504942
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (399, 1)
Final i_max used is 399
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds_binA', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 750
n_indexes (n of file IDs read for each batch): 50
batch size: 3000
n_batches : 15
For each batch we read 50 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 3000 training examples
Input batch size: 3000
N of batches to cover all file IDs: 15
len(fname_list), batch_size, n_noisy_samples, n_batches: 4500, 3000, 10, 15
------------ DONE ------------

------------ BUILDING MODEL ------------
Input shape (399, 4)
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 399, 4)]          0         
                                                                 
 conv1d_flipout (Conv1DFlip  (None, 195, 8)            648       
 out)                                                            
                                                                 
 max_pooling1d (MaxPooling1  (None, 97, 8)             0         
 D)                                                              
                                                                 
 batch_normalization (Batch  (None, 97, 8)             32        
 Normalization)                                                  
                                                                 
 conv1d_flipout_1 (Conv1DFl  (None, 47, 16)            1296      
 ipout)                                                          
                                                                 
 max_pooling1d_1 (MaxPoolin  (None, 46, 16)            0         
 g1D)                                                            
                                                                 
 batch_normalization_1 (Bat  (None, 46, 16)            64        
 chNormalization)                                                
                                                                 
 conv1d_flipout_2 (Conv1DFl  (None, 45, 32)            2080      
 ipout)                                                          
                                                                 
 batch_normalization_2 (Bat  (None, 45, 32)            128       
 chNormalization)                                                
                                                                 
 global_average_pooling1d (  (None, 32)                0         
 GlobalAveragePooling1D)                                         
                                                                 
 dense_flipout (DenseFlipou  (None, 32)                2080      
 t)                                                              
                                                                 
 batch_normalization_3 (Bat  (None, 32)                128       
 chNormalization)                                                
                                                                 
 dense_flipout_1 (DenseFlip  (None, 6)                 390       
 out)                                                            
                                                                 
=================================================================
Total params: 6846 (26.74 KB)
Trainable params: 6670 (26.05 KB)
Non-trainable params: 176 (704.00 Byte)
_________________________________________________________________
None
Found GPU at: /device:GPU:0
------------ TRAINING ------------

Features shape: (3000, 399, 4)
Labels shape: (3000, 6)
Initializing checkpoint from scratch.
Epoch 0
Validation loss decreased. Saved checkpoint for step 1: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-1
Time:  34.43s, ---- Loss: 1.1469, Acc.: 0.4481, Val. Loss: 2.6593, Val. Acc.: 0.2292

Epoch 1
Validation loss decreased. Saved checkpoint for step 2: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-2
Time:  1.11s, ---- Loss: 0.9660, Acc.: 0.5882, Val. Loss: 1.9789, Val. Acc.: 0.3493

Epoch 2
Validation loss decreased. Saved checkpoint for step 3: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-3
Time:  1.09s, ---- Loss: 0.9169, Acc.: 0.6356, Val. Loss: 1.7635, Val. Acc.: 0.4260

Epoch 3
Validation loss decreased. Saved checkpoint for step 4: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-4
Time:  1.09s, ---- Loss: 0.8533, Acc.: 0.6581, Val. Loss: 1.5264, Val. Acc.: 0.4814

Epoch 4
Validation loss decreased. Saved checkpoint for step 5: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-5
Time:  1.07s, ---- Loss: 0.8030, Acc.: 0.6754, Val. Loss: 1.2441, Val. Acc.: 0.5840

Epoch 5
Validation loss decreased. Saved checkpoint for step 6: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-6
Time:  1.07s, ---- Loss: 0.7839, Acc.: 0.6864, Val. Loss: 1.0501, Val. Acc.: 0.6504

Epoch 6
Loss did not decrease. Count = 1
Time:  0.95s, ---- Loss: 0.7459, Acc.: 0.6953, Val. Loss: 2.2791, Val. Acc.: 0.3502

Epoch 7
Loss did not decrease. Count = 2
Time:  0.95s, ---- Loss: 0.7375, Acc.: 0.7019, Val. Loss: 1.9310, Val. Acc.: 0.3943

Epoch 8
Loss did not decrease. Count = 3
Time:  0.94s, ---- Loss: 0.7315, Acc.: 0.7083, Val. Loss: 1.1584, Val. Acc.: 0.6096

Epoch 9
Validation loss decreased. Saved checkpoint for step 10: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-7
Time:  1.07s, ---- Loss: 0.7149, Acc.: 0.7117, Val. Loss: 1.0174, Val. Acc.: 0.6674

Epoch 10
Validation loss decreased. Saved checkpoint for step 11: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-8
Time:  1.06s, ---- Loss: 0.7036, Acc.: 0.7147, Val. Loss: 0.9479, Val. Acc.: 0.6959

Epoch 11
Loss did not decrease. Count = 1
Time:  0.94s, ---- Loss: 0.6897, Acc.: 0.7181, Val. Loss: 0.9574, Val. Acc.: 0.6894

Epoch 12
Loss did not decrease. Count = 2
Time:  0.94s, ---- Loss: 0.6838, Acc.: 0.7213, Val. Loss: 0.9512, Val. Acc.: 0.6947

Epoch 13
Validation loss decreased. Saved checkpoint for step 14: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-9
Time:  1.07s, ---- Loss: 0.6743, Acc.: 0.7229, Val. Loss: 0.9278, Val. Acc.: 0.7044

Epoch 14
Validation loss decreased. Saved checkpoint for step 15: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-10
Time:  1.09s, ---- Loss: 0.6758, Acc.: 0.7249, Val. Loss: 0.9230, Val. Acc.: 0.7049

Epoch 15
Validation loss decreased. Saved checkpoint for step 16: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-11
Time:  1.08s, ---- Loss: 0.6709, Acc.: 0.7273, Val. Loss: 0.8987, Val. Acc.: 0.7148

Epoch 16
Loss did not decrease. Count = 1
Time:  0.95s, ---- Loss: 0.6664, Acc.: 0.7283, Val. Loss: 0.9022, Val. Acc.: 0.7131

Epoch 17
Loss did not decrease. Count = 2
Time:  0.96s, ---- Loss: 0.6567, Acc.: 0.7310, Val. Loss: 0.9201, Val. Acc.: 0.7060

Epoch 18
Loss did not decrease. Count = 3
Time:  0.96s, ---- Loss: 0.6601, Acc.: 0.7312, Val. Loss: 0.9003, Val. Acc.: 0.7161

Epoch 19
Loss did not decrease. Count = 4
Time:  0.94s, ---- Loss: 0.6614, Acc.: 0.7323, Val. Loss: 0.9001, Val. Acc.: 0.7158

Epoch 20
Validation loss decreased. Saved checkpoint for step 21: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-12
Time:  1.06s, ---- Loss: 0.6565, Acc.: 0.7340, Val. Loss: 0.8919, Val. Acc.: 0.7217

Epoch 21
Validation loss decreased. Saved checkpoint for step 22: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-13
Time:  1.06s, ---- Loss: 0.6574, Acc.: 0.7348, Val. Loss: 0.8899, Val. Acc.: 0.7212

Epoch 22
Validation loss decreased. Saved checkpoint for step 23: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-14
Time:  1.07s, ---- Loss: 0.6446, Acc.: 0.7366, Val. Loss: 0.8825, Val. Acc.: 0.7241

Epoch 23
Loss did not decrease. Count = 1
Time:  0.96s, ---- Loss: 0.6514, Acc.: 0.7366, Val. Loss: 0.8927, Val. Acc.: 0.7229

Epoch 24
Loss did not decrease. Count = 2
Time:  0.95s, ---- Loss: 0.6508, Acc.: 0.7376, Val. Loss: 0.8910, Val. Acc.: 0.7212

Epoch 25
Loss did not decrease. Count = 3
Time:  0.97s, ---- Loss: 0.6360, Acc.: 0.7388, Val. Loss: 0.8923, Val. Acc.: 0.7194

Epoch 26
Loss did not decrease. Count = 4
Time:  0.95s, ---- Loss: 0.6383, Acc.: 0.7392, Val. Loss: 0.8851, Val. Acc.: 0.7255

Epoch 27
Loss did not decrease. Count = 5
Time:  0.95s, ---- Loss: 0.6384, Acc.: 0.7407, Val. Loss: 0.8835, Val. Acc.: 0.7247

Epoch 28
Validation loss decreased. Saved checkpoint for step 29: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-15
Time:  1.07s, ---- Loss: 0.6363, Acc.: 0.7408, Val. Loss: 0.8807, Val. Acc.: 0.7270

Epoch 29
Validation loss decreased. Saved checkpoint for step 30: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-16
Time:  1.06s, ---- Loss: 0.6300, Acc.: 0.7417, Val. Loss: 0.8795, Val. Acc.: 0.7281

Epoch 30
Validation loss decreased. Saved checkpoint for step 31: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-17
Time:  1.07s, ---- Loss: 0.6214, Acc.: 0.7429, Val. Loss: 0.8767, Val. Acc.: 0.7308

Epoch 31
Validation loss decreased. Saved checkpoint for step 32: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-18
Time:  1.06s, ---- Loss: 0.6192, Acc.: 0.7428, Val. Loss: 0.8752, Val. Acc.: 0.7311

Epoch 32
Validation loss decreased. Saved checkpoint for step 33: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-19
Time:  1.06s, ---- Loss: 0.6236, Acc.: 0.7436, Val. Loss: 0.8748, Val. Acc.: 0.7300

Epoch 33
Validation loss decreased. Saved checkpoint for step 34: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-20
Time:  1.08s, ---- Loss: 0.6258, Acc.: 0.7446, Val. Loss: 0.8742, Val. Acc.: 0.7331

Epoch 34
Validation loss decreased. Saved checkpoint for step 35: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-21
Time:  1.07s, ---- Loss: 0.6214, Acc.: 0.7446, Val. Loss: 0.8689, Val. Acc.: 0.7327

Epoch 35
Loss did not decrease. Count = 1
Time:  0.96s, ---- Loss: 0.6237, Acc.: 0.7451, Val. Loss: 0.8700, Val. Acc.: 0.7336

Epoch 36
Validation loss decreased. Saved checkpoint for step 37: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-22
Time:  1.09s, ---- Loss: 0.6138, Acc.: 0.7454, Val. Loss: 0.8665, Val. Acc.: 0.7344

Epoch 37
Loss did not decrease. Count = 1
Time:  0.95s, ---- Loss: 0.6130, Acc.: 0.7464, Val. Loss: 0.8729, Val. Acc.: 0.7323

Epoch 38
Loss did not decrease. Count = 2
Time:  0.95s, ---- Loss: 0.6144, Acc.: 0.7467, Val. Loss: 0.8746, Val. Acc.: 0.7314

Epoch 39
Loss did not decrease. Count = 3
Time:  0.95s, ---- Loss: 0.6123, Acc.: 0.7474, Val. Loss: 0.8715, Val. Acc.: 0.7332

Epoch 40
Loss did not decrease. Count = 4
Time:  0.94s, ---- Loss: 0.6147, Acc.: 0.7468, Val. Loss: 0.8684, Val. Acc.: 0.7327

Epoch 41
Loss did not decrease. Count = 5
Time:  0.94s, ---- Loss: 0.6118, Acc.: 0.7483, Val. Loss: 0.8683, Val. Acc.: 0.7348

Epoch 42
Loss did not decrease. Count = 6
Time:  0.95s, ---- Loss: 0.6154, Acc.: 0.7478, Val. Loss: 0.8697, Val. Acc.: 0.7339

Epoch 43
Loss did not decrease. Count = 7
Time:  0.95s, ---- Loss: 0.6158, Acc.: 0.7484, Val. Loss: 0.8696, Val. Acc.: 0.7340

Epoch 44
Loss did not decrease. Count = 8
Time:  0.94s, ---- Loss: 0.6102, Acc.: 0.7496, Val. Loss: 0.8689, Val. Acc.: 0.7340

Epoch 45
Loss did not decrease. Count = 9
Time:  0.94s, ---- Loss: 0.6076, Acc.: 0.7494, Val. Loss: 0.8668, Val. Acc.: 0.7351

Epoch 46
Loss did not decrease. Count = 10
Time:  0.96s, ---- Loss: 0.6114, Acc.: 0.7499, Val. Loss: 0.8735, Val. Acc.: 0.7332

Epoch 47
Validation loss decreased. Saved checkpoint for step 48: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-23
Time:  1.07s, ---- Loss: 0.6060, Acc.: 0.7494, Val. Loss: 0.8652, Val. Acc.: 0.7338

Epoch 48
Loss did not decrease. Count = 1
Time:  0.96s, ---- Loss: 0.6023, Acc.: 0.7501, Val. Loss: 0.8677, Val. Acc.: 0.7344

Epoch 49
Validation loss decreased. Saved checkpoint for step 50: models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/tf_ckpts/ckpt-24
Time:  1.07s, ---- Loss: 0.6077, Acc.: 0.7501, Val. Loss: 0.8617, Val. Acc.: 0.7384

Saving at models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binA/hist.png
Done in 1062.81s
