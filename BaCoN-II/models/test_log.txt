out: Directory models/test not created
out: Logger creating log file: models/test/test_log.txt
out: -------- Parameters:
out: bayesian True
out: test_mode False
out: n_test_idx 2
out: seed 1312
out: fine_tune False
out: one_vs_all False
out: c_0 ['lcdm']
out: c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']
out: dataset_balanced False
out: include_last False
out: log_path models/test_log.txt
out: restore False
out: fname test
out: model_name custom
out: my_path None
out: DIR data/minimal
out: TEST_DIR data/test_data/
out: models_dir models/
out: save_ckpt True
out: out_path_overwrite False
out: curves_folder data/curve_files_sys/theory_error
out: save_processed_spectra True
out: im_depth 500
out: im_width 1
out: im_channels 4
out: swap_axes True
out: sort_labels True
out: norm_data_name /planck_ee2.txt
out: normalization stdcosmo
out: sample_pace 4
out: k_max 5.0
out: k_min 0.8508632
out: i_max None
out: i_min None
out: add_noise True
out: n_noisy_samples 10
out: add_shot False
out: add_sys True
out: add_cosvar True
out: sigma_sys None
out: sys_scaled None
out: sys_factor None
out: sys_max None
out: sigma_curves 0.05
out: sigma_curves_default 0.05
out: rescale_curves uniform
out: z_bins [0, 1, 2, 3]
out: n_dense 1
out: filters [8, 16, 32]
out: kernel_sizes [10, 5, 2]
out: strides [2, 2, 1]
out: pool_sizes [2, 2, 0]
out: strides_pooling [2, 1, 0]
out: add_FT_dense False
out: trainable False
out: unfreeze False
out: lr 0.01
out: drop 0.5
out: n_epochs 1
out: val_size 0.15
out: test_size 0.0
out: batch_size 60
out: patience 50
out: GPU True
out: TPU False
out: decay 0.95
out: BatchNorm True
out: padding same
out: shuffle False
out: ------------ CREATING DATASETS ------------
out: labels : ['dgp', 'ds_binA', 'fr', 'lcdm', 'rand', 'wcdm']
out: Labels encoding:
out: {'dgp': 0, 'ds_binA': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
out: n_labels : 6
out: dgp - 20 training examples
out: ds_binA - 20 training examples
out: fr - 20 training examples
out: lcdm - 20 training examples
out: rand - 20 training examples
out: wcdm - 20 training examples
out: get_all_indexes labels dict: {'dgp': 0, 'ds_binA': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
out: create_generators n_labels: 6
out: create_generators n_labels_eff: 6
out: create_generators len_c1: 1
out: Check for no duplicates in test: (0=ok):
out: 0.0
out: Check for no duplicates in val: (0=ok):
out: 0
out: N of files in training set: 6N of files in training set: 6N of files in training set: 6N of files in training set: 6N of files in training set: 6N of files in training set: 6N of files in training set: 6N of files in training set: 6N of files in training set: 6N of files in training set: 6N of files in training set: 6N of files in training set: 6N of files in training set: 6N of files in training set: 6N of files in training set: 6N of files in training set: 6N of files in training set: 6
out: N of files in validation set: 6N of files in validation set: 6N of files in validation set: 6
out: Check - total per class: 20
out: --create_generators, train indexes
out: batch_size: 60
out: - Cut sample
out: bs: 60
out: N_labels: 6
out: N_noise: 10
out: len_c1: 1
out: Train index length: 17
out: --create_generators, validation indexes
out: - Cut sample
out: bs: 60
out: N_labels: 6
out: N_noise: 10
out: len_c1: 1
out: Val index length: 3
out: len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 17, 60, 6, 10
out: --DataSet Train
out: DataSet Initialization
out: Using z bins [0, 1, 2, 3]
out: Normalisation file is /planck_ee2.txt
out: Specified k_max is 5.0
out: Corresponding i_max is 112
out: Closest k to k_max is 4.936132
out: Specified k_min is 0.8508632
out: Corresponding i_min is 80
out: Closest k to k_min is 0.8391656
out: New data dim: (32, 1)
out: Final i_max used is 112
out: Final i_min used is 80
out: one_vs_all: False
out: dataset_balanced: False
out: base_case_dataset: True
out: N. classes: 6
out: N. n_classes in output: 6
out: LABELS: ['dgp', 'ds_binA', 'fr', 'lcdm', 'rand', 'wcdm']
out: list_IDs length: 17
out: n_indexes (n of file IDs read for each batch): 1
out: batch size: 60
out: n_batches : 17
out: For each batch we read 1 file IDs
out: For each file ID we have 6 labels
out: For each ID, label we have 10 realizations of noise
out: In total, for each batch we have 60 training examples
out: Input batch size: 60
out: N of batches to cover all file IDs: 17
out: len(fname_list), batch_size, n_noisy_samples, n_batches: 102, 60, 10, 17
out: Saving processed (noisy and normalised) spectra in models/test/processed_spectra/processed_spectra_zbin0.txt
out: Saving processed (noisy and normalised) spectra in models/test/processed_spectra/processed_spectra_zbin1.txt
out: Saving processed (noisy and normalised) spectra in models/test/processed_spectra/processed_spectra_zbin2.txt
out: Saving processed (noisy and normalised) spectra in models/test/processed_spectra/processed_spectra_zbin3.txt
out: --DataSet Validation
out: DataSet Initialization
out: Using z bins [0, 1, 2, 3]
out: Normalisation file is /planck_ee2.txt
out: Specified k_max is 5.0
out: Corresponding i_max is 112
out: Closest k to k_max is 4.936132
out: Specified k_min is 0.8508632
out: Corresponding i_min is 80
out: Closest k to k_min is 0.8391656
out: New data dim: (32, 1)
out: Final i_max used is 112
out: Final i_min used is 80
out: one_vs_all: False
out: dataset_balanced: False
out: base_case_dataset: True
out: N. classes: 6
out: N. n_classes in output: 6
out: LABELS: ['dgp', 'ds_binA', 'fr', 'lcdm', 'rand', 'wcdm']
out: list_IDs length: 3
out: n_indexes (n of file IDs read for each batch): 1
out: batch size: 60
out: n_batches : 3
out: For each batch we read 1 file IDs
out: For each file ID we have 6 labels
out: For each ID, label we have 10 realizations of noise
out: In total, for each batch we have 60 training examples
out: Input batch size: 60
out: N of batches to cover all file IDs: 3
out: len(fname_list), batch_size, n_noisy_samples, n_batches: 18, 60, 10, 3
out: ------------ DONE ------------
out: ------------ BUILDING MODEL ------------
out: Input shape (32, 4)
out: using 1D layers and 4 channels
out: Expected output dimension of layer conv1d_flipout: 12.0
err: /Users/arnav/.pyenv/versions/3.11.8/lib/python3.11/site-packages/tensorflow_probability/python/layers/util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
err: loc = add_variable_fn(
err: /Users/arnav/.pyenv/versions/3.11.8/lib/python3.11/site-packages/tensorflow_probability/python/layers/util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
err: untransformed_scale = add_variable_fn(
out: Expected output dimension of layer max_pooling1d: 6.0
out: Expected output dimension of layer conv1d_flipout_1: 1.5
out: Expected output dimension of layer max_pooling1d_1: 0.5
out: Expected output dimension of layer conv1d_flipout_2: -0.5
out: Model: "model"
out: _________________________________________________________________
out: Layer (type)                Output Shape              Param #
out: =================================================================
out: input_1 (InputLayer)        [(None, 32, 4)]           0
out: conv1d_flipout (Conv1DFlip  (None, 16, 8)             648
out: out)
out: max_pooling1d (MaxPooling1  (None, 8, 8)              0
out: D)
out: batch_normalization (Batch  (None, 8, 8)              32
out: Normalization)
out: conv1d_flipout_1 (Conv1DFl  (None, 4, 16)             1296
out: ipout)
out: max_pooling1d_1 (MaxPoolin  (None, 4, 16)             0
out: g1D)
out: batch_normalization_1 (Bat  (None, 4, 16)             64
out: chNormalization)
out: conv1d_flipout_2 (Conv1DFl  (None, 4, 32)             2080
out: ipout)
out: batch_normalization_2 (Bat  (None, 4, 32)             128
out: chNormalization)
out: global_average_pooling1d (  (None, 32)                0
out: GlobalAveragePooling1D)
out: dense_flipout (DenseFlipou  (None, 32)                2080
out: t)
out: batch_normalization_3 (Bat  (None, 32)                128
out: chNormalization)
out: dense_flipout_1 (DenseFlip  (None, 6)                 390
out: out)
out: =================================================================
out: Total params: 6846 (26.74 KB)
out: Trainable params: 6670 (26.05 KB)
out: Non-trainable params: 176 (704.00 Byte)
out: _________________________________________________________________
out: None
out: GPU device not found ! Device:
out: ------------ TRAINING ------------
out: Features shape: (60, 32, 4)
out: Labels shape: (60,)
out: Initializing checkpoint from scratch.
out: 1/17 [>.............................] - ETA: 17s - loss: 8109.3931 - accuracy: 0.0833
out: Epoch 1: Validation loss decreased to 7612.8403. Checkpoint saved: models/test/tf_ckpts/ckpt-1
out: Time:  1.54
out: 17/17 [==============================] - 2s 26ms/step - loss: 7853.7842 - accuracy: 0.1843 - val_loss: 7612.8403 - val_accuracy: 0.1889
out: Saving at models/test/hist.png
out: Done in 3.98s
