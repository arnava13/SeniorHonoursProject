
 -------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune False
one_vs_all False
c_0 ['lcdm']
c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']
dataset_balanced False
include_last False
log_path models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_space-1_log.txt
restore False
fname sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_space-1
model_name custom
my_path None
DIR data/train
TEST_DIR data/test
models_dir models/
save_ckpt True
out_path_overwrite False
curves_folder data/curve_files_sys/theory_error
save_processed_spectra False
cache_dir False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
norm_data_name /planck_ee2.txt
normalization stdcosmo
sample_pace 4
k_max 2.5
k_min 0.0
i_max None
i_min None
add_noise True
n_noisy_samples 10
add_shot False
add_sys True
add_cosvar True
sigma_sys None
sys_scaled None
sys_factor None
sys_max None
sigma_curves 0.05
sigma_curves_default 0.05
rescale_curves uniform
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable False
unfreeze False
lr 0.05
drop 0.5
n_epochs 50
val_size 0.15
test_size 0.0
batch_size 3000
patience 20
GPU True
TPU False
decay 0.95
BatchNorm True
padding valid
shuffle True

------------ CREATING DATA GENERATORS ------------
labels : ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
Labels encoding: 
{'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
n_labels : 6
dgp - 20000 training examples
ds - 20000 training examples
fr - 20000 training examples
lcdm - 20000 training examples
rand - 20000 training examples
wcdm - 20000 training examples

N. of data files: 20000
get_all_indexes labels dict: {'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
create_generators n_labels: 6
create_generators n_labels_eff: 6
create_generators len_c1: 1
Check for no duplicates in test: (0=ok):
0.0
Check for no duplicates in val: (0=ok):
0
N of indexes in training set: 17000
N of indexes in validation set: 3000
N of indexes in test set: 0
Check - total per class: 20000
--create_generators, train indexes
batch_size: 3000
- Cut sample
bs: 3000
N_labels: 6
N_noise: 10
len_c1: 1
Train index length: 17000
--create_generators, validation indexes
- Cut sample
bs: 3000
N_labels: 6
N_noise: 10
len_c1: 1
Val index length: 3000
len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 17000, 3000, 6, 10

--DataSet Train
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 100
Closest k to k_max is 2.539859
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (100, 1)
Final i_max used is 100
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 17000
n_indexes (n of file IDs read for each batch): 50
batch size: 3000
n_batches : 340
For each batch we read 50 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 3000 training examples
Input batch size: 3000
N of batches to cover all file IDs: 340
len(fname_list), batch_size, n_noisy_samples, n_batches: 102000, 3000, 10, 340

--DataSet Validation
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 100
Closest k to k_max is 2.539859
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (100, 1)
Final i_max used is 100
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 3000
n_indexes (n of file IDs read for each batch): 50
batch size: 3000
n_batches : 60
For each batch we read 50 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 3000 training examples
Input batch size: 3000
N of batches to cover all file IDs: 60
len(fname_list), batch_size, n_noisy_samples, n_batches: 18000, 3000, 10, 60
------------ DONE ------------

------------ BUILDING MODEL ------------
Input shape (100, 4)
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 100, 4)]          0         
                                                                 
 conv1d_flipout (Conv1DFlip  (None, 46, 8)             648       
 out)                                                            
                                                                 
 max_pooling1d (MaxPooling1  (None, 23, 8)             0         
 D)                                                              
                                                                 
 batch_normalization (Batch  (None, 23, 8)             32        
 Normalization)                                                  
                                                                 
 conv1d_flipout_1 (Conv1DFl  (None, 10, 16)            1296      
 ipout)                                                          
                                                                 
 max_pooling1d_1 (MaxPoolin  (None, 9, 16)             0         
 g1D)                                                            
                                                                 
 batch_normalization_1 (Bat  (None, 9, 16)             64        
 chNormalization)                                                
                                                                 
 conv1d_flipout_2 (Conv1DFl  (None, 8, 32)             2080      
 ipout)                                                          
                                                                 
 batch_normalization_2 (Bat  (None, 8, 32)             128       
 chNormalization)                                                
                                                                 
 global_average_pooling1d (  (None, 32)                0         
 GlobalAveragePooling1D)                                         
                                                                 
 dense_flipout (DenseFlipou  (None, 32)                2080      
 t)                                                              
                                                                 
 batch_normalization_3 (Bat  (None, 32)                128       
 chNormalization)                                                
                                                                 
 dense_flipout_1 (DenseFlip  (None, 6)                 390       
 out)                                                            
                                                                 
=================================================================
Total params: 6846 (26.74 KB)
Trainable params: 6670 (26.05 KB)
Non-trainable params: 176 (704.00 Byte)
_________________________________________________________________
None
Found GPU at: /device:GPU:0
------------ TRAINING ------------

Features shape: (3000, 100, 4)
Labels shape: (3000, 6)
Initializing checkpoint from scratch.
Epoch 0
Validation loss decreased. Saved checkpoint for step 1: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-1
Time:  35.33s, ---- Loss: 0.7254, Acc.: 0.6238, Val. Loss: 1.9416, Val. Acc.: 0.4341

Epoch 1
Validation loss decreased. Saved checkpoint for step 2: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-2
Time:  2.48s, ---- Loss: 0.6160, Acc.: 0.7286, Val. Loss: 1.2021, Val. Acc.: 0.6006

Epoch 2
Validation loss decreased. Saved checkpoint for step 3: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-3
Time:  2.38s, ---- Loss: 0.5758, Acc.: 0.7505, Val. Loss: 0.7543, Val. Acc.: 0.7144

Epoch 3
Loss did not decrease. Count = 1
Time:  2.30s, ---- Loss: 0.5565, Acc.: 0.7618, Val. Loss: 0.8711, Val. Acc.: 0.6898

Epoch 4
Loss did not decrease. Count = 2
Time:  2.26s, ---- Loss: 0.5407, Acc.: 0.7685, Val. Loss: 0.8350, Val. Acc.: 0.7057

Epoch 5
Validation loss decreased. Saved checkpoint for step 6: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-4
Time:  2.37s, ---- Loss: 0.5371, Acc.: 0.7727, Val. Loss: 0.6637, Val. Acc.: 0.7495

Epoch 6
Validation loss decreased. Saved checkpoint for step 7: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-5
Time:  2.40s, ---- Loss: 0.5402, Acc.: 0.7756, Val. Loss: 0.6399, Val. Acc.: 0.7604

Epoch 7
Validation loss decreased. Saved checkpoint for step 8: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-6
Time:  2.38s, ---- Loss: 0.5423, Acc.: 0.7786, Val. Loss: 0.5999, Val. Acc.: 0.7705

Epoch 8
Loss did not decrease. Count = 1
Time:  2.29s, ---- Loss: 0.5336, Acc.: 0.7810, Val. Loss: 0.6032, Val. Acc.: 0.7699

Epoch 9
Loss did not decrease. Count = 2
Time:  2.27s, ---- Loss: 0.5275, Acc.: 0.7827, Val. Loss: 0.6324, Val. Acc.: 0.7620

Epoch 10
Validation loss decreased. Saved checkpoint for step 11: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-7
Time:  2.39s, ---- Loss: 0.5284, Acc.: 0.7841, Val. Loss: 0.5887, Val. Acc.: 0.7749

Epoch 11
Validation loss decreased. Saved checkpoint for step 12: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-8
Time:  2.44s, ---- Loss: 0.5269, Acc.: 0.7853, Val. Loss: 0.5823, Val. Acc.: 0.7795

Epoch 12
Loss did not decrease. Count = 1
Time:  2.29s, ---- Loss: 0.5319, Acc.: 0.7866, Val. Loss: 0.5889, Val. Acc.: 0.7778

Epoch 13
Loss did not decrease. Count = 2
Time:  2.35s, ---- Loss: 0.5227, Acc.: 0.7875, Val. Loss: 0.6029, Val. Acc.: 0.7731

Epoch 14
Loss did not decrease. Count = 3
Time:  2.34s, ---- Loss: 0.5203, Acc.: 0.7884, Val. Loss: 0.6102, Val. Acc.: 0.7705

Epoch 15
Loss did not decrease. Count = 4
Time:  2.28s, ---- Loss: 0.5187, Acc.: 0.7892, Val. Loss: 0.5906, Val. Acc.: 0.7751

Epoch 16
Loss did not decrease. Count = 5
Time:  2.29s, ---- Loss: 0.5098, Acc.: 0.7903, Val. Loss: 0.5867, Val. Acc.: 0.7794

Epoch 17
Loss did not decrease. Count = 6
Time:  2.25s, ---- Loss: 0.5114, Acc.: 0.7908, Val. Loss: 0.5982, Val. Acc.: 0.7782

Epoch 18
Loss did not decrease. Count = 7
Time:  2.25s, ---- Loss: 0.5055, Acc.: 0.7914, Val. Loss: 0.5857, Val. Acc.: 0.7799

Epoch 19
Validation loss decreased. Saved checkpoint for step 20: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-9
Time:  2.39s, ---- Loss: 0.5042, Acc.: 0.7923, Val. Loss: 0.5786, Val. Acc.: 0.7827

Epoch 20
Loss did not decrease. Count = 1
Time:  2.26s, ---- Loss: 0.5004, Acc.: 0.7930, Val. Loss: 0.5948, Val. Acc.: 0.7787

Epoch 21
Loss did not decrease. Count = 2
Time:  2.27s, ---- Loss: 0.4988, Acc.: 0.7934, Val. Loss: 0.5919, Val. Acc.: 0.7767

Epoch 22
Loss did not decrease. Count = 3
Time:  2.25s, ---- Loss: 0.4976, Acc.: 0.7940, Val. Loss: 0.5802, Val. Acc.: 0.7840

Epoch 23
Loss did not decrease. Count = 4
Time:  2.24s, ---- Loss: 0.4940, Acc.: 0.7944, Val. Loss: 0.6173, Val. Acc.: 0.7683

Epoch 24
Validation loss decreased. Saved checkpoint for step 25: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-10
Time:  2.37s, ---- Loss: 0.4941, Acc.: 0.7949, Val. Loss: 0.5642, Val. Acc.: 0.7876

Epoch 25
Loss did not decrease. Count = 1
Time:  2.27s, ---- Loss: 0.4885, Acc.: 0.7956, Val. Loss: 0.5670, Val. Acc.: 0.7863

Epoch 26
Loss did not decrease. Count = 2
Time:  2.26s, ---- Loss: 0.4880, Acc.: 0.7960, Val. Loss: 0.6197, Val. Acc.: 0.7689

Epoch 27
Loss did not decrease. Count = 3
Time:  2.22s, ---- Loss: 0.4878, Acc.: 0.7965, Val. Loss: 0.6088, Val. Acc.: 0.7714

Epoch 28
Loss did not decrease. Count = 4
Time:  2.22s, ---- Loss: 0.4850, Acc.: 0.7967, Val. Loss: 0.5808, Val. Acc.: 0.7804

Epoch 29
Loss did not decrease. Count = 5
Time:  2.22s, ---- Loss: 0.4866, Acc.: 0.7972, Val. Loss: 0.5658, Val. Acc.: 0.7865

Epoch 30
Loss did not decrease. Count = 6
Time:  2.24s, ---- Loss: 0.4850, Acc.: 0.7977, Val. Loss: 0.5814, Val. Acc.: 0.7808

Epoch 31
Validation loss decreased. Saved checkpoint for step 32: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-11
Time:  2.37s, ---- Loss: 0.4821, Acc.: 0.7982, Val. Loss: 0.5580, Val. Acc.: 0.7888

Epoch 32
Validation loss decreased. Saved checkpoint for step 33: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-12
Time:  2.36s, ---- Loss: 0.4850, Acc.: 0.7983, Val. Loss: 0.5464, Val. Acc.: 0.7930

Epoch 33
Loss did not decrease. Count = 1
Time:  2.22s, ---- Loss: 0.4784, Acc.: 0.7986, Val. Loss: 0.5503, Val. Acc.: 0.7918

Epoch 34
Validation loss decreased. Saved checkpoint for step 35: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-13
Time:  2.34s, ---- Loss: 0.4812, Acc.: 0.7991, Val. Loss: 0.5455, Val. Acc.: 0.7940

Epoch 35
Loss did not decrease. Count = 1
Time:  2.25s, ---- Loss: 0.4832, Acc.: 0.7991, Val. Loss: 0.5467, Val. Acc.: 0.7929

Epoch 36
Validation loss decreased. Saved checkpoint for step 37: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-14
Time:  2.40s, ---- Loss: 0.4794, Acc.: 0.7995, Val. Loss: 0.5442, Val. Acc.: 0.7948

Epoch 37
Validation loss decreased. Saved checkpoint for step 38: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-15
Time:  2.37s, ---- Loss: 0.4790, Acc.: 0.7998, Val. Loss: 0.5437, Val. Acc.: 0.7952

Epoch 38
Loss did not decrease. Count = 1
Time:  2.28s, ---- Loss: 0.4778, Acc.: 0.8001, Val. Loss: 0.5451, Val. Acc.: 0.7949

Epoch 39
Loss did not decrease. Count = 2
Time:  2.24s, ---- Loss: 0.4795, Acc.: 0.8004, Val. Loss: 0.5444, Val. Acc.: 0.7948

Epoch 40
Loss did not decrease. Count = 3
Time:  2.30s, ---- Loss: 0.4740, Acc.: 0.8003, Val. Loss: 0.5450, Val. Acc.: 0.7939

Epoch 41
Validation loss decreased. Saved checkpoint for step 42: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-16
Time:  2.43s, ---- Loss: 0.4748, Acc.: 0.8010, Val. Loss: 0.5436, Val. Acc.: 0.7957

Epoch 42
Validation loss decreased. Saved checkpoint for step 43: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-17
Time:  2.36s, ---- Loss: 0.4738, Acc.: 0.8010, Val. Loss: 0.5416, Val. Acc.: 0.7958

Epoch 43
Loss did not decrease. Count = 1
Time:  2.26s, ---- Loss: 0.4741, Acc.: 0.8013, Val. Loss: 0.5709, Val. Acc.: 0.7829

Epoch 44
Loss did not decrease. Count = 2
Time:  2.28s, ---- Loss: 0.4721, Acc.: 0.8011, Val. Loss: 0.5566, Val. Acc.: 0.7894

Epoch 45
Loss did not decrease. Count = 3
Time:  2.29s, ---- Loss: 0.4733, Acc.: 0.8015, Val. Loss: 0.5452, Val. Acc.: 0.7948

Epoch 46
Loss did not decrease. Count = 4
Time:  2.27s, ---- Loss: 0.4730, Acc.: 0.8018, Val. Loss: 0.5464, Val. Acc.: 0.7943

Epoch 47
Validation loss decreased. Saved checkpoint for step 48: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-18
Time:  2.38s, ---- Loss: 0.4730, Acc.: 0.8019, Val. Loss: 0.5415, Val. Acc.: 0.7964

Epoch 48
Loss did not decrease. Count = 1
Time:  2.27s, ---- Loss: 0.4712, Acc.: 0.8020, Val. Loss: 0.5457, Val. Acc.: 0.7945

Epoch 49
Loss did not decrease. Count = 2
Time:  2.26s, ---- Loss: 0.4709, Acc.: 0.8022, Val. Loss: 0.5422, Val. Acc.: 0.7963

Saving at models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/hist.png
Done in 3938.41s

 -------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune False
one_vs_all False
c_0 ['lcdm']
c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']
dataset_balanced False
include_last False
log_path models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5__log.txt
restore False
fname sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_
model_name custom
my_path None
DIR data/train
TEST_DIR data/test
models_dir models/
save_ckpt True
out_path_overwrite False
curves_folder data/curve_files_sys/theory_error
save_processed_spectra False
cache_dir False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
norm_data_name /planck_ee2.txt
normalization stdcosmo
sample_pace 1
k_max 2.5
k_min 0.0
i_max None
i_min None
add_noise True
n_noisy_samples 10
add_shot False
add_sys True
add_cosvar True
sigma_sys None
sys_scaled None
sys_factor None
sys_max None
sigma_curves 0.05
sigma_curves_default 0.05
rescale_curves uniform
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable False
unfreeze False
lr 0.05
drop 0.5
n_epochs 50
val_size 0.15
test_size 0.0
batch_size 3000
patience 20
GPU True
TPU False
decay 0.95
BatchNorm True
padding valid
shuffle True

------------ CREATING DATA GENERATORS ------------
labels : ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
Labels encoding: 
{'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
n_labels : 6
dgp - 20000 training examples
ds - 20000 training examples
fr - 20000 training examples
lcdm - 20000 training examples
rand - 20000 training examples
wcdm - 20000 training examples

N. of data files: 20000
get_all_indexes labels dict: {'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
create_generators n_labels: 6
create_generators n_labels_eff: 6
create_generators len_c1: 1
Check for no duplicates in test: (0=ok):
0.0
Check for no duplicates in val: (0=ok):
0
N of indexes in training set: 17000
N of indexes in validation set: 3000
N of indexes in test set: 0
Check - total per class: 20000
--create_generators, train indexes
batch_size: 3000
- Cut sample
bs: 3000
N_labels: 6
N_noise: 10
len_c1: 1
Train index length: 17000
--create_generators, validation indexes
- Cut sample
bs: 3000
N_labels: 6
N_noise: 10
len_c1: 1
Val index length: 3000
len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 17000, 3000, 6, 10

--DataSet Train
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 399
Closest k to k_max is 2.504942
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (399, 1)
Final i_max used is 399
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 17000
n_indexes (n of file IDs read for each batch): 50
batch size: 3000
n_batches : 340
For each batch we read 50 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 3000 training examples
Input batch size: 3000
N of batches to cover all file IDs: 340
len(fname_list), batch_size, n_noisy_samples, n_batches: 102000, 3000, 10, 340

--DataSet Validation
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 399
Closest k to k_max is 2.504942
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (399, 1)
Final i_max used is 399
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 3000
n_indexes (n of file IDs read for each batch): 50
batch size: 3000
n_batches : 60
For each batch we read 50 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 3000 training examples
Input batch size: 3000
N of batches to cover all file IDs: 60
len(fname_list), batch_size, n_noisy_samples, n_batches: 18000, 3000, 10, 60
------------ DONE ------------

------------ BUILDING MODEL ------------
Input shape (399, 4)
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 399, 4)]          0         
                                                                 
 conv1d_flipout (Conv1DFlip  (None, 195, 8)            648       
 out)                                                            
                                                                 
 max_pooling1d (MaxPooling1  (None, 97, 8)             0         
 D)                                                              
                                                                 
 batch_normalization (Batch  (None, 97, 8)             32        
 Normalization)                                                  
                                                                 
 conv1d_flipout_1 (Conv1DFl  (None, 47, 16)            1296      
 ipout)                                                          
                                                                 
 max_pooling1d_1 (MaxPoolin  (None, 46, 16)            0         
 g1D)                                                            
                                                                 
 batch_normalization_1 (Bat  (None, 46, 16)            64        
 chNormalization)                                                
                                                                 
 conv1d_flipout_2 (Conv1DFl  (None, 45, 32)            2080      
 ipout)                                                          
                                                                 
 batch_normalization_2 (Bat  (None, 45, 32)            128       
 chNormalization)                                                
                                                                 
 global_average_pooling1d (  (None, 32)                0         
 GlobalAveragePooling1D)                                         
                                                                 
 dense_flipout (DenseFlipou  (None, 32)                2080      
 t)                                                              
                                                                 
 batch_normalization_3 (Bat  (None, 32)                128       
 chNormalization)                                                
                                                                 
 dense_flipout_1 (DenseFlip  (None, 6)                 390       
 out)                                                            
                                                                 
=================================================================
Total params: 6846 (26.74 KB)
Trainable params: 6670 (26.05 KB)
Non-trainable params: 176 (704.00 Byte)
_________________________________________________________________
None
Found GPU at: /device:GPU:0
------------ TRAINING ------------

Features shape: (3000, 399, 4)
Labels shape: (3000, 6)
Initializing checkpoint from scratch.
Epoch 0
Validation loss decreased. Saved checkpoint for step 1: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-1
Time:  45.09s, ---- Loss: 0.7949, Acc.: 0.6148, Val. Loss: 1.1566, Val. Acc.: 0.5786

Epoch 1
Validation loss decreased. Saved checkpoint for step 2: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-2
Time:  4.02s, ---- Loss: 0.7009, Acc.: 0.7017, Val. Loss: 0.9438, Val. Acc.: 0.6562

Epoch 2
Loss did not decrease. Count = 1
Time:  3.86s, ---- Loss: 0.6504, Acc.: 0.7195, Val. Loss: 1.3130, Val. Acc.: 0.5220

Epoch 3
Loss did not decrease. Count = 2
Time:  3.88s, ---- Loss: 0.6168, Acc.: 0.7360, Val. Loss: 0.9851, Val. Acc.: 0.6504

Epoch 4
Loss did not decrease. Count = 3
Time:  3.84s, ---- Loss: 0.6037, Acc.: 0.7451, Val. Loss: 1.1635, Val. Acc.: 0.6070

Epoch 5
Validation loss decreased. Saved checkpoint for step 6: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-3
Time:  3.94s, ---- Loss: 0.6088, Acc.: 0.7503, Val. Loss: 0.7431, Val. Acc.: 0.7176

Epoch 6
Validation loss decreased. Saved checkpoint for step 7: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-4
Time:  3.99s, ---- Loss: 0.5904, Acc.: 0.7556, Val. Loss: 0.7351, Val. Acc.: 0.7180

Epoch 7
Loss did not decrease. Count = 1
Time:  3.85s, ---- Loss: 0.5770, Acc.: 0.7599, Val. Loss: 0.8017, Val. Acc.: 0.6977

Epoch 8
Validation loss decreased. Saved checkpoint for step 9: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-5
Time:  3.95s, ---- Loss: 0.5570, Acc.: 0.7637, Val. Loss: 0.7341, Val. Acc.: 0.7223

Epoch 9
Loss did not decrease. Count = 1
Time:  3.85s, ---- Loss: 0.5551, Acc.: 0.7670, Val. Loss: 0.7402, Val. Acc.: 0.7327

Epoch 10
Validation loss decreased. Saved checkpoint for step 11: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-6
Time:  3.96s, ---- Loss: 0.5559, Acc.: 0.7692, Val. Loss: 0.7096, Val. Acc.: 0.7414

Epoch 11
Loss did not decrease. Count = 1
Time:  3.83s, ---- Loss: 0.5554, Acc.: 0.7712, Val. Loss: 0.7109, Val. Acc.: 0.7394

Epoch 12
Loss did not decrease. Count = 2
Time:  3.85s, ---- Loss: 0.5529, Acc.: 0.7728, Val. Loss: 0.7607, Val. Acc.: 0.7320

Epoch 13
Validation loss decreased. Saved checkpoint for step 14: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-7
Time:  4.01s, ---- Loss: 0.5436, Acc.: 0.7747, Val. Loss: 0.6665, Val. Acc.: 0.7486

Epoch 14
Loss did not decrease. Count = 1
Time:  4.12s, ---- Loss: 0.5439, Acc.: 0.7765, Val. Loss: 0.7016, Val. Acc.: 0.7414

Epoch 15
Loss did not decrease. Count = 2
Time:  3.87s, ---- Loss: 0.5537, Acc.: 0.7777, Val. Loss: 0.7411, Val. Acc.: 0.7313

Epoch 16
Validation loss decreased. Saved checkpoint for step 17: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-8
Time:  3.96s, ---- Loss: 0.5429, Acc.: 0.7791, Val. Loss: 0.6389, Val. Acc.: 0.7624

Epoch 17
Loss did not decrease. Count = 1
Time:  3.83s, ---- Loss: 0.5364, Acc.: 0.7804, Val. Loss: 0.6494, Val. Acc.: 0.7551

Epoch 18
Loss did not decrease. Count = 2
Time:  3.86s, ---- Loss: 0.5312, Acc.: 0.7818, Val. Loss: 0.6546, Val. Acc.: 0.7576

Epoch 19
Loss did not decrease. Count = 3
Time:  3.82s, ---- Loss: 0.5311, Acc.: 0.7832, Val. Loss: 0.6944, Val. Acc.: 0.7406

Epoch 20
Loss did not decrease. Count = 4
Time:  3.82s, ---- Loss: 0.5416, Acc.: 0.7842, Val. Loss: 0.7023, Val. Acc.: 0.7387

Epoch 21
Loss did not decrease. Count = 5
Time:  3.87s, ---- Loss: 0.5257, Acc.: 0.7848, Val. Loss: 0.7700, Val. Acc.: 0.7123

Epoch 22
Loss did not decrease. Count = 6
Time:  3.84s, ---- Loss: 0.5286, Acc.: 0.7854, Val. Loss: 0.8073, Val. Acc.: 0.7066

Epoch 23
Loss did not decrease. Count = 7
Time:  3.84s, ---- Loss: 0.5217, Acc.: 0.7862, Val. Loss: 0.7112, Val. Acc.: 0.7416

Epoch 24
Loss did not decrease. Count = 8
Time:  3.85s, ---- Loss: 0.5192, Acc.: 0.7872, Val. Loss: 1.0920, Val. Acc.: 0.6395

Epoch 25
Loss did not decrease. Count = 9
Time:  3.83s, ---- Loss: 0.5169, Acc.: 0.7876, Val. Loss: 0.7024, Val. Acc.: 0.7464

Epoch 26
Loss did not decrease. Count = 10
Time:  3.81s, ---- Loss: 0.5133, Acc.: 0.7886, Val. Loss: 0.9050, Val. Acc.: 0.6835

Epoch 27
Loss did not decrease. Count = 11
Time:  3.85s, ---- Loss: 0.5090, Acc.: 0.7894, Val. Loss: 0.9932, Val. Acc.: 0.6624

Epoch 28
Loss did not decrease. Count = 12
Time:  3.82s, ---- Loss: 0.5060, Acc.: 0.7898, Val. Loss: 0.8528, Val. Acc.: 0.6934

Epoch 29
Loss did not decrease. Count = 13
Time:  3.82s, ---- Loss: 0.5070, Acc.: 0.7905, Val. Loss: 0.7105, Val. Acc.: 0.7414

Epoch 30
Loss did not decrease. Count = 14
Time:  3.87s, ---- Loss: 0.5003, Acc.: 0.7908, Val. Loss: 0.8177, Val. Acc.: 0.7116

Epoch 31
Loss did not decrease. Count = 15
Time:  3.84s, ---- Loss: 0.5065, Acc.: 0.7918, Val. Loss: 0.6670, Val. Acc.: 0.7563

Epoch 32
Loss did not decrease. Count = 16
Time:  3.88s, ---- Loss: 0.5011, Acc.: 0.7920, Val. Loss: 0.6892, Val. Acc.: 0.7481

Epoch 33
Loss did not decrease. Count = 17
Time:  3.89s, ---- Loss: 0.4979, Acc.: 0.7926, Val. Loss: 0.6504, Val. Acc.: 0.7607

Epoch 34
Validation loss decreased. Saved checkpoint for step 35: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-9
Time:  3.96s, ---- Loss: 0.4987, Acc.: 0.7928, Val. Loss: 0.6277, Val. Acc.: 0.7688

Epoch 35
Validation loss decreased. Saved checkpoint for step 36: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-10
Time:  3.96s, ---- Loss: 0.4967, Acc.: 0.7935, Val. Loss: 0.5960, Val. Acc.: 0.7807

Epoch 36
Loss did not decrease. Count = 1
Time:  3.84s, ---- Loss: 0.4969, Acc.: 0.7941, Val. Loss: 0.6058, Val. Acc.: 0.7778

Epoch 37
Loss did not decrease. Count = 2
Time:  3.82s, ---- Loss: 0.4944, Acc.: 0.7944, Val. Loss: 0.6443, Val. Acc.: 0.7641

Epoch 38
Loss did not decrease. Count = 3
Time:  3.83s, ---- Loss: 0.4956, Acc.: 0.7948, Val. Loss: 0.6351, Val. Acc.: 0.7657

Epoch 39
Loss did not decrease. Count = 4
Time:  3.87s, ---- Loss: 0.4940, Acc.: 0.7951, Val. Loss: 0.6364, Val. Acc.: 0.7654

Epoch 40
Loss did not decrease. Count = 5
Time:  3.82s, ---- Loss: 0.4934, Acc.: 0.7959, Val. Loss: 0.6173, Val. Acc.: 0.7717

Epoch 41
Loss did not decrease. Count = 6
Time:  3.82s, ---- Loss: 0.4936, Acc.: 0.7957, Val. Loss: 0.6136, Val. Acc.: 0.7736

Epoch 42
Loss did not decrease. Count = 7
Time:  3.88s, ---- Loss: 0.4933, Acc.: 0.7963, Val. Loss: 0.6057, Val. Acc.: 0.7767

Epoch 43
Loss did not decrease. Count = 8
Time:  3.84s, ---- Loss: 0.4916, Acc.: 0.7964, Val. Loss: 0.6037, Val. Acc.: 0.7776

Epoch 44
Loss did not decrease. Count = 9
Time:  3.81s, ---- Loss: 0.4896, Acc.: 0.7967, Val. Loss: 0.6164, Val. Acc.: 0.7728

Epoch 45
Validation loss decreased. Saved checkpoint for step 46: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-11
Time:  3.97s, ---- Loss: 0.4909, Acc.: 0.7970, Val. Loss: 0.5886, Val. Acc.: 0.7835

Epoch 46
Loss did not decrease. Count = 1
Time:  3.83s, ---- Loss: 0.4887, Acc.: 0.7974, Val. Loss: 0.6122, Val. Acc.: 0.7743

Epoch 47
Loss did not decrease. Count = 2
Time:  3.87s, ---- Loss: 0.4880, Acc.: 0.7972, Val. Loss: 0.5917, Val. Acc.: 0.7829

Epoch 48
Validation loss decreased. Saved checkpoint for step 49: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-12
Time:  4.00s, ---- Loss: 0.4858, Acc.: 0.7976, Val. Loss: 0.5825, Val. Acc.: 0.7864

Epoch 49
Loss did not decrease. Count = 1
Time:  3.82s, ---- Loss: 0.4844, Acc.: 0.7981, Val. Loss: 0.5847, Val. Acc.: 0.7858

Saving at models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/hist.png
Done in 4200.01s
