2024-04-01 04:24:40.831977: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-01 04:24:40.832027: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-01 04:24:40.833916: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-01 04:24:41.883889: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-04-01 04:24:44.304948: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
using 1D layers and 4 channels
/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
loc = add_variable_fn(
/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
untransformed_scale = add_variable_fn(
Expected output dimension after layer: conv1d_flipout : 15
Expected output dimension after layer: conv1d_flipout_1 : 5
Expected output dimension after layer: conv1d_flipout_2 : 4
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1711949255.423778   67738 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
Creating directory models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_
-------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune False
one_vs_all False
c_0 ['lcdm']
c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']
dataset_balanced False
include_last False
log_path models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5__log.txt
restore False
fname sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_
model_name custom
my_path None
DIR data/train
TEST_DIR data/test
models_dir models/
save_ckpt True
out_path_overwrite False
curves_folder data/curve_files_sys/theory_error
save_processed_spectra False
cache_dir False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
norm_data_name /planck_ee2.txt
normalization stdcosmo
sample_pace 4
k_max 2.5
k_min 0.06
i_max None
i_min None
add_noise True
n_noisy_samples 10
add_shot False
add_sys True
add_cosvar True
sigma_sys None
sys_scaled None
sys_factor None
sys_max None
sigma_curves 0.05
sigma_curves_default 0.05
rescale_curves uniform
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable False
unfreeze False
lr 0.05
drop 0.5
n_epochs 50
val_size 0.15
test_size 0.0
batch_size 3000
patience 20
GPU True
TPU False
decay 0.95
BatchNorm True
padding valid
shuffle True
------------ CREATING DATA GENERATORS ------------
labels : ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
Labels encoding:
{'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
n_labels : 6
dgp - 20000 training examples
ds - 20000 training examples
fr - 20000 training examples
lcdm - 20000 training examples
rand - 20000 training examples
wcdm - 20000 training examples
N. of data files: 20000
get_all_indexes labels dict: {'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
create_generators n_labels: 6
create_generators n_labels_eff: 6
create_generators len_c1: 1
Check for no duplicates in test: (0=ok):
0.0
Check for no duplicates in val: (0=ok):
0
N of indexes in training set: 17000
N of indexes in validation set: 3000
N of indexes in test set: 0
Check - total per class: 20000
--create_generators, train indexes
batch_size: 3000
- Cut sample
bs: 3000
N_labels: 6
N_noise: 10
len_c1: 1
Train index length: 17000
--create_generators, validation indexes
- Cut sample
bs: 3000
N_labels: 6
N_noise: 10
len_c1: 1
Val index length: 3000
len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 17000, 3000, 6, 10
--DataSet Train
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 100
Closest k to k_max is 2.539859
Specified k_min is 0.06
Corresponding i_min is 32
Closest k to k_min is 0.0588219
New data dim: (68, 1)
Final i_max used is 100
Final i_min used is 32
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 17000
n_indexes (n of file IDs read for each batch): 50
batch size: 3000
n_batches : 340
For each batch we read 50 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 3000 training examples
Input batch size: 3000
N of batches to cover all file IDs: 340
len(fname_list), batch_size, n_noisy_samples, n_batches: 102000, 3000, 10, 340
--DataSet Validation
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 100
Closest k to k_max is 2.539859
Specified k_min is 0.06
Corresponding i_min is 32
Closest k to k_min is 0.0588219
New data dim: (68, 1)
Final i_max used is 100
Final i_min used is 32
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 3000
n_indexes (n of file IDs read for each batch): 50
batch size: 3000
n_batches : 60
For each batch we read 50 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 3000 training examples
Input batch size: 3000
N of batches to cover all file IDs: 60
len(fname_list), batch_size, n_noisy_samples, n_batches: 18000, 3000, 10, 60
------------ DONE ------------
------------ BUILDING MODEL ------------
Input shape (68, 4)
Model: "model"
_________________________________________________________________
Layer (type)                Output Shape              Param #
=================================================================
input_1 (InputLayer)        [(None, 68, 4)]           0
conv1d_flipout (Conv1DFlip  (None, 30, 8)             648
out)
max_pooling1d (MaxPooling1  (None, 15, 8)             0
D)
batch_normalization (Batch  (None, 15, 8)             32
Normalization)
conv1d_flipout_1 (Conv1DFl  (None, 6, 16)             1296
ipout)
max_pooling1d_1 (MaxPoolin  (None, 5, 16)             0
g1D)
batch_normalization_1 (Bat  (None, 5, 16)             64
chNormalization)
conv1d_flipout_2 (Conv1DFl  (None, 4, 32)             2080
ipout)
batch_normalization_2 (Bat  (None, 4, 32)             128
chNormalization)
global_average_pooling1d (  (None, 32)                0
GlobalAveragePooling1D)
dense_flipout (DenseFlipou  (None, 32)                2080
t)
batch_normalization_3 (Bat  (None, 32)                128
chNormalization)
dense_flipout_1 (DenseFlip  (None, 6)                 390
out)
=================================================================
Total params: 6846 (26.74 KB)
Trainable params: 6670 (26.05 KB)
Non-trainable params: 176 (704.00 Byte)
_________________________________________________________________
None
Found GPU at: /device:GPU:0
------------ TRAINING ------------
Features shape: (3000, 68, 4)
Labels shape: (3000, 6)
Initializing checkpoint from scratch.
Epoch 0
Validation loss decreased. Saved checkpoint for step 1: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-1
Time:  35.95s, ---- Loss: 0.7731, Acc.: 0.5908, Val. Loss: 1.4879, Val. Acc.: 0.4666
Epoch 1
Loss did not decrease. Count = 1
Time:  2.11s, ---- Loss: 0.6480, Acc.: 0.7042, Val. Loss: 3.5866, Val. Acc.: 0.2860
Epoch 2
Validation loss decreased. Saved checkpoint for step 3: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-2
Time:  2.24s, ---- Loss: 0.6240, Acc.: 0.7281, Val. Loss: 1.4350, Val. Acc.: 0.5314
Epoch 3
Loss did not decrease. Count = 1
Time:  2.17s, ---- Loss: 0.6042, Acc.: 0.7365, Val. Loss: 1.8117, Val. Acc.: 0.4949
Epoch 4
Loss did not decrease. Count = 2
Time:  2.13s, ---- Loss: 0.5932, Acc.: 0.7432, Val. Loss: 1.8301, Val. Acc.: 0.5131
Epoch 5
Validation loss decreased. Saved checkpoint for step 6: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-3
Time:  2.25s, ---- Loss: 0.5680, Acc.: 0.7477, Val. Loss: 1.1588, Val. Acc.: 0.6072
Epoch 6
Validation loss decreased. Saved checkpoint for step 7: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-4
Time:  2.24s, ---- Loss: 0.5657, Acc.: 0.7515, Val. Loss: 0.7164, Val. Acc.: 0.7237
Epoch 7
Validation loss decreased. Saved checkpoint for step 8: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-5
Time:  2.27s, ---- Loss: 0.5548, Acc.: 0.7551, Val. Loss: 0.6793, Val. Acc.: 0.7408
Epoch 8
Loss did not decrease. Count = 1
Time:  2.15s, ---- Loss: 0.5520, Acc.: 0.7582, Val. Loss: 0.7264, Val. Acc.: 0.7221
Epoch 9
Loss did not decrease. Count = 2
Time:  2.14s, ---- Loss: 0.5436, Acc.: 0.7603, Val. Loss: 0.7572, Val. Acc.: 0.7181
Epoch 10
Loss did not decrease. Count = 3
Time:  2.12s, ---- Loss: 0.5370, Acc.: 0.7619, Val. Loss: 0.8381, Val. Acc.: 0.7093
Epoch 11
Loss did not decrease. Count = 4
Time:  2.10s, ---- Loss: 0.5325, Acc.: 0.7633, Val. Loss: 0.7829, Val. Acc.: 0.7217
Epoch 12
Loss did not decrease. Count = 5
Time:  2.12s, ---- Loss: 0.5297, Acc.: 0.7644, Val. Loss: 0.7960, Val. Acc.: 0.7106
Epoch 13
Validation loss decreased. Saved checkpoint for step 14: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-6
Time:  2.29s, ---- Loss: 0.5300, Acc.: 0.7655, Val. Loss: 0.6708, Val. Acc.: 0.7418
Epoch 14
Validation loss decreased. Saved checkpoint for step 15: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-7
Time:  2.26s, ---- Loss: 0.5288, Acc.: 0.7660, Val. Loss: 0.6683, Val. Acc.: 0.7457
Epoch 15
Validation loss decreased. Saved checkpoint for step 16: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-8
Time:  2.23s, ---- Loss: 0.5247, Acc.: 0.7673, Val. Loss: 0.6395, Val. Acc.: 0.7567
Epoch 16
Validation loss decreased. Saved checkpoint for step 17: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-9
Time:  2.23s, ---- Loss: 0.5304, Acc.: 0.7679, Val. Loss: 0.6277, Val. Acc.: 0.7622
Epoch 17
Loss did not decrease. Count = 1
Time:  2.09s, ---- Loss: 0.5255, Acc.: 0.7686, Val. Loss: 0.6455, Val. Acc.: 0.7550
Epoch 18
Loss did not decrease. Count = 2
Time:  2.13s, ---- Loss: 0.5238, Acc.: 0.7694, Val. Loss: 0.6290, Val. Acc.: 0.7617
Epoch 19
Loss did not decrease. Count = 3
Time:  2.14s, ---- Loss: 0.5278, Acc.: 0.7698, Val. Loss: 0.6554, Val. Acc.: 0.7515
Epoch 20
Loss did not decrease. Count = 4
Time:  2.12s, ---- Loss: 0.5178, Acc.: 0.7702, Val. Loss: 0.6588, Val. Acc.: 0.7483
Epoch 21
Loss did not decrease. Count = 5
Time:  2.12s, ---- Loss: 0.5222, Acc.: 0.7708, Val. Loss: 0.6294, Val. Acc.: 0.7616
Epoch 22
Loss did not decrease. Count = 6
Time:  2.13s, ---- Loss: 0.5202, Acc.: 0.7715, Val. Loss: 0.6521, Val. Acc.: 0.7486
Epoch 23
Validation loss decreased. Saved checkpoint for step 24: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-10
Time:  2.26s, ---- Loss: 0.5173, Acc.: 0.7721, Val. Loss: 0.6249, Val. Acc.: 0.7642
Epoch 24
Loss did not decrease. Count = 1
Time:  2.15s, ---- Loss: 0.5216, Acc.: 0.7724, Val. Loss: 0.6446, Val. Acc.: 0.7538
Epoch 25
Loss did not decrease. Count = 2
Time:  2.15s, ---- Loss: 0.5182, Acc.: 0.7730, Val. Loss: 0.6316, Val. Acc.: 0.7583
Epoch 26
Loss did not decrease. Count = 3
Time:  2.17s, ---- Loss: 0.5171, Acc.: 0.7734, Val. Loss: 0.6258, Val. Acc.: 0.7624
Epoch 27
Validation loss decreased. Saved checkpoint for step 28: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-11
Time:  2.30s, ---- Loss: 0.5168, Acc.: 0.7739, Val. Loss: 0.6216, Val. Acc.: 0.7644
Epoch 28
Validation loss decreased. Saved checkpoint for step 29: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-12
Time:  2.28s, ---- Loss: 0.5146, Acc.: 0.7742, Val. Loss: 0.6176, Val. Acc.: 0.7664
Epoch 29
Loss did not decrease. Count = 1
Time:  2.21s, ---- Loss: 0.5177, Acc.: 0.7745, Val. Loss: 0.6200, Val. Acc.: 0.7658
Epoch 30
Loss did not decrease. Count = 2
Time:  2.14s, ---- Loss: 0.5143, Acc.: 0.7752, Val. Loss: 0.6301, Val. Acc.: 0.7621
Epoch 31
Validation loss decreased. Saved checkpoint for step 32: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-13
Time:  2.27s, ---- Loss: 0.5132, Acc.: 0.7755, Val. Loss: 0.6175, Val. Acc.: 0.7681
Epoch 32
Loss did not decrease. Count = 1
Time:  2.13s, ---- Loss: 0.5144, Acc.: 0.7757, Val. Loss: 0.6330, Val. Acc.: 0.7609
Epoch 33
Loss did not decrease. Count = 2
Time:  2.14s, ---- Loss: 0.5128, Acc.: 0.7760, Val. Loss: 0.6176, Val. Acc.: 0.7660
Epoch 34
Loss did not decrease. Count = 3
Time:  2.17s, ---- Loss: 0.5124, Acc.: 0.7763, Val. Loss: 0.6190, Val. Acc.: 0.7651
Epoch 35
Validation loss decreased. Saved checkpoint for step 36: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-14
Time:  2.29s, ---- Loss: 0.5094, Acc.: 0.7765, Val. Loss: 0.6162, Val. Acc.: 0.7657
Epoch 36
Validation loss decreased. Saved checkpoint for step 37: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-15
Time:  2.26s, ---- Loss: 0.5072, Acc.: 0.7770, Val. Loss: 0.6124, Val. Acc.: 0.7678
Epoch 37
Loss did not decrease. Count = 1
Time:  2.13s, ---- Loss: 0.5092, Acc.: 0.7771, Val. Loss: 0.6582, Val. Acc.: 0.7462
Epoch 38
Loss did not decrease. Count = 2
Time:  2.12s, ---- Loss: 0.5096, Acc.: 0.7773, Val. Loss: 0.6553, Val. Acc.: 0.7476
Epoch 39
Validation loss decreased. Saved checkpoint for step 40: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-16
Time:  2.26s, ---- Loss: 0.5089, Acc.: 0.7777, Val. Loss: 0.6100, Val. Acc.: 0.7694
Epoch 40
Validation loss decreased. Saved checkpoint for step 41: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-17
Time:  2.25s, ---- Loss: 0.5061, Acc.: 0.7779, Val. Loss: 0.6096, Val. Acc.: 0.7702
Epoch 41
Validation loss decreased. Saved checkpoint for step 42: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-18
Time:  2.25s, ---- Loss: 0.5073, Acc.: 0.7780, Val. Loss: 0.6073, Val. Acc.: 0.7706
Epoch 42
Validation loss decreased. Saved checkpoint for step 43: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-19
Time:  2.26s, ---- Loss: 0.5058, Acc.: 0.7785, Val. Loss: 0.6057, Val. Acc.: 0.7711
Epoch 43
Loss did not decrease. Count = 1
Time:  2.11s, ---- Loss: 0.5046, Acc.: 0.7787, Val. Loss: 0.6201, Val. Acc.: 0.7657
Epoch 44
Validation loss decreased. Saved checkpoint for step 45: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-20
Time:  2.25s, ---- Loss: 0.5043, Acc.: 0.7788, Val. Loss: 0.6041, Val. Acc.: 0.7726
Epoch 45
Validation loss decreased. Saved checkpoint for step 46: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/tf_ckpts/ckpt-21
Time:  2.30s, ---- Loss: 0.5048, Acc.: 0.7790, Val. Loss: 0.6020, Val. Acc.: 0.7736
Epoch 46
Loss did not decrease. Count = 1
Time:  2.13s, ---- Loss: 0.5046, Acc.: 0.7791, Val. Loss: 0.6055, Val. Acc.: 0.7724
Epoch 47
Loss did not decrease. Count = 2
Time:  2.16s, ---- Loss: 0.5056, Acc.: 0.7794, Val. Loss: 0.6036, Val. Acc.: 0.7734
Epoch 48
Loss did not decrease. Count = 3
Time:  2.20s, ---- Loss: 0.5042, Acc.: 0.7795, Val. Loss: 0.6043, Val. Acc.: 0.7727
Epoch 49
Loss did not decrease. Count = 4
Time:  2.15s, ---- Loss: 0.5047, Acc.: 0.7796, Val. Loss: 0.6024, Val. Acc.: 0.7736
Saving at models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.06_kmax-2.5_/hist.png
Done in 3908.95s
