2024-04-06 21:06:48.699980: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-06 21:06:48.700031: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-06 21:06:48.701421: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-06 21:06:49.757999: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-04-06 21:06:52.330482: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
using 1D layers and 4 channels
/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
loc = add_variable_fn(
/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
untransformed_scale = add_variable_fn(
Expected output dimension after layer: conv1d_flipout : 7
Expected output dimension after layer: conv1d_flipout_1 : 1
Traceback (most recent call last):
File "/content/SeniorHonoursProject/BaCoN-II/train.py", line 825, in <module>
main()
File "/content/SeniorHonoursProject/BaCoN-II/train.py", line 607, in main
model=make_model(     model_name=model_name,
File "/content/SeniorHonoursProject/BaCoN-II/models.py", line 21, in make_model
return make_custom_model(**params)
File "/content/SeniorHonoursProject/BaCoN-II/models.py", line 107, in make_custom_model
x = conv1(x)
File "/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py", line 70, in error_handler
raise e.with_traceback(filtered_tb) from None
File "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/conv_variational.py", line 226, in call
outputs = self._apply_variational_kernel(inputs)
File "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/conv_variational.py", line 1071, in _apply_variational_kernel
outputs = self._convolution_op(
ValueError: Exception encountered when calling layer "conv1d_flipout_2" (type Conv1DFlipout).
Negative dimension size caused by subtracting 2 from 1 for '{{node conv1d_flipout_2/conv1d}} = Conv2D[T=DT_FLOAT, data_format="NHWC", dilations=[1, 1, 1, 1], explicit_paddings=[], padding="VALID", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_flipout_2/conv1d/ExpandDims, conv1d_flipout_2/conv1d/ExpandDims_1)' with input shapes: [?,1,1,16], [1,2,16,32].
Call arguments received by layer "conv1d_flipout_2" (type Conv1DFlipout):
â€¢ inputs=tf.Tensor(shape=(None, 1, 16), dtype=float32)
Creating directory models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.5_kmax-2.5_
-------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune False
one_vs_all False
c_0 ['lcdm']
c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']
dataset_balanced False
include_last False
log_path models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.5_kmax-2.5__log.txt
restore False
fname sixlabel_20k_EE2_equalexamples-randoms_kmin-1.5_kmax-2.5_
model_name custom
my_path None
DIR data/train
TEST_DIR
models_dir models/
save_ckpt True
out_path_overwrite False
curves_folder data/curve_files_sys/theory_error
save_processed_spectra False
cache_dir False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
norm_data_name /planck_ee2.txt
normalization stdcosmo
sample_pace 1
k_max 2.5
k_min 1.5
i_max None
i_min None
add_noise True
n_noisy_samples 10
add_shot False
add_sys True
add_cosvar True
sigma_sys None
sys_scaled None
sys_factor None
sys_max None
sigma_curves 0.05
sigma_curves_default 0.05
rescale_curves uniform
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable False
unfreeze False
lr 0.01
drop 0.5
n_epochs 100
val_size 0.15
test_size 0.0
batch_size 12000
patience 20
GPU True
TPU False
decay 0.95
BatchNorm True
padding valid
shuffle True
------------ CREATING DATA GENERATORS ------------
labels : ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
Labels encoding:
{'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
n_labels : 6
dgp - 20000 training examples
ds - 20000 training examples
fr - 20000 training examples
lcdm - 20000 training examples
rand - 20000 training examples
wcdm - 20000 training examples
N. of data files: 20000
get_all_indexes labels dict: {'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
create_generators n_labels: 6
create_generators n_labels_eff: 6
create_generators len_c1: 1
Check for no duplicates in test: (0=ok):
0.0
Check for no duplicates in val: (0=ok):
0
N of indexes in training set: 17000
N of indexes in validation set: 3000
N of indexes in test set: 0
Check - total per class: 20000
--create_generators, train indexes
batch_size: 12000
- Cut sample
bs: 12000
N_labels: 6
N_noise: 10
len_c1: 1
Train index length: 17000
--create_generators, validation indexes
- Cut sample
bs: 12000
N_labels: 6
N_noise: 10
len_c1: 1
Val index length: 3000
len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 17000, 12000, 6, 10
--DataSet Train
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 399
Closest k to k_max is 2.504942
Specified k_min is 1.5
Corresponding i_min is 362
Closest k to k_min is 1.500903
New data dim: (37, 1)
Final i_max used is 399
Final i_min used is 362
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 17000
n_indexes (n of file IDs read for each batch): 200
batch size: 12000
n_batches : 85
For each batch we read 200 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 12000 training examples
Input batch size: 12000
N of batches to cover all file IDs: 85
len(fname_list), batch_size, n_noisy_samples, n_batches: 102000, 12000, 10, 85
--DataSet Validation
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 399
Closest k to k_max is 2.504942
Specified k_min is 1.5
Corresponding i_min is 362
Closest k to k_min is 1.500903
New data dim: (37, 1)
Final i_max used is 399
Final i_min used is 362
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 3000
n_indexes (n of file IDs read for each batch): 200
batch size: 12000
n_batches : 15
For each batch we read 200 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 12000 training examples
Input batch size: 12000
N of batches to cover all file IDs: 15
len(fname_list), batch_size, n_noisy_samples, n_batches: 18000, 12000, 10, 15
------------ DONE ------------
------------ BUILDING MODEL ------------
Input shape (37, 4)
