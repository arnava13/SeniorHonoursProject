
 -------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune False
one_vs_all False
c_0 ['lcdm']
c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']
dataset_balanced False
include_last False
log_path models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0__log.txt
restore False
fname sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_
model_name custom
my_path None
DIR data/ds_ee2_train_1k_lcdm_rands
TEST_DIR data/test_data/
models_dir models/
save_ckpt True
out_path_overwrite False
curves_folder data/curve_files_sys/theory_error
save_processed_spectra False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
norm_data_name /planck_ee2.txt
normalization stdcosmo
sample_pace 4
k_max 5.0
k_min 0.0
i_max None
i_min None
add_noise True
n_noisy_samples 10
add_shot False
add_sys True
add_cosvar True
sigma_sys None
sys_scaled None
sys_factor None
sys_max None
sigma_curves 0.05
sigma_curves_default 0.05
rescale_curves uniform
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable False
unfreeze False
lr 0.01
drop 0.5
n_epochs 50
val_size 0.15
test_size 0.0
batch_size 3000
patience 50
GPU True
decay 0.95
BatchNorm True

------------ CREATING DATA GENERATORS ------------
labels : ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
Labels encoding: 
{'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
n_labels : 6
dgp - 1000 training examples
ds - 1000 training examples
fr - 1000 training examples
lcdm - 1000 training examples
rand - 1000 training examples
wcdm - 1000 training examples

N. of data files: 1000
get_all_indexes labels dict: {'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
create_generators n_labels: 6
create_generators n_labels_eff: 6
create_generators len_c1: 1
Check for no duplicates in test: (0=ok):
0.0
Check for no duplicates in val: (0=ok):
0
N of files in training set: 850
N of files in validation set: 150
N of files in test set: 0
Check - total: 1000
--create_generators, train indexes
batch_size: 3000
- Cut sample
bs: 3000
N_labels: 6
N_noise: 10
len_c1: 1
Train index length: 850
--create_generators, validation indexes
- Cut sample
bs: 3000
N_labels: 6
N_noise: 10
len_c1: 1
Val index length: 150
len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 850, 3000, 6, 10

--DataGenerator Train
Data Generator Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 5.0
Corresponding i_max is 112
Closest k to k_max is 4.936132
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (112, 1)
Final i_max used is 112
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 850
n_indexes (n of file IDs read for each batch): 50
batch size: 3000
n_batches : 17
For each batch we read 50 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 3000 training examples
Input batch size: 3000
N of batches to cover all file IDs: 17

--DataGenerator Validation
Data Generator Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 5.0
Corresponding i_max is 112
Closest k to k_max is 4.936132
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (112, 1)
Final i_max used is 112
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 150
n_indexes (n of file IDs read for each batch): 50
batch size: 3000
n_batches : 3
For each batch we read 50 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 3000 training examples
Input batch size: 3000
N of batches to cover all file IDs: 3
------------ DONE ------------

------------ BUILDING MODEL ------------
Input shape (112, 4)
using 1D layers and 4 channels
Expected output dimension of layer conv1d_flipout: 52.0
Expected output dimension of layer max_pooling1d: 26.0
Expected output dimension of layer conv1d_flipout_1: 11.5
Expected output dimension of layer max_pooling1d_1: 10.5
Expected output dimension of layer conv1d_flipout_2: 9.5
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 112, 4)]          0         
                                                                 
 conv1d_flipout (Conv1DFlip  (None, 52, 8)             648       
 out)                                                            
                                                                 
 max_pooling1d (MaxPooling1  (None, 26, 8)             0         
 D)                                                              
                                                                 
 batch_normalization (Batch  (None, 26, 8)             32        
 Normalization)                                                  
                                                                 
 conv1d_flipout_1 (Conv1DFl  (None, 11, 16)            1296      
 ipout)                                                          
                                                                 
 max_pooling1d_1 (MaxPoolin  (None, 10, 16)            0         
 g1D)                                                            
                                                                 
 batch_normalization_1 (Bat  (None, 10, 16)            64        
 chNormalization)                                                
                                                                 
 conv1d_flipout_2 (Conv1DFl  (None, 9, 32)             2080      
 ipout)                                                          
                                                                 
 batch_normalization_2 (Bat  (None, 9, 32)             128       
 chNormalization)                                                
                                                                 
 global_average_pooling1d (  (None, 32)                0         
 GlobalAveragePooling1D)                                         
                                                                 
 dense_flipout (DenseFlipou  (None, 32)                2080      
 t)                                                              
                                                                 
 batch_normalization_3 (Bat  (None, 32)                128       
 chNormalization)                                                
                                                                 
 dense_flipout_1 (DenseFlip  (None, 6)                 390       
 out)                                                            
                                                                 
=================================================================
Total params: 6846 (26.74 KB)
Trainable params: 6670 (26.05 KB)
Non-trainable params: 176 (704.00 Byte)
_________________________________________________________________
None
Found GPU at: /device:GPU:0
------------ TRAINING ------------

Features shape: (3000, 112, 4)
Labels shape: (3000, 6)
Initializing checkpoint from scratch.
Epoch 0
Validation loss decreased. Saved checkpoint for step 1: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-1
Time:  99.51s, ---- Loss: 1.6239, Acc.: 0.3016, Val. Loss: 2.6752, Val. Acc.: 0.1900

Epoch 1
Loss did not decrease. Count = 1
Time:  86.80s, ---- Loss: 1.3846, Acc.: 0.4583, Val. Loss: 2.6787, Val. Acc.: 0.1733

Epoch 2
Validation loss decreased. Saved checkpoint for step 3: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-2
Time:  86.46s, ---- Loss: 1.2525, Acc.: 0.5262, Val. Loss: 2.6252, Val. Acc.: 0.1843

Epoch 3
Validation loss decreased. Saved checkpoint for step 4: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-3
Time:  86.29s, ---- Loss: 1.2022, Acc.: 0.5584, Val. Loss: 2.5233, Val. Acc.: 0.2340

Epoch 4
Validation loss decreased. Saved checkpoint for step 5: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-4
Time:  85.87s, ---- Loss: 1.1827, Acc.: 0.5786, Val. Loss: 2.4361, Val. Acc.: 0.2897

Epoch 5
Validation loss decreased. Saved checkpoint for step 6: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-5
Time:  85.41s, ---- Loss: 1.1207, Acc.: 0.5936, Val. Loss: 2.4324, Val. Acc.: 0.2973

Epoch 6
Validation loss decreased. Saved checkpoint for step 7: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-6
Time:  85.82s, ---- Loss: 1.0986, Acc.: 0.6015, Val. Loss: 2.4206, Val. Acc.: 0.3174

Epoch 7
Loss did not decrease. Count = 1
Time:  85.41s, ---- Loss: 1.0858, Acc.: 0.6120, Val. Loss: 2.4357, Val. Acc.: 0.3316

Epoch 8
Validation loss decreased. Saved checkpoint for step 9: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-7
Time:  86.07s, ---- Loss: 1.0445, Acc.: 0.6202, Val. Loss: 2.4166, Val. Acc.: 0.3449

Epoch 9
Loss did not decrease. Count = 1
Time:  85.51s, ---- Loss: 1.0584, Acc.: 0.6212, Val. Loss: 2.4539, Val. Acc.: 0.3544

Epoch 10
Loss did not decrease. Count = 2
Time:  81.65s, ---- Loss: 1.0330, Acc.: 0.6310, Val. Loss: 2.4367, Val. Acc.: 0.3691

Epoch 11
Validation loss decreased. Saved checkpoint for step 12: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-8
Time:  80.85s, ---- Loss: 1.0436, Acc.: 0.6313, Val. Loss: 2.3614, Val. Acc.: 0.4162

Epoch 12
Validation loss decreased. Saved checkpoint for step 13: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-9
Time:  80.63s, ---- Loss: 1.0279, Acc.: 0.6362, Val. Loss: 2.3288, Val. Acc.: 0.4226

Epoch 13
Validation loss decreased. Saved checkpoint for step 14: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-10
Time:  80.49s, ---- Loss: 1.0094, Acc.: 0.6428, Val. Loss: 2.2927, Val. Acc.: 0.4238

Epoch 14
Validation loss decreased. Saved checkpoint for step 15: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-11
Time:  80.37s, ---- Loss: 0.9986, Acc.: 0.6436, Val. Loss: 2.2927, Val. Acc.: 0.4498

Epoch 15
Validation loss decreased. Saved checkpoint for step 16: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-12
Time:  80.13s, ---- Loss: 0.9916, Acc.: 0.6464, Val. Loss: 2.2593, Val. Acc.: 0.4681

Epoch 16
Validation loss decreased. Saved checkpoint for step 17: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-13
Time:  79.75s, ---- Loss: 0.9766, Acc.: 0.6473, Val. Loss: 2.2296, Val. Acc.: 0.4750

Epoch 17
Validation loss decreased. Saved checkpoint for step 18: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-14
Time:  79.48s, ---- Loss: 0.9639, Acc.: 0.6511, Val. Loss: 2.1947, Val. Acc.: 0.4923

Epoch 18
Validation loss decreased. Saved checkpoint for step 19: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-15
Time:  79.70s, ---- Loss: 0.9801, Acc.: 0.6547, Val. Loss: 2.1533, Val. Acc.: 0.5031

Epoch 19
Validation loss decreased. Saved checkpoint for step 20: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-16
Time:  79.30s, ---- Loss: 0.9637, Acc.: 0.6588, Val. Loss: 2.1184, Val. Acc.: 0.5257

Epoch 20
Validation loss decreased. Saved checkpoint for step 21: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-17
Time:  80.03s, ---- Loss: 0.9634, Acc.: 0.6619, Val. Loss: 2.0803, Val. Acc.: 0.5297

Epoch 21
Validation loss decreased. Saved checkpoint for step 22: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-18
Time:  79.43s, ---- Loss: 0.9621, Acc.: 0.6640, Val. Loss: 2.0505, Val. Acc.: 0.5441

Epoch 22
Validation loss decreased. Saved checkpoint for step 23: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-19
Time:  80.12s, ---- Loss: 0.9446, Acc.: 0.6642, Val. Loss: 1.9938, Val. Acc.: 0.5612

Epoch 23
Validation loss decreased. Saved checkpoint for step 24: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-20
Time:  79.52s, ---- Loss: 0.9509, Acc.: 0.6659, Val. Loss: 1.9662, Val. Acc.: 0.5746

Epoch 24
Validation loss decreased. Saved checkpoint for step 25: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-21
Time:  80.11s, ---- Loss: 0.9340, Acc.: 0.6698, Val. Loss: 1.9303, Val. Acc.: 0.5849

Epoch 25
Validation loss decreased. Saved checkpoint for step 26: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-22
Time:  79.75s, ---- Loss: 0.9323, Acc.: 0.6714, Val. Loss: 1.8947, Val. Acc.: 0.5884

Epoch 26
Validation loss decreased. Saved checkpoint for step 27: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-23
Time:  80.18s, ---- Loss: 0.9186, Acc.: 0.6713, Val. Loss: 1.8492, Val. Acc.: 0.6154

Epoch 27
Validation loss decreased. Saved checkpoint for step 28: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-24
Time:  79.95s, ---- Loss: 0.9405, Acc.: 0.6751, Val. Loss: 1.8240, Val. Acc.: 0.6249

Epoch 28
Validation loss decreased. Saved checkpoint for step 29: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-25
Time:  80.82s, ---- Loss: 0.9303, Acc.: 0.6743, Val. Loss: 1.7878, Val. Acc.: 0.6359

Epoch 29
Validation loss decreased. Saved checkpoint for step 30: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-26
Time:  81.78s, ---- Loss: 0.9191, Acc.: 0.6735, Val. Loss: 1.7602, Val. Acc.: 0.6387

Epoch 30
Validation loss decreased. Saved checkpoint for step 31: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-27
Time:  81.57s, ---- Loss: 0.9228, Acc.: 0.6777, Val. Loss: 1.7306, Val. Acc.: 0.6560

Epoch 31
Validation loss decreased. Saved checkpoint for step 32: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-28
Time:  81.23s, ---- Loss: 0.9320, Acc.: 0.6785, Val. Loss: 1.7203, Val. Acc.: 0.6587

Epoch 32
Validation loss decreased. Saved checkpoint for step 33: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-29
Time:  81.20s, ---- Loss: 0.8949, Acc.: 0.6821, Val. Loss: 1.7191, Val. Acc.: 0.6586

Epoch 33
Validation loss decreased. Saved checkpoint for step 34: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-30
Time:  80.96s, ---- Loss: 0.8951, Acc.: 0.6836, Val. Loss: 1.7082, Val. Acc.: 0.6667

Epoch 34
Validation loss decreased. Saved checkpoint for step 35: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-31
Time:  81.34s, ---- Loss: 0.8997, Acc.: 0.6826, Val. Loss: 1.6915, Val. Acc.: 0.6763

Epoch 35
Validation loss decreased. Saved checkpoint for step 36: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-32
Time:  81.32s, ---- Loss: 0.9184, Acc.: 0.6845, Val. Loss: 1.6885, Val. Acc.: 0.6670

Epoch 36
Validation loss decreased. Saved checkpoint for step 37: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-33
Time:  81.29s, ---- Loss: 0.9215, Acc.: 0.6862, Val. Loss: 1.6684, Val. Acc.: 0.6807

Epoch 37
Validation loss decreased. Saved checkpoint for step 38: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-34
Time:  81.62s, ---- Loss: 0.8943, Acc.: 0.6872, Val. Loss: 1.6684, Val. Acc.: 0.6789

Epoch 38
Loss did not decrease. Count = 1
Time:  81.59s, ---- Loss: 0.8881, Acc.: 0.6857, Val. Loss: 1.6695, Val. Acc.: 0.6800

Epoch 39
Validation loss decreased. Saved checkpoint for step 40: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-35
Time:  81.21s, ---- Loss: 0.8833, Acc.: 0.6917, Val. Loss: 1.6437, Val. Acc.: 0.6889

Epoch 40
Loss did not decrease. Count = 1
Time:  80.77s, ---- Loss: 0.9022, Acc.: 0.6858, Val. Loss: 1.6585, Val. Acc.: 0.6840

Epoch 41
Loss did not decrease. Count = 2
Time:  80.59s, ---- Loss: 0.8854, Acc.: 0.6900, Val. Loss: 1.6686, Val. Acc.: 0.6782

Epoch 42
Loss did not decrease. Count = 3
Time:  80.67s, ---- Loss: 0.9068, Acc.: 0.6882, Val. Loss: 1.6777, Val. Acc.: 0.6756

Epoch 43
Loss did not decrease. Count = 4
Time:  80.70s, ---- Loss: 0.8922, Acc.: 0.6900, Val. Loss: 1.6527, Val. Acc.: 0.6850

Epoch 44
Loss did not decrease. Count = 5
Time:  80.60s, ---- Loss: 0.8851, Acc.: 0.6920, Val. Loss: 1.6600, Val. Acc.: 0.6837

Epoch 45
Loss did not decrease. Count = 6
Time:  80.81s, ---- Loss: 0.8876, Acc.: 0.6901, Val. Loss: 1.6653, Val. Acc.: 0.6817

Epoch 46
Loss did not decrease. Count = 7
Time:  81.07s, ---- Loss: 0.8812, Acc.: 0.6917, Val. Loss: 1.6513, Val. Acc.: 0.6894

Epoch 47
Loss did not decrease. Count = 8
Time:  81.01s, ---- Loss: 0.8946, Acc.: 0.6935, Val. Loss: 1.6633, Val. Acc.: 0.6844

Epoch 48
Loss did not decrease. Count = 9
Time:  81.68s, ---- Loss: 0.8734, Acc.: 0.6906, Val. Loss: 1.6588, Val. Acc.: 0.6901

Epoch 49
Loss did not decrease. Count = 10
Time:  81.80s, ---- Loss: 0.8796, Acc.: 0.6913, Val. Loss: 1.6605, Val. Acc.: 0.6838

Saving at models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/hist.png
Done in 4110.96s
