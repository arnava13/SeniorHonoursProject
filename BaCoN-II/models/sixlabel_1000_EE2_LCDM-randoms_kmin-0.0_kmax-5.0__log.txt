err: 2024-03-03 19:44:26.729758: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
err: 2024-03-03 19:44:26.729819: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
err: 2024-03-03 19:44:26.731166: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
err: 2024-03-03 19:44:27.878027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
err: /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
err: loc = add_variable_fn(
err: 2024-03-03 19:44:30.716985: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
err: /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
err: untransformed_scale = add_variable_fn(
err: WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
err: I0000 00:00:1709495091.734019   10742 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
out: Directory models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_ not created
out: Logger creating log file: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0__log.txt
out: -------- Parameters:
out: bayesian True
out: test_mode False
out: n_test_idx 2
out: seed 1312
out: fine_tune False
out: one_vs_all False
out: c_0 ['lcdm']
out: c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']
out: dataset_balanced False
out: include_last False
out: log_path models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0__log.txt
out: restore False
out: fname sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_
out: model_name custom
out: my_path None
out: DIR data/ds_ee2_train_1k_lcdm_rands
out: TEST_DIR data/test_data/
out: models_dir models/
out: save_ckpt True
out: out_path_overwrite False
out: curves_folder data/curve_files_sys/theory_error
out: save_processed_spectra False
out: im_depth 500
out: im_width 1
out: im_channels 4
out: swap_axes True
out: sort_labels True
out: norm_data_name /planck_ee2.txt
out: normalization stdcosmo
out: sample_pace 4
out: k_max 5.0
out: k_min 0.0
out: i_max None
out: i_min None
out: add_noise True
out: n_noisy_samples 10
out: add_shot False
out: add_sys True
out: add_cosvar True
out: sigma_sys None
out: sys_scaled None
out: sys_factor None
out: sys_max None
out: sigma_curves 0.05
out: sigma_curves_default 0.05
out: rescale_curves uniform
out: z_bins [0, 1, 2, 3]
out: n_dense 1
out: filters [8, 16, 32]
out: kernel_sizes [10, 5, 2]
out: strides [2, 2, 1]
out: pool_sizes [2, 2, 0]
out: strides_pooling [2, 1, 0]
out: add_FT_dense False
out: trainable False
out: unfreeze False
out: lr 0.01
out: drop 0.5
out: n_epochs 50
out: val_size 0.15
out: test_size 0.0
out: batch_size 3000
out: patience 50
out: GPU True
out: decay 0.95
out: BatchNorm True
out: ------------ CREATING DATA GENERATORS ------------
out: labels : ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
out: Labels encoding:
out: {'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
out: n_labels : 6
out: dgp - 1000 training examples
out: ds - 1000 training examples
out: fr - 1000 training examples
out: lcdm - 1000 training examples
out: rand - 1000 training examples
out: wcdm - 1000 training examples
out: N. of data files: 1000
out: get_all_indexes labels dict: {'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
out: create_generators n_labels: 6
out: create_generators n_labels_eff: 6
out: create_generators len_c1: 1
out: Check for no duplicates in test: (0=ok):
out: 0.0
out: Check for no duplicates in val: (0=ok):
out: 0
out: N of files in training set: 850
out: N of files in validation set: 150
out: N of files in test set: 0
out: Check - total: 1000
out: --create_generators, train indexes
out: batch_size: 3000
out: - Cut sample
out: bs: 3000
out: N_labels: 6
out: N_noise: 10
out: len_c1: 1
out: Train index length: 850
out: --create_generators, validation indexes
out: - Cut sample
out: bs: 3000
out: N_labels: 6
out: N_noise: 10
out: len_c1: 1
out: Val index length: 150
out: len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 850, 3000, 6, 10
out: --DataGenerator Train
out: Data Generator Initialization
out: Using z bins [0, 1, 2, 3]
out: Normalisation file is /planck_ee2.txt
out: Specified k_max is 5.0
out: Corresponding i_max is 112
out: Closest k to k_max is 4.936132
out: Specified k_min is 0.0
out: Corresponding i_min is 0
out: Closest k to k_min is 0.01
out: New data dim: (112, 1)
out: Final i_max used is 112
out: Final i_min used is 0
out: one_vs_all: False
out: dataset_balanced: False
out: base_case_dataset: True
out: N. classes: 6
out: N. n_classes in output: 6
out: LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
out: list_IDs length: 850
out: n_indexes (n of file IDs read for each batch): 50
out: batch size: 3000
out: n_batches : 17
out: For each batch we read 50 file IDs
out: For each file ID we have 6 labels
out: For each ID, label we have 10 realizations of noise
out: In total, for each batch we have 3000 training examples
out: Input batch size: 3000
out: N of batches to cover all file IDs: 17
out: --DataGenerator Validation
out: Data Generator Initialization
out: Using z bins [0, 1, 2, 3]
out: Normalisation file is /planck_ee2.txt
out: Specified k_max is 5.0
out: Corresponding i_max is 112
out: Closest k to k_max is 4.936132
out: Specified k_min is 0.0
out: Corresponding i_min is 0
out: Closest k to k_min is 0.01
out: New data dim: (112, 1)
out: Final i_max used is 112
out: Final i_min used is 0
out: one_vs_all: False
out: dataset_balanced: False
out: base_case_dataset: True
out: N. classes: 6
out: N. n_classes in output: 6
out: LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
out: list_IDs length: 150
out: n_indexes (n of file IDs read for each batch): 50
out: batch size: 3000
out: n_batches : 3
out: For each batch we read 50 file IDs
out: For each file ID we have 6 labels
out: For each ID, label we have 10 realizations of noise
out: In total, for each batch we have 3000 training examples
out: Input batch size: 3000
out: N of batches to cover all file IDs: 3
out: ------------ DONE ------------
out: ------------ BUILDING MODEL ------------
out: Input shape (112, 4)
out: using 1D layers and 4 channels
out: Expected output dimension of layer conv1d_flipout: 52.0
out: Expected output dimension of layer max_pooling1d: 26.0
out: Expected output dimension of layer conv1d_flipout_1: 11.5
out: Expected output dimension of layer max_pooling1d_1: 10.5
out: Expected output dimension of layer conv1d_flipout_2: 9.5
out: Model: "model"
out: _________________________________________________________________
out: Layer (type)                Output Shape              Param #
out: =================================================================
out: input_1 (InputLayer)        [(None, 112, 4)]          0
out: conv1d_flipout (Conv1DFlip  (None, 52, 8)             648
out: out)
out: max_pooling1d (MaxPooling1  (None, 26, 8)             0
out: D)
out: batch_normalization (Batch  (None, 26, 8)             32
out: Normalization)
out: conv1d_flipout_1 (Conv1DFl  (None, 11, 16)            1296
out: ipout)
out: max_pooling1d_1 (MaxPoolin  (None, 10, 16)            0
out: g1D)
out: batch_normalization_1 (Bat  (None, 10, 16)            64
out: chNormalization)
out: conv1d_flipout_2 (Conv1DFl  (None, 9, 32)             2080
out: ipout)
out: batch_normalization_2 (Bat  (None, 9, 32)             128
out: chNormalization)
out: global_average_pooling1d (  (None, 32)                0
out: GlobalAveragePooling1D)
out: dense_flipout (DenseFlipou  (None, 32)                2080
out: t)
out: batch_normalization_3 (Bat  (None, 32)                128
out: chNormalization)
out: dense_flipout_1 (DenseFlip  (None, 6)                 390
out: out)
out: =================================================================
out: Total params: 6846 (26.74 KB)
out: Trainable params: 6670 (26.05 KB)
out: Non-trainable params: 176 (704.00 Byte)
out: _________________________________________________________________
out: None
out: Found GPU at: /device:GPU:0
out: ------------ TRAINING ------------
out: Features shape: (3000, 112, 4)
out: Labels shape: (3000, 6)
out: Initializing checkpoint from scratch.
out: Epoch 0
out: Validation loss decreased. Saved checkpoint for step 1: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-1
out: Time:  99.51s, ---- Loss: 1.6239, Acc.: 0.3016, Val. Loss: 2.6752, Val. Acc.: 0.1900
out: Epoch 1
out: Loss did not decrease. Count = 1
out: Time:  86.80s, ---- Loss: 1.3846, Acc.: 0.4583, Val. Loss: 2.6787, Val. Acc.: 0.1733
out: Epoch 2
out: Validation loss decreased. Saved checkpoint for step 3: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-2
out: Time:  86.46s, ---- Loss: 1.2525, Acc.: 0.5262, Val. Loss: 2.6252, Val. Acc.: 0.1843
out: Epoch 3
out: Validation loss decreased. Saved checkpoint for step 4: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-3
out: Time:  86.29s, ---- Loss: 1.2022, Acc.: 0.5584, Val. Loss: 2.5233, Val. Acc.: 0.2340
out: Epoch 4
out: Validation loss decreased. Saved checkpoint for step 5: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-4
out: Time:  85.87s, ---- Loss: 1.1827, Acc.: 0.5786, Val. Loss: 2.4361, Val. Acc.: 0.2897
out: Epoch 5
out: Validation loss decreased. Saved checkpoint for step 6: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-5
out: Time:  85.41s, ---- Loss: 1.1207, Acc.: 0.5936, Val. Loss: 2.4324, Val. Acc.: 0.2973
out: Epoch 6
out: Validation loss decreased. Saved checkpoint for step 7: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-6
out: Time:  85.82s, ---- Loss: 1.0986, Acc.: 0.6015, Val. Loss: 2.4206, Val. Acc.: 0.3174
out: Epoch 7
out: Loss did not decrease. Count = 1
out: Time:  85.41s, ---- Loss: 1.0858, Acc.: 0.6120, Val. Loss: 2.4357, Val. Acc.: 0.3316
out: Epoch 8
out: Validation loss decreased. Saved checkpoint for step 9: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-7
out: Time:  86.07s, ---- Loss: 1.0445, Acc.: 0.6202, Val. Loss: 2.4166, Val. Acc.: 0.3449
out: Epoch 9
out: Loss did not decrease. Count = 1
out: Time:  85.51s, ---- Loss: 1.0584, Acc.: 0.6212, Val. Loss: 2.4539, Val. Acc.: 0.3544
out: Epoch 10
out: Loss did not decrease. Count = 2
out: Time:  81.65s, ---- Loss: 1.0330, Acc.: 0.6310, Val. Loss: 2.4367, Val. Acc.: 0.3691
out: Epoch 11
out: Validation loss decreased. Saved checkpoint for step 12: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-8
out: Time:  80.85s, ---- Loss: 1.0436, Acc.: 0.6313, Val. Loss: 2.3614, Val. Acc.: 0.4162
out: Epoch 12
out: Validation loss decreased. Saved checkpoint for step 13: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-9
out: Time:  80.63s, ---- Loss: 1.0279, Acc.: 0.6362, Val. Loss: 2.3288, Val. Acc.: 0.4226
out: Epoch 13
out: Validation loss decreased. Saved checkpoint for step 14: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-10
out: Time:  80.49s, ---- Loss: 1.0094, Acc.: 0.6428, Val. Loss: 2.2927, Val. Acc.: 0.4238
out: Epoch 14
out: Validation loss decreased. Saved checkpoint for step 15: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-11
out: Time:  80.37s, ---- Loss: 0.9986, Acc.: 0.6436, Val. Loss: 2.2927, Val. Acc.: 0.4498
out: Epoch 15
out: Validation loss decreased. Saved checkpoint for step 16: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-12
out: Time:  80.13s, ---- Loss: 0.9916, Acc.: 0.6464, Val. Loss: 2.2593, Val. Acc.: 0.4681
out: Epoch 16
out: Validation loss decreased. Saved checkpoint for step 17: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-13
out: Time:  79.75s, ---- Loss: 0.9766, Acc.: 0.6473, Val. Loss: 2.2296, Val. Acc.: 0.4750
out: Epoch 17
out: Validation loss decreased. Saved checkpoint for step 18: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-14
out: Time:  79.48s, ---- Loss: 0.9639, Acc.: 0.6511, Val. Loss: 2.1947, Val. Acc.: 0.4923
out: Epoch 18
out: Validation loss decreased. Saved checkpoint for step 19: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-15
out: Time:  79.70s, ---- Loss: 0.9801, Acc.: 0.6547, Val. Loss: 2.1533, Val. Acc.: 0.5031
out: Epoch 19
out: Validation loss decreased. Saved checkpoint for step 20: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-16
out: Time:  79.30s, ---- Loss: 0.9637, Acc.: 0.6588, Val. Loss: 2.1184, Val. Acc.: 0.5257
out: Epoch 20
out: Validation loss decreased. Saved checkpoint for step 21: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-17
out: Time:  80.03s, ---- Loss: 0.9634, Acc.: 0.6619, Val. Loss: 2.0803, Val. Acc.: 0.5297
out: Epoch 21
out: Validation loss decreased. Saved checkpoint for step 22: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-18
out: Time:  79.43s, ---- Loss: 0.9621, Acc.: 0.6640, Val. Loss: 2.0505, Val. Acc.: 0.5441
out: Epoch 22
out: Validation loss decreased. Saved checkpoint for step 23: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-19
out: Time:  80.12s, ---- Loss: 0.9446, Acc.: 0.6642, Val. Loss: 1.9938, Val. Acc.: 0.5612
out: Epoch 23
out: Validation loss decreased. Saved checkpoint for step 24: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-20
out: Time:  79.52s, ---- Loss: 0.9509, Acc.: 0.6659, Val. Loss: 1.9662, Val. Acc.: 0.5746
out: Epoch 24
out: Validation loss decreased. Saved checkpoint for step 25: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-21
out: Time:  80.11s, ---- Loss: 0.9340, Acc.: 0.6698, Val. Loss: 1.9303, Val. Acc.: 0.5849
out: Epoch 25
out: Validation loss decreased. Saved checkpoint for step 26: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-22
out: Time:  79.75s, ---- Loss: 0.9323, Acc.: 0.6714, Val. Loss: 1.8947, Val. Acc.: 0.5884
out: Epoch 26
out: Validation loss decreased. Saved checkpoint for step 27: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-23
out: Time:  80.18s, ---- Loss: 0.9186, Acc.: 0.6713, Val. Loss: 1.8492, Val. Acc.: 0.6154
out: Epoch 27
out: Validation loss decreased. Saved checkpoint for step 28: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-24
out: Time:  79.95s, ---- Loss: 0.9405, Acc.: 0.6751, Val. Loss: 1.8240, Val. Acc.: 0.6249
out: Epoch 28
out: Validation loss decreased. Saved checkpoint for step 29: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-25
out: Time:  80.82s, ---- Loss: 0.9303, Acc.: 0.6743, Val. Loss: 1.7878, Val. Acc.: 0.6359
out: Epoch 29
out: Validation loss decreased. Saved checkpoint for step 30: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-26
out: Time:  81.78s, ---- Loss: 0.9191, Acc.: 0.6735, Val. Loss: 1.7602, Val. Acc.: 0.6387
out: Epoch 30
out: Validation loss decreased. Saved checkpoint for step 31: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-27
out: Time:  81.57s, ---- Loss: 0.9228, Acc.: 0.6777, Val. Loss: 1.7306, Val. Acc.: 0.6560
out: Epoch 31
out: Validation loss decreased. Saved checkpoint for step 32: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-28
out: Time:  81.23s, ---- Loss: 0.9320, Acc.: 0.6785, Val. Loss: 1.7203, Val. Acc.: 0.6587
out: Epoch 32
out: Validation loss decreased. Saved checkpoint for step 33: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-29
out: Time:  81.20s, ---- Loss: 0.8949, Acc.: 0.6821, Val. Loss: 1.7191, Val. Acc.: 0.6586
out: Epoch 33
out: Validation loss decreased. Saved checkpoint for step 34: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-30
out: Time:  80.96s, ---- Loss: 0.8951, Acc.: 0.6836, Val. Loss: 1.7082, Val. Acc.: 0.6667
out: Epoch 34
out: Validation loss decreased. Saved checkpoint for step 35: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-31
out: Time:  81.34s, ---- Loss: 0.8997, Acc.: 0.6826, Val. Loss: 1.6915, Val. Acc.: 0.6763
out: Epoch 35
out: Validation loss decreased. Saved checkpoint for step 36: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-32
out: Time:  81.32s, ---- Loss: 0.9184, Acc.: 0.6845, Val. Loss: 1.6885, Val. Acc.: 0.6670
out: Epoch 36
out: Validation loss decreased. Saved checkpoint for step 37: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-33
out: Time:  81.29s, ---- Loss: 0.9215, Acc.: 0.6862, Val. Loss: 1.6684, Val. Acc.: 0.6807
out: Epoch 37
out: Validation loss decreased. Saved checkpoint for step 38: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-34
out: Time:  81.62s, ---- Loss: 0.8943, Acc.: 0.6872, Val. Loss: 1.6684, Val. Acc.: 0.6789
out: Epoch 38
out: Loss did not decrease. Count = 1
out: Time:  81.59s, ---- Loss: 0.8881, Acc.: 0.6857, Val. Loss: 1.6695, Val. Acc.: 0.6800
out: Epoch 39
out: Validation loss decreased. Saved checkpoint for step 40: models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/tf_ckpts/ckpt-35
out: Time:  81.21s, ---- Loss: 0.8833, Acc.: 0.6917, Val. Loss: 1.6437, Val. Acc.: 0.6889
out: Epoch 40
out: Loss did not decrease. Count = 1
out: Time:  80.77s, ---- Loss: 0.9022, Acc.: 0.6858, Val. Loss: 1.6585, Val. Acc.: 0.6840
out: Epoch 41
out: Loss did not decrease. Count = 2
out: Time:  80.59s, ---- Loss: 0.8854, Acc.: 0.6900, Val. Loss: 1.6686, Val. Acc.: 0.6782
out: Epoch 42
out: Loss did not decrease. Count = 3
out: Time:  80.67s, ---- Loss: 0.9068, Acc.: 0.6882, Val. Loss: 1.6777, Val. Acc.: 0.6756
out: Epoch 43
out: Loss did not decrease. Count = 4
out: Time:  80.70s, ---- Loss: 0.8922, Acc.: 0.6900, Val. Loss: 1.6527, Val. Acc.: 0.6850
out: Epoch 44
out: Loss did not decrease. Count = 5
out: Time:  80.60s, ---- Loss: 0.8851, Acc.: 0.6920, Val. Loss: 1.6600, Val. Acc.: 0.6837
out: Epoch 45
out: Loss did not decrease. Count = 6
out: Time:  80.81s, ---- Loss: 0.8876, Acc.: 0.6901, Val. Loss: 1.6653, Val. Acc.: 0.6817
out: Epoch 46
out: Loss did not decrease. Count = 7
out: Time:  81.07s, ---- Loss: 0.8812, Acc.: 0.6917, Val. Loss: 1.6513, Val. Acc.: 0.6894
out: Epoch 47
out: Loss did not decrease. Count = 8
out: Time:  81.01s, ---- Loss: 0.8946, Acc.: 0.6935, Val. Loss: 1.6633, Val. Acc.: 0.6844
out: Epoch 48
out: Loss did not decrease. Count = 9
out: Time:  81.68s, ---- Loss: 0.8734, Acc.: 0.6906, Val. Loss: 1.6588, Val. Acc.: 0.6901
out: Epoch 49
out: Loss did not decrease. Count = 10
out: Time:  81.80s, ---- Loss: 0.8796, Acc.: 0.6913, Val. Loss: 1.6605, Val. Acc.: 0.6838
out: Saving at models/sixlabel_1000_EE2_LCDM-randoms_kmin-0.0_kmax-5.0_/hist.png
out: Done in 4110.96s
