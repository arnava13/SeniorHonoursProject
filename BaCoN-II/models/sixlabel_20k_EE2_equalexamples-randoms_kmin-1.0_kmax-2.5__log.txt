2024-04-04 12:59:46.360337: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-04 12:59:46.360398: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-04 12:59:46.361554: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-04 12:59:47.412226: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-04-04 12:59:49.872729: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
using 1D layers and 4 channels
/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
loc = add_variable_fn(
/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
untransformed_scale = add_variable_fn(
Expected output dimension after layer: conv1d_flipout : 14
Expected output dimension after layer: conv1d_flipout_1 : 4
Expected output dimension after layer: conv1d_flipout_2 : 3
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1712239600.941364    3433 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
W0000 00:00:1712239614.731541    3433 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update
W0000 00:00:1712239630.800809    3436 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update
Creating directory models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_
-------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune False
one_vs_all False
c_0 ['lcdm']
c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']
dataset_balanced False
include_last False
log_path models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5__log.txt
restore False
fname sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_
model_name custom
my_path None
DIR data/train
TEST_DIR data/test
models_dir models/
save_ckpt True
out_path_overwrite False
curves_folder data/curve_files_sys/theory_error
save_processed_spectra False
cache_dir False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
norm_data_name /planck_ee2.txt
normalization stdcosmo
sample_pace 1
k_max 2.5
k_min 1.0
i_max None
i_min None
add_noise True
n_noisy_samples 10
add_shot False
add_sys True
add_cosvar True
sigma_sys None
sys_scaled None
sys_factor None
sys_max None
sigma_curves 0.05
sigma_curves_default 0.05
rescale_curves uniform
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable False
unfreeze False
lr 0.01
drop 0.5
n_epochs 100
val_size 0.15
test_size 0.0
batch_size 12000
patience 20
GPU True
TPU False
decay 0.95
BatchNorm True
padding valid
shuffle True
------------ CREATING DATA GENERATORS ------------
labels : ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
Labels encoding:
{'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
n_labels : 6
dgp - 20000 training examples
ds - 20000 training examples
fr - 20000 training examples
lcdm - 20000 training examples
rand - 20000 training examples
wcdm - 20000 training examples
N. of data files: 20000
get_all_indexes labels dict: {'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
create_generators n_labels: 6
create_generators n_labels_eff: 6
create_generators len_c1: 1
Check for no duplicates in test: (0=ok):
0.0
Check for no duplicates in val: (0=ok):
0
N of indexes in training set: 17000
N of indexes in validation set: 3000
N of indexes in test set: 0
Check - total per class: 20000
--create_generators, train indexes
batch_size: 12000
- Cut sample
bs: 12000
N_labels: 6
N_noise: 10
len_c1: 1
Train index length: 17000
--create_generators, validation indexes
- Cut sample
bs: 12000
N_labels: 6
N_noise: 10
len_c1: 1
Val index length: 3000
len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 17000, 12000, 6, 10
--DataSet Train
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 399
Closest k to k_max is 2.504942
Specified k_min is 1.0
Corresponding i_min is 333
Closest k to k_min is 1.004625
New data dim: (66, 1)
Final i_max used is 399
Final i_min used is 333
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 17000
n_indexes (n of file IDs read for each batch): 200
batch size: 12000
n_batches : 85
For each batch we read 200 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 12000 training examples
Input batch size: 12000
N of batches to cover all file IDs: 85
len(fname_list), batch_size, n_noisy_samples, n_batches: 102000, 12000, 10, 85
--DataSet Validation
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 399
Closest k to k_max is 2.504942
Specified k_min is 1.0
Corresponding i_min is 333
Closest k to k_min is 1.004625
New data dim: (66, 1)
Final i_max used is 399
Final i_min used is 333
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 3000
n_indexes (n of file IDs read for each batch): 200
batch size: 12000
n_batches : 15
For each batch we read 200 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 12000 training examples
Input batch size: 12000
N of batches to cover all file IDs: 15
len(fname_list), batch_size, n_noisy_samples, n_batches: 18000, 12000, 10, 15
------------ DONE ------------
------------ BUILDING MODEL ------------
Input shape (66, 4)
Model: "model"
_________________________________________________________________
Layer (type)                Output Shape              Param #
=================================================================
input_1 (InputLayer)        [(None, 66, 4)]           0
conv1d_flipout (Conv1DFlip  (None, 29, 8)             648
out)
max_pooling1d (MaxPooling1  (None, 14, 8)             0
D)
batch_normalization (Batch  (None, 14, 8)             32
Normalization)
conv1d_flipout_1 (Conv1DFl  (None, 5, 16)             1296
ipout)
max_pooling1d_1 (MaxPoolin  (None, 4, 16)             0
g1D)
batch_normalization_1 (Bat  (None, 4, 16)             64
chNormalization)
conv1d_flipout_2 (Conv1DFl  (None, 3, 32)             2080
ipout)
batch_normalization_2 (Bat  (None, 3, 32)             128
chNormalization)
global_average_pooling1d (  (None, 32)                0
GlobalAveragePooling1D)
dense_flipout (DenseFlipou  (None, 32)                2080
t)
batch_normalization_3 (Bat  (None, 32)                128
chNormalization)
dense_flipout_1 (DenseFlip  (None, 6)                 390
out)
=================================================================
Total params: 6846 (26.74 KB)
Trainable params: 6670 (26.05 KB)
Non-trainable params: 176 (704.00 Byte)
_________________________________________________________________
None
Found GPU at: /device:GPU:0
------------ TRAINING ------------
Features shape: (12000, 66, 4)
Labels shape: (12000, 6)
Initializing checkpoint from scratch.
Epoch 0
Validation loss decreased. Saved checkpoint for step 1: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-1
Time:  38.10s, ---- Loss: 1.4150, Acc.: 0.3457, Val. Loss: 1.8012, Val. Acc.: 0.1755
Epoch 1
Validation loss decreased. Saved checkpoint for step 2: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-2
Time:  1.38s, ---- Loss: 1.3264, Acc.: 0.4646, Val. Loss: 1.6375, Val. Acc.: 0.3533
Epoch 2
Loss did not decrease. Count = 1
Time:  1.23s, ---- Loss: 1.2070, Acc.: 0.4992, Val. Loss: 1.6804, Val. Acc.: 0.3461
Epoch 3
Loss did not decrease. Count = 2
Time:  1.21s, ---- Loss: 1.1323, Acc.: 0.5344, Val. Loss: 1.8512, Val. Acc.: 0.3285
Epoch 4
Loss did not decrease. Count = 3
Time:  1.22s, ---- Loss: 1.0902, Acc.: 0.5568, Val. Loss: 1.7594, Val. Acc.: 0.3318
Epoch 5
Loss did not decrease. Count = 4
Time:  1.21s, ---- Loss: 1.0700, Acc.: 0.5705, Val. Loss: 1.6492, Val. Acc.: 0.3897
Epoch 6
Validation loss decreased. Saved checkpoint for step 7: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-3
Time:  1.34s, ---- Loss: 1.0392, Acc.: 0.5800, Val. Loss: 1.3879, Val. Acc.: 0.4717
Epoch 7
Validation loss decreased. Saved checkpoint for step 8: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-4
Time:  1.33s, ---- Loss: 1.0140, Acc.: 0.5876, Val. Loss: 1.2836, Val. Acc.: 0.5078
Epoch 8
Validation loss decreased. Saved checkpoint for step 9: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-5
Time:  1.32s, ---- Loss: 1.0157, Acc.: 0.5923, Val. Loss: 1.2188, Val. Acc.: 0.5333
Epoch 9
Validation loss decreased. Saved checkpoint for step 10: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-6
Time:  1.33s, ---- Loss: 0.9994, Acc.: 0.5966, Val. Loss: 1.1683, Val. Acc.: 0.5562
Epoch 10
Validation loss decreased. Saved checkpoint for step 11: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-7
Time:  1.34s, ---- Loss: 0.9907, Acc.: 0.6004, Val. Loss: 1.1354, Val. Acc.: 0.5706
Epoch 11
Validation loss decreased. Saved checkpoint for step 12: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-8
Time:  1.35s, ---- Loss: 0.9833, Acc.: 0.6034, Val. Loss: 1.1135, Val. Acc.: 0.5774
Epoch 12
Validation loss decreased. Saved checkpoint for step 13: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-9
Time:  1.31s, ---- Loss: 0.9832, Acc.: 0.6057, Val. Loss: 1.0902, Val. Acc.: 0.5882
Epoch 13
Validation loss decreased. Saved checkpoint for step 14: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-10
Time:  1.32s, ---- Loss: 0.9739, Acc.: 0.6077, Val. Loss: 1.0779, Val. Acc.: 0.5935
Epoch 14
Validation loss decreased. Saved checkpoint for step 15: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-11
Time:  1.32s, ---- Loss: 0.9729, Acc.: 0.6094, Val. Loss: 1.0612, Val. Acc.: 0.6011
Epoch 15
Validation loss decreased. Saved checkpoint for step 16: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-12
Time:  1.32s, ---- Loss: 0.9726, Acc.: 0.6114, Val. Loss: 1.0590, Val. Acc.: 0.6018
Epoch 16
Validation loss decreased. Saved checkpoint for step 17: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-13
Time:  1.32s, ---- Loss: 0.9673, Acc.: 0.6122, Val. Loss: 1.0503, Val. Acc.: 0.6055
Epoch 17
Validation loss decreased. Saved checkpoint for step 18: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-14
Time:  1.34s, ---- Loss: 0.9630, Acc.: 0.6134, Val. Loss: 1.0431, Val. Acc.: 0.6096
Epoch 18
Loss did not decrease. Count = 1
Time:  1.21s, ---- Loss: 0.9605, Acc.: 0.6147, Val. Loss: 1.0456, Val. Acc.: 0.6087
Epoch 19
Validation loss decreased. Saved checkpoint for step 20: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-15
Time:  1.34s, ---- Loss: 0.9647, Acc.: 0.6153, Val. Loss: 1.0411, Val. Acc.: 0.6102
Epoch 20
Validation loss decreased. Saved checkpoint for step 21: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-16
Time:  1.34s, ---- Loss: 0.9611, Acc.: 0.6164, Val. Loss: 1.0361, Val. Acc.: 0.6114
Epoch 21
Validation loss decreased. Saved checkpoint for step 22: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-17
Time:  1.35s, ---- Loss: 0.9558, Acc.: 0.6169, Val. Loss: 1.0358, Val. Acc.: 0.6121
Epoch 22
Loss did not decrease. Count = 1
Time:  1.21s, ---- Loss: 0.9518, Acc.: 0.6177, Val. Loss: 1.0394, Val. Acc.: 0.6116
Epoch 23
Loss did not decrease. Count = 2
Time:  1.21s, ---- Loss: 0.9511, Acc.: 0.6184, Val. Loss: 1.0399, Val. Acc.: 0.6115
Epoch 24
Loss did not decrease. Count = 3
Time:  1.20s, ---- Loss: 0.9508, Acc.: 0.6191, Val. Loss: 1.0361, Val. Acc.: 0.6137
Epoch 25
Validation loss decreased. Saved checkpoint for step 26: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-18
Time:  1.31s, ---- Loss: 0.9523, Acc.: 0.6197, Val. Loss: 1.0278, Val. Acc.: 0.6176
Epoch 26
Validation loss decreased. Saved checkpoint for step 27: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-19
Time:  1.31s, ---- Loss: 0.9473, Acc.: 0.6199, Val. Loss: 1.0262, Val. Acc.: 0.6175
Epoch 27
Validation loss decreased. Saved checkpoint for step 28: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-20
Time:  1.33s, ---- Loss: 0.9475, Acc.: 0.6202, Val. Loss: 1.0244, Val. Acc.: 0.6192
Epoch 28
Loss did not decrease. Count = 1
Time:  1.21s, ---- Loss: 0.9463, Acc.: 0.6209, Val. Loss: 1.0257, Val. Acc.: 0.6181
Epoch 29
Loss did not decrease. Count = 2
Time:  1.20s, ---- Loss: 0.9450, Acc.: 0.6213, Val. Loss: 1.0259, Val. Acc.: 0.6180
Epoch 30
Loss did not decrease. Count = 3
Time:  1.21s, ---- Loss: 0.9432, Acc.: 0.6217, Val. Loss: 1.0267, Val. Acc.: 0.6171
Epoch 31
Loss did not decrease. Count = 4
Time:  1.20s, ---- Loss: 0.9410, Acc.: 0.6220, Val. Loss: 1.0254, Val. Acc.: 0.6185
Epoch 32
Loss did not decrease. Count = 5
Time:  1.21s, ---- Loss: 0.9393, Acc.: 0.6225, Val. Loss: 1.0251, Val. Acc.: 0.6185
Epoch 33
Validation loss decreased. Saved checkpoint for step 34: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-21
Time:  1.34s, ---- Loss: 0.9375, Acc.: 0.6227, Val. Loss: 1.0218, Val. Acc.: 0.6207
Epoch 34
Loss did not decrease. Count = 1
Time:  1.20s, ---- Loss: 0.9368, Acc.: 0.6231, Val. Loss: 1.0231, Val. Acc.: 0.6207
Epoch 35
Validation loss decreased. Saved checkpoint for step 36: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-22
Time:  1.33s, ---- Loss: 0.9381, Acc.: 0.6233, Val. Loss: 1.0203, Val. Acc.: 0.6218
Epoch 36
Validation loss decreased. Saved checkpoint for step 37: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-23
Time:  1.33s, ---- Loss: 0.9350, Acc.: 0.6239, Val. Loss: 1.0190, Val. Acc.: 0.6220
Epoch 37
Validation loss decreased. Saved checkpoint for step 38: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-24
Time:  1.33s, ---- Loss: 0.9355, Acc.: 0.6240, Val. Loss: 1.0172, Val. Acc.: 0.6231
Epoch 38
Loss did not decrease. Count = 1
Time:  1.20s, ---- Loss: 0.9353, Acc.: 0.6241, Val. Loss: 1.0180, Val. Acc.: 0.6237
Epoch 39
Loss did not decrease. Count = 2
Time:  1.21s, ---- Loss: 0.9358, Acc.: 0.6247, Val. Loss: 1.0180, Val. Acc.: 0.6223
Epoch 40
Validation loss decreased. Saved checkpoint for step 41: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-25
Time:  1.33s, ---- Loss: 0.9338, Acc.: 0.6247, Val. Loss: 1.0159, Val. Acc.: 0.6233
Epoch 41
Validation loss decreased. Saved checkpoint for step 42: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-26
Time:  1.34s, ---- Loss: 0.9327, Acc.: 0.6250, Val. Loss: 1.0153, Val. Acc.: 0.6238
Epoch 42
Validation loss decreased. Saved checkpoint for step 43: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-27
Time:  1.32s, ---- Loss: 0.9337, Acc.: 0.6250, Val. Loss: 1.0126, Val. Acc.: 0.6252
Epoch 43
Loss did not decrease. Count = 1
Time:  1.19s, ---- Loss: 0.9339, Acc.: 0.6254, Val. Loss: 1.0128, Val. Acc.: 0.6245
Epoch 44
Validation loss decreased. Saved checkpoint for step 45: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-28
Time:  1.30s, ---- Loss: 0.9339, Acc.: 0.6253, Val. Loss: 1.0125, Val. Acc.: 0.6247
Epoch 45
Validation loss decreased. Saved checkpoint for step 46: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-29
Time:  1.31s, ---- Loss: 0.9332, Acc.: 0.6256, Val. Loss: 1.0120, Val. Acc.: 0.6247
Epoch 46
Loss did not decrease. Count = 1
Time:  1.20s, ---- Loss: 0.9324, Acc.: 0.6259, Val. Loss: 1.0123, Val. Acc.: 0.6247
Epoch 47
Validation loss decreased. Saved checkpoint for step 48: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-30
Time:  1.32s, ---- Loss: 0.9318, Acc.: 0.6262, Val. Loss: 1.0109, Val. Acc.: 0.6259
Epoch 48
Validation loss decreased. Saved checkpoint for step 49: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-31
Time:  1.31s, ---- Loss: 0.9348, Acc.: 0.6262, Val. Loss: 1.0099, Val. Acc.: 0.6258
Epoch 49
Validation loss decreased. Saved checkpoint for step 50: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-32
Time:  1.32s, ---- Loss: 0.9315, Acc.: 0.6264, Val. Loss: 1.0090, Val. Acc.: 0.6262
Epoch 50
Validation loss decreased. Saved checkpoint for step 51: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-33
Time:  1.32s, ---- Loss: 0.9323, Acc.: 0.6267, Val. Loss: 1.0086, Val. Acc.: 0.6259
Epoch 51
Loss did not decrease. Count = 1
Time:  1.21s, ---- Loss: 0.9313, Acc.: 0.6270, Val. Loss: 1.0089, Val. Acc.: 0.6262
Epoch 52
Validation loss decreased. Saved checkpoint for step 53: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-34
Time:  1.31s, ---- Loss: 0.9303, Acc.: 0.6266, Val. Loss: 1.0078, Val. Acc.: 0.6257
Epoch 53
Validation loss decreased. Saved checkpoint for step 54: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-35
Time:  1.32s, ---- Loss: 0.9312, Acc.: 0.6271, Val. Loss: 1.0077, Val. Acc.: 0.6259
Epoch 54
Validation loss decreased. Saved checkpoint for step 55: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-36
Time:  1.34s, ---- Loss: 0.9294, Acc.: 0.6270, Val. Loss: 1.0076, Val. Acc.: 0.6266
Epoch 55
Validation loss decreased. Saved checkpoint for step 56: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-37
Time:  1.33s, ---- Loss: 0.9304, Acc.: 0.6273, Val. Loss: 1.0070, Val. Acc.: 0.6262
Epoch 56
Loss did not decrease. Count = 1
Time:  1.20s, ---- Loss: 0.9291, Acc.: 0.6273, Val. Loss: 1.0076, Val. Acc.: 0.6256
Epoch 57
Loss did not decrease. Count = 2
Time:  1.21s, ---- Loss: 0.9286, Acc.: 0.6274, Val. Loss: 1.0071, Val. Acc.: 0.6265
Epoch 58
Loss did not decrease. Count = 3
Time:  1.21s, ---- Loss: 0.9285, Acc.: 0.6273, Val. Loss: 1.0073, Val. Acc.: 0.6273
Epoch 59
Validation loss decreased. Saved checkpoint for step 60: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-38
Time:  1.32s, ---- Loss: 0.9274, Acc.: 0.6277, Val. Loss: 1.0067, Val. Acc.: 0.6267
Epoch 60
Validation loss decreased. Saved checkpoint for step 61: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-39
Time:  1.32s, ---- Loss: 0.9276, Acc.: 0.6279, Val. Loss: 1.0065, Val. Acc.: 0.6271
Epoch 61
Validation loss decreased. Saved checkpoint for step 62: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-40
Time:  1.32s, ---- Loss: 0.9278, Acc.: 0.6276, Val. Loss: 1.0062, Val. Acc.: 0.6275
Epoch 62
Validation loss decreased. Saved checkpoint for step 63: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-41
Time:  1.32s, ---- Loss: 0.9254, Acc.: 0.6278, Val. Loss: 1.0055, Val. Acc.: 0.6274
Epoch 63
Loss did not decrease. Count = 1
Time:  1.20s, ---- Loss: 0.9285, Acc.: 0.6278, Val. Loss: 1.0061, Val. Acc.: 0.6269
Epoch 64
Loss did not decrease. Count = 2
Time:  1.19s, ---- Loss: 0.9269, Acc.: 0.6277, Val. Loss: 1.0056, Val. Acc.: 0.6271
Epoch 65
Loss did not decrease. Count = 3
Time:  1.19s, ---- Loss: 0.9266, Acc.: 0.6280, Val. Loss: 1.0058, Val. Acc.: 0.6271
Epoch 66
Validation loss decreased. Saved checkpoint for step 67: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-42
Time:  1.31s, ---- Loss: 0.9273, Acc.: 0.6281, Val. Loss: 1.0053, Val. Acc.: 0.6274
Epoch 67
Loss did not decrease. Count = 1
Time:  1.20s, ---- Loss: 0.9245, Acc.: 0.6284, Val. Loss: 1.0056, Val. Acc.: 0.6281
Epoch 68
Validation loss decreased. Saved checkpoint for step 69: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-43
Time:  1.33s, ---- Loss: 0.9252, Acc.: 0.6285, Val. Loss: 1.0045, Val. Acc.: 0.6279
Epoch 69
Loss did not decrease. Count = 1
Time:  1.20s, ---- Loss: 0.9281, Acc.: 0.6285, Val. Loss: 1.0045, Val. Acc.: 0.6277
Epoch 70
Loss did not decrease. Count = 2
Time:  1.21s, ---- Loss: 0.9265, Acc.: 0.6285, Val. Loss: 1.0048, Val. Acc.: 0.6271
Epoch 71
Loss did not decrease. Count = 3
Time:  1.21s, ---- Loss: 0.9248, Acc.: 0.6284, Val. Loss: 1.0048, Val. Acc.: 0.6281
Epoch 72
Validation loss decreased. Saved checkpoint for step 73: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-44
Time:  1.34s, ---- Loss: 0.9262, Acc.: 0.6283, Val. Loss: 1.0034, Val. Acc.: 0.6283
Epoch 73
Loss did not decrease. Count = 1
Time:  1.22s, ---- Loss: 0.9250, Acc.: 0.6284, Val. Loss: 1.0043, Val. Acc.: 0.6279
Epoch 74
Loss did not decrease. Count = 2
Time:  1.21s, ---- Loss: 0.9234, Acc.: 0.6287, Val. Loss: 1.0041, Val. Acc.: 0.6282
Epoch 75
Loss did not decrease. Count = 3
Time:  1.20s, ---- Loss: 0.9248, Acc.: 0.6285, Val. Loss: 1.0042, Val. Acc.: 0.6281
Epoch 76
Loss did not decrease. Count = 4
Time:  1.21s, ---- Loss: 0.9237, Acc.: 0.6289, Val. Loss: 1.0040, Val. Acc.: 0.6277
Epoch 77
Loss did not decrease. Count = 5
Time:  1.20s, ---- Loss: 0.9239, Acc.: 0.6290, Val. Loss: 1.0038, Val. Acc.: 0.6276
Epoch 78
Loss did not decrease. Count = 6
Time:  1.20s, ---- Loss: 0.9244, Acc.: 0.6289, Val. Loss: 1.0038, Val. Acc.: 0.6287
Epoch 79
Loss did not decrease. Count = 7
Time:  1.19s, ---- Loss: 0.9259, Acc.: 0.6291, Val. Loss: 1.0035, Val. Acc.: 0.6286
Epoch 80
Validation loss decreased. Saved checkpoint for step 81: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-45
Time:  1.31s, ---- Loss: 0.9262, Acc.: 0.6290, Val. Loss: 1.0030, Val. Acc.: 0.6289
Epoch 81
Loss did not decrease. Count = 1
Time:  1.19s, ---- Loss: 0.9257, Acc.: 0.6289, Val. Loss: 1.0030, Val. Acc.: 0.6286
Epoch 82
Loss did not decrease. Count = 2
Time:  1.20s, ---- Loss: 0.9227, Acc.: 0.6291, Val. Loss: 1.0032, Val. Acc.: 0.6288
Epoch 83
Loss did not decrease. Count = 3
Time:  1.19s, ---- Loss: 0.9249, Acc.: 0.6292, Val. Loss: 1.0033, Val. Acc.: 0.6285
Epoch 84
Validation loss decreased. Saved checkpoint for step 85: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-46
Time:  1.30s, ---- Loss: 0.9252, Acc.: 0.6289, Val. Loss: 1.0026, Val. Acc.: 0.6290
Epoch 85
Loss did not decrease. Count = 1
Time:  1.20s, ---- Loss: 0.9238, Acc.: 0.6291, Val. Loss: 1.0029, Val. Acc.: 0.6286
Epoch 86
Loss did not decrease. Count = 2
Time:  1.19s, ---- Loss: 0.9235, Acc.: 0.6291, Val. Loss: 1.0029, Val. Acc.: 0.6289
Epoch 87
Loss did not decrease. Count = 3
Time:  1.21s, ---- Loss: 0.9248, Acc.: 0.6289, Val. Loss: 1.0028, Val. Acc.: 0.6289
Epoch 88
Loss did not decrease. Count = 4
Time:  1.20s, ---- Loss: 0.9228, Acc.: 0.6293, Val. Loss: 1.0031, Val. Acc.: 0.6279
Epoch 89
Loss did not decrease. Count = 5
Time:  1.21s, ---- Loss: 0.9229, Acc.: 0.6289, Val. Loss: 1.0030, Val. Acc.: 0.6287
Epoch 90
Loss did not decrease. Count = 6
Time:  1.21s, ---- Loss: 0.9230, Acc.: 0.6292, Val. Loss: 1.0030, Val. Acc.: 0.6286
Epoch 91
Validation loss decreased. Saved checkpoint for step 92: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-47
Time:  1.35s, ---- Loss: 0.9238, Acc.: 0.6291, Val. Loss: 1.0026, Val. Acc.: 0.6287
Epoch 92
Validation loss decreased. Saved checkpoint for step 93: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-48
Time:  1.33s, ---- Loss: 0.9234, Acc.: 0.6289, Val. Loss: 1.0025, Val. Acc.: 0.6288
Epoch 93
Loss did not decrease. Count = 1
Time:  1.21s, ---- Loss: 0.9235, Acc.: 0.6295, Val. Loss: 1.0031, Val. Acc.: 0.6284
Epoch 94
Validation loss decreased. Saved checkpoint for step 95: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-49
Time:  1.34s, ---- Loss: 0.9243, Acc.: 0.6293, Val. Loss: 1.0021, Val. Acc.: 0.6298
Epoch 95
Loss did not decrease. Count = 1
Time:  1.21s, ---- Loss: 0.9233, Acc.: 0.6293, Val. Loss: 1.0023, Val. Acc.: 0.6290
Epoch 96
Loss did not decrease. Count = 2
Time:  1.21s, ---- Loss: 0.9247, Acc.: 0.6291, Val. Loss: 1.0022, Val. Acc.: 0.6292
Epoch 97
Loss did not decrease. Count = 3
Time:  1.19s, ---- Loss: 0.9220, Acc.: 0.6294, Val. Loss: 1.0023, Val. Acc.: 0.6292
Epoch 98
Validation loss decreased. Saved checkpoint for step 99: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/tf_ckpts/ckpt-50
Time:  1.31s, ---- Loss: 0.9238, Acc.: 0.6295, Val. Loss: 1.0021, Val. Acc.: 0.6291
Epoch 99
Loss did not decrease. Count = 1
Time:  1.20s, ---- Loss: 0.9236, Acc.: 0.6293, Val. Loss: 1.0025, Val. Acc.: 0.6288
Saving at models/sixlabel_20k_EE2_equalexamples-randoms_kmin-1.0_kmax-2.5_/hist.png
Done in 4167.86s
