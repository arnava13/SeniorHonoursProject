
 -------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune False
one_vs_all False
c_0 ['lcdm']
c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']
dataset_balanced False
include_last False
log_path models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5__log.txt
restore False
fname fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_
model_name custom
my_path None
DIR data/fivelabel_train
TEST_DIR data/test_data/
models_dir models/
save_ckpt True
out_path_overwrite False
curves_folder data/curve_files_sys/theory_error
save_processed_spectra False
cache_dir False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
norm_data_name /planck_ee2.txt
normalization stdcosmo
sample_pace 2
k_max 2.5
k_min 0.0
i_max None
i_min None
add_noise True
n_noisy_samples 10
add_shot False
add_sys True
add_cosvar True
sigma_sys None
sys_scaled None
sys_factor None
sys_max None
sigma_curves 0.05
sigma_curves_default 0.05
rescale_curves uniform
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable False
unfreeze False
lr 0.03
drop 0.5
n_epochs 100
val_size 0.15
test_size 0.0
batch_size 3000
patience 20
GPU True
TPU False
decay 0.95
BatchNorm True
padding valid
shuffle False

------------ CREATING DATA GENERATORS ------------
labels : ['dgp', 'fr', 'lcdm', 'rand', 'wcdm']
Labels encoding: 
{'dgp': 0, 'fr': 1, 'lcdm': 2, 'rand': 3, 'wcdm': 4}
n_labels : 5
dgp - 5000 training examples
fr - 5000 training examples
lcdm - 5000 training examples
rand - 5000 training examples
wcdm - 5000 training examples

N. of data files: 5000
get_all_indexes labels dict: {'dgp': 0, 'fr': 1, 'lcdm': 2, 'rand': 3, 'wcdm': 4}
create_generators n_labels: 5
create_generators n_labels_eff: 5
create_generators len_c1: 1
Check for no duplicates in test: (0=ok):
0.0
Check for no duplicates in val: (0=ok):
0
N of indexes in training set: 4250
N of indexes in validation set: 750
N of indexes in test set: 0
Check - total per class: 5000
--create_generators, train indexes
batch_size: 3000
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
Train index length: 4200
--create_generators, validation indexes
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 3000
N_labels: 5
N_noise: 10
len_c1: 1
Val index length: 720
len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 4200, 3000, 5, 10

--DataSet Train
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 199
Closest k to k_max is 2.470504
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (199, 1)
Final i_max used is 199
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 5
N. n_classes in output: 5
LABELS: ['dgp', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 4200
n_indexes (n of file IDs read for each batch): 60
batch size: 3000
n_batches : 70
For each batch we read 60 file IDs
For each file ID we have 5 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 3000 training examples
Input batch size: 3000
N of batches to cover all file IDs: 70
len(fname_list), batch_size, n_noisy_samples, n_batches: 21000, 3000, 10, 70

--DataSet Validation
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 2.5
Corresponding i_max is 199
Closest k to k_max is 2.470504
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (199, 1)
Final i_max used is 199
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 5
N. n_classes in output: 5
LABELS: ['dgp', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 720
n_indexes (n of file IDs read for each batch): 60
batch size: 3000
n_batches : 12
For each batch we read 60 file IDs
For each file ID we have 5 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 3000 training examples
Input batch size: 3000
N of batches to cover all file IDs: 12
len(fname_list), batch_size, n_noisy_samples, n_batches: 3600, 3000, 10, 12
------------ DONE ------------

------------ BUILDING MODEL ------------
Input shape (199, 4)
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 199, 4)]          0         
                                                                 
 conv1d_flipout (Conv1DFlip  (None, 95, 8)             648       
 out)                                                            
                                                                 
 max_pooling1d (MaxPooling1  (None, 47, 8)             0         
 D)                                                              
                                                                 
 batch_normalization (Batch  (None, 47, 8)             32        
 Normalization)                                                  
                                                                 
 conv1d_flipout_1 (Conv1DFl  (None, 22, 16)            1296      
 ipout)                                                          
                                                                 
 max_pooling1d_1 (MaxPoolin  (None, 21, 16)            0         
 g1D)                                                            
                                                                 
 batch_normalization_1 (Bat  (None, 21, 16)            64        
 chNormalization)                                                
                                                                 
 conv1d_flipout_2 (Conv1DFl  (None, 20, 32)            2080      
 ipout)                                                          
                                                                 
 batch_normalization_2 (Bat  (None, 20, 32)            128       
 chNormalization)                                                
                                                                 
 global_average_pooling1d (  (None, 32)                0         
 GlobalAveragePooling1D)                                         
                                                                 
 dense_flipout (DenseFlipou  (None, 32)                2080      
 t)                                                              
                                                                 
 batch_normalization_3 (Bat  (None, 32)                128       
 chNormalization)                                                
                                                                 
 dense_flipout_1 (DenseFlip  (None, 5)                 325       
 out)                                                            
                                                                 
=================================================================
Total params: 6781 (26.49 KB)
Trainable params: 6605 (25.80 KB)
Non-trainable params: 176 (704.00 Byte)
_________________________________________________________________
None
Found GPU at: /device:GPU:0
------------ TRAINING ------------

Features shape: (3000, 199, 4)
Labels shape: (3000, 5)
Initializing checkpoint from scratch.
Epoch 0
Validation loss decreased. Saved checkpoint for step 1: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-1
Time:  32.11s, ---- Loss: 0.7887, Acc.: 0.5809, Val. Loss: 2.2636, Val. Acc.: 0.3632

Epoch 1
Validation loss decreased. Saved checkpoint for step 2: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-2
Time:  0.84s, ---- Loss: 0.6423, Acc.: 0.7367, Val. Loss: 1.9812, Val. Acc.: 0.3343

Epoch 2
Loss did not decrease. Count = 1
Time:  0.72s, ---- Loss: 0.5692, Acc.: 0.7854, Val. Loss: 2.3426, Val. Acc.: 0.4766

Epoch 3
Loss did not decrease. Count = 2
Time:  0.72s, ---- Loss: 0.5357, Acc.: 0.8043, Val. Loss: 1.9833, Val. Acc.: 0.4426

Epoch 4
Validation loss decreased. Saved checkpoint for step 5: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-3
Time:  0.83s, ---- Loss: 0.5059, Acc.: 0.8168, Val. Loss: 1.5654, Val. Acc.: 0.5590

Epoch 5
Loss did not decrease. Count = 1
Time:  0.71s, ---- Loss: 0.4908, Acc.: 0.8245, Val. Loss: 2.0303, Val. Acc.: 0.5422

Epoch 6
Loss did not decrease. Count = 2
Time:  0.71s, ---- Loss: 0.4754, Acc.: 0.8308, Val. Loss: 2.8464, Val. Acc.: 0.4234

Epoch 7
Loss did not decrease. Count = 3
Time:  0.72s, ---- Loss: 0.4563, Acc.: 0.8366, Val. Loss: 2.5634, Val. Acc.: 0.4753

Epoch 8
Loss did not decrease. Count = 4
Time:  0.72s, ---- Loss: 0.4465, Acc.: 0.8410, Val. Loss: 2.1895, Val. Acc.: 0.5224

Epoch 9
Loss did not decrease. Count = 5
Time:  0.73s, ---- Loss: 0.4344, Acc.: 0.8453, Val. Loss: 2.2806, Val. Acc.: 0.4773

Epoch 10
Loss did not decrease. Count = 6
Time:  0.72s, ---- Loss: 0.4286, Acc.: 0.8483, Val. Loss: 1.9522, Val. Acc.: 0.5291

Epoch 11
Loss did not decrease. Count = 7
Time:  0.71s, ---- Loss: 0.4216, Acc.: 0.8524, Val. Loss: 2.3427, Val. Acc.: 0.4563

Epoch 12
Loss did not decrease. Count = 8
Time:  0.72s, ---- Loss: 0.4189, Acc.: 0.8544, Val. Loss: 1.8553, Val. Acc.: 0.5395

Epoch 13
Validation loss decreased. Saved checkpoint for step 14: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-4
Time:  0.83s, ---- Loss: 0.4105, Acc.: 0.8581, Val. Loss: 1.5093, Val. Acc.: 0.6118

Epoch 14
Validation loss decreased. Saved checkpoint for step 15: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-5
Time:  0.84s, ---- Loss: 0.4072, Acc.: 0.8596, Val. Loss: 1.4843, Val. Acc.: 0.6333

Epoch 15
Loss did not decrease. Count = 1
Time:  0.72s, ---- Loss: 0.4109, Acc.: 0.8608, Val. Loss: 1.6601, Val. Acc.: 0.6109

Epoch 16
Validation loss decreased. Saved checkpoint for step 17: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-6
Time:  0.83s, ---- Loss: 0.4101, Acc.: 0.8627, Val. Loss: 1.4298, Val. Acc.: 0.6383

Epoch 17
Validation loss decreased. Saved checkpoint for step 18: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-7
Time:  0.84s, ---- Loss: 0.4037, Acc.: 0.8644, Val. Loss: 1.3573, Val. Acc.: 0.6581

Epoch 18
Validation loss decreased. Saved checkpoint for step 19: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-8
Time:  0.84s, ---- Loss: 0.3999, Acc.: 0.8660, Val. Loss: 1.0363, Val. Acc.: 0.7300

Epoch 19
Loss did not decrease. Count = 1
Time:  0.72s, ---- Loss: 0.3985, Acc.: 0.8670, Val. Loss: 1.1053, Val. Acc.: 0.7184

Epoch 20
Validation loss decreased. Saved checkpoint for step 21: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-9
Time:  0.83s, ---- Loss: 0.3951, Acc.: 0.8678, Val. Loss: 0.8911, Val. Acc.: 0.7764

Epoch 21
Validation loss decreased. Saved checkpoint for step 22: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-10
Time:  0.83s, ---- Loss: 0.3810, Acc.: 0.8702, Val. Loss: 0.7616, Val. Acc.: 0.8169

Epoch 22
Validation loss decreased. Saved checkpoint for step 23: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-11
Time:  0.83s, ---- Loss: 0.3792, Acc.: 0.8717, Val. Loss: 0.7496, Val. Acc.: 0.8221

Epoch 23
Loss did not decrease. Count = 1
Time:  0.73s, ---- Loss: 0.3771, Acc.: 0.8722, Val. Loss: 0.7965, Val. Acc.: 0.8086

Epoch 24
Validation loss decreased. Saved checkpoint for step 25: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-12
Time:  0.82s, ---- Loss: 0.3719, Acc.: 0.8735, Val. Loss: 0.7022, Val. Acc.: 0.8386

Epoch 25
Loss did not decrease. Count = 1
Time:  0.72s, ---- Loss: 0.3781, Acc.: 0.8739, Val. Loss: 0.7154, Val. Acc.: 0.8334

Epoch 26
Validation loss decreased. Saved checkpoint for step 27: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-13
Time:  0.83s, ---- Loss: 0.3806, Acc.: 0.8741, Val. Loss: 0.6960, Val. Acc.: 0.8417

Epoch 27
Validation loss decreased. Saved checkpoint for step 28: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-14
Time:  0.83s, ---- Loss: 0.3731, Acc.: 0.8746, Val. Loss: 0.6580, Val. Acc.: 0.8585

Epoch 28
Validation loss decreased. Saved checkpoint for step 29: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-15
Time:  0.83s, ---- Loss: 0.3717, Acc.: 0.8758, Val. Loss: 0.6454, Val. Acc.: 0.8643

Epoch 29
Validation loss decreased. Saved checkpoint for step 30: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-16
Time:  0.83s, ---- Loss: 0.3679, Acc.: 0.8769, Val. Loss: 0.6439, Val. Acc.: 0.8652

Epoch 30
Validation loss decreased. Saved checkpoint for step 31: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-17
Time:  0.84s, ---- Loss: 0.3727, Acc.: 0.8767, Val. Loss: 0.6399, Val. Acc.: 0.8679

Epoch 31
Validation loss decreased. Saved checkpoint for step 32: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-18
Time:  0.83s, ---- Loss: 0.3616, Acc.: 0.8784, Val. Loss: 0.6373, Val. Acc.: 0.8678

Epoch 32
Validation loss decreased. Saved checkpoint for step 33: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-19
Time:  0.83s, ---- Loss: 0.3618, Acc.: 0.8781, Val. Loss: 0.6309, Val. Acc.: 0.8718

Epoch 33
Validation loss decreased. Saved checkpoint for step 34: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-20
Time:  0.85s, ---- Loss: 0.3640, Acc.: 0.8789, Val. Loss: 0.6266, Val. Acc.: 0.8727

Epoch 34
Loss did not decrease. Count = 1
Time:  0.72s, ---- Loss: 0.3626, Acc.: 0.8793, Val. Loss: 0.6272, Val. Acc.: 0.8730

Epoch 35
Validation loss decreased. Saved checkpoint for step 36: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-21
Time:  0.83s, ---- Loss: 0.3609, Acc.: 0.8797, Val. Loss: 0.6260, Val. Acc.: 0.8736

Epoch 36
Validation loss decreased. Saved checkpoint for step 37: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-22
Time:  0.84s, ---- Loss: 0.3618, Acc.: 0.8795, Val. Loss: 0.6243, Val. Acc.: 0.8749

Epoch 37
Validation loss decreased. Saved checkpoint for step 38: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-23
Time:  0.84s, ---- Loss: 0.3585, Acc.: 0.8805, Val. Loss: 0.6243, Val. Acc.: 0.8747

Epoch 38
Validation loss decreased. Saved checkpoint for step 39: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-24
Time:  0.91s, ---- Loss: 0.3530, Acc.: 0.8803, Val. Loss: 0.6231, Val. Acc.: 0.8748

Epoch 39
Loss did not decrease. Count = 1
Time:  0.72s, ---- Loss: 0.3578, Acc.: 0.8804, Val. Loss: 0.6297, Val. Acc.: 0.8717

Epoch 40
Validation loss decreased. Saved checkpoint for step 41: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-25
Time:  0.85s, ---- Loss: 0.3602, Acc.: 0.8810, Val. Loss: 0.6229, Val. Acc.: 0.8756

Epoch 41
Validation loss decreased. Saved checkpoint for step 42: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-26
Time:  0.84s, ---- Loss: 0.3526, Acc.: 0.8821, Val. Loss: 0.6223, Val. Acc.: 0.8743

Epoch 42
Validation loss decreased. Saved checkpoint for step 43: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-27
Time:  0.84s, ---- Loss: 0.3572, Acc.: 0.8810, Val. Loss: 0.6211, Val. Acc.: 0.8763

Epoch 43
Loss did not decrease. Count = 1
Time:  0.71s, ---- Loss: 0.3550, Acc.: 0.8819, Val. Loss: 0.6302, Val. Acc.: 0.8724

Epoch 44
Loss did not decrease. Count = 2
Time:  0.72s, ---- Loss: 0.3470, Acc.: 0.8815, Val. Loss: 0.6234, Val. Acc.: 0.8761

Epoch 45
Loss did not decrease. Count = 3
Time:  0.72s, ---- Loss: 0.3552, Acc.: 0.8823, Val. Loss: 0.6238, Val. Acc.: 0.8755

Epoch 46
Loss did not decrease. Count = 4
Time:  0.71s, ---- Loss: 0.3559, Acc.: 0.8832, Val. Loss: 0.6236, Val. Acc.: 0.8759

Epoch 47
Loss did not decrease. Count = 5
Time:  0.71s, ---- Loss: 0.3547, Acc.: 0.8824, Val. Loss: 0.6212, Val. Acc.: 0.8771

Epoch 48
Validation loss decreased. Saved checkpoint for step 49: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-28
Time:  0.82s, ---- Loss: 0.3535, Acc.: 0.8830, Val. Loss: 0.6208, Val. Acc.: 0.8759

Epoch 49
Validation loss decreased. Saved checkpoint for step 50: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-29
Time:  0.83s, ---- Loss: 0.3513, Acc.: 0.8837, Val. Loss: 0.6168, Val. Acc.: 0.8789

Epoch 50
Validation loss decreased. Saved checkpoint for step 51: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-30
Time:  0.84s, ---- Loss: 0.3517, Acc.: 0.8835, Val. Loss: 0.6163, Val. Acc.: 0.8786

Epoch 51
Validation loss decreased. Saved checkpoint for step 52: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-31
Time:  0.85s, ---- Loss: 0.3545, Acc.: 0.8834, Val. Loss: 0.6133, Val. Acc.: 0.8788

Epoch 52
Validation loss decreased. Saved checkpoint for step 53: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-32
Time:  0.84s, ---- Loss: 0.3489, Acc.: 0.8838, Val. Loss: 0.6130, Val. Acc.: 0.8813

Epoch 53
Loss did not decrease. Count = 1
Time:  0.73s, ---- Loss: 0.3551, Acc.: 0.8845, Val. Loss: 0.6137, Val. Acc.: 0.8800

Epoch 54
Loss did not decrease. Count = 2
Time:  0.72s, ---- Loss: 0.3488, Acc.: 0.8846, Val. Loss: 0.6143, Val. Acc.: 0.8798

Epoch 55
Loss did not decrease. Count = 3
Time:  0.73s, ---- Loss: 0.3473, Acc.: 0.8840, Val. Loss: 0.6145, Val. Acc.: 0.8794

Epoch 56
Validation loss decreased. Saved checkpoint for step 57: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-33
Time:  0.83s, ---- Loss: 0.3508, Acc.: 0.8845, Val. Loss: 0.6113, Val. Acc.: 0.8814

Epoch 57
Loss did not decrease. Count = 1
Time:  0.72s, ---- Loss: 0.3474, Acc.: 0.8848, Val. Loss: 0.6116, Val. Acc.: 0.8808

Epoch 58
Loss did not decrease. Count = 2
Time:  0.72s, ---- Loss: 0.3496, Acc.: 0.8853, Val. Loss: 0.6134, Val. Acc.: 0.8801

Epoch 59
Loss did not decrease. Count = 3
Time:  0.72s, ---- Loss: 0.3466, Acc.: 0.8854, Val. Loss: 0.6113, Val. Acc.: 0.8817

Epoch 60
Loss did not decrease. Count = 4
Time:  0.72s, ---- Loss: 0.3503, Acc.: 0.8856, Val. Loss: 0.6124, Val. Acc.: 0.8808

Epoch 61
Validation loss decreased. Saved checkpoint for step 62: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-34
Time:  0.84s, ---- Loss: 0.3464, Acc.: 0.8859, Val. Loss: 0.6108, Val. Acc.: 0.8814

Epoch 62
Loss did not decrease. Count = 1
Time:  0.72s, ---- Loss: 0.3487, Acc.: 0.8857, Val. Loss: 0.6115, Val. Acc.: 0.8804

Epoch 63
Validation loss decreased. Saved checkpoint for step 64: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-35
Time:  0.85s, ---- Loss: 0.3465, Acc.: 0.8864, Val. Loss: 0.6097, Val. Acc.: 0.8817

Epoch 64
Loss did not decrease. Count = 1
Time:  0.72s, ---- Loss: 0.3482, Acc.: 0.8857, Val. Loss: 0.6122, Val. Acc.: 0.8820

Epoch 65
Loss did not decrease. Count = 2
Time:  0.72s, ---- Loss: 0.3495, Acc.: 0.8860, Val. Loss: 0.6130, Val. Acc.: 0.8799

Epoch 66
Loss did not decrease. Count = 3
Time:  0.73s, ---- Loss: 0.3454, Acc.: 0.8863, Val. Loss: 0.6134, Val. Acc.: 0.8802

Epoch 67
Loss did not decrease. Count = 4
Time:  0.72s, ---- Loss: 0.3464, Acc.: 0.8864, Val. Loss: 0.6113, Val. Acc.: 0.8815

Epoch 68
Loss did not decrease. Count = 5
Time:  0.72s, ---- Loss: 0.3478, Acc.: 0.8867, Val. Loss: 0.6100, Val. Acc.: 0.8818

Epoch 69
Loss did not decrease. Count = 6
Time:  0.71s, ---- Loss: 0.3454, Acc.: 0.8870, Val. Loss: 0.6098, Val. Acc.: 0.8819

Epoch 70
Loss did not decrease. Count = 7
Time:  0.71s, ---- Loss: 0.3444, Acc.: 0.8866, Val. Loss: 0.6101, Val. Acc.: 0.8825

Epoch 71
Loss did not decrease. Count = 8
Time:  0.71s, ---- Loss: 0.3428, Acc.: 0.8870, Val. Loss: 0.6103, Val. Acc.: 0.8816

Epoch 72
Validation loss decreased. Saved checkpoint for step 73: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-36
Time:  0.83s, ---- Loss: 0.3491, Acc.: 0.8872, Val. Loss: 0.6095, Val. Acc.: 0.8824

Epoch 73
Validation loss decreased. Saved checkpoint for step 74: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-37
Time:  0.83s, ---- Loss: 0.3475, Acc.: 0.8873, Val. Loss: 0.6089, Val. Acc.: 0.8815

Epoch 74
Loss did not decrease. Count = 1
Time:  0.71s, ---- Loss: 0.3460, Acc.: 0.8866, Val. Loss: 0.6090, Val. Acc.: 0.8832

Epoch 75
Validation loss decreased. Saved checkpoint for step 76: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-38
Time:  0.82s, ---- Loss: 0.3468, Acc.: 0.8869, Val. Loss: 0.6080, Val. Acc.: 0.8836

Epoch 76
Loss did not decrease. Count = 1
Time:  0.71s, ---- Loss: 0.3462, Acc.: 0.8874, Val. Loss: 0.6082, Val. Acc.: 0.8833

Epoch 77
Loss did not decrease. Count = 2
Time:  0.72s, ---- Loss: 0.3421, Acc.: 0.8873, Val. Loss: 0.6093, Val. Acc.: 0.8823

Epoch 78
Loss did not decrease. Count = 3
Time:  0.71s, ---- Loss: 0.3416, Acc.: 0.8875, Val. Loss: 0.6089, Val. Acc.: 0.8826

Epoch 79
Loss did not decrease. Count = 4
Time:  0.71s, ---- Loss: 0.3428, Acc.: 0.8875, Val. Loss: 0.6088, Val. Acc.: 0.8834

Epoch 80
Validation loss decreased. Saved checkpoint for step 81: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-39
Time:  0.84s, ---- Loss: 0.3408, Acc.: 0.8876, Val. Loss: 0.6077, Val. Acc.: 0.8841

Epoch 81
Loss did not decrease. Count = 1
Time:  0.73s, ---- Loss: 0.3434, Acc.: 0.8878, Val. Loss: 0.6080, Val. Acc.: 0.8842

Epoch 82
Loss did not decrease. Count = 2
Time:  0.73s, ---- Loss: 0.3424, Acc.: 0.8880, Val. Loss: 0.6079, Val. Acc.: 0.8821

Epoch 83
Validation loss decreased. Saved checkpoint for step 84: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-40
Time:  0.85s, ---- Loss: 0.3460, Acc.: 0.8877, Val. Loss: 0.6076, Val. Acc.: 0.8824

Epoch 84
Validation loss decreased. Saved checkpoint for step 85: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-41
Time:  0.85s, ---- Loss: 0.3516, Acc.: 0.8880, Val. Loss: 0.6052, Val. Acc.: 0.8828

Epoch 85
Loss did not decrease. Count = 1
Time:  0.72s, ---- Loss: 0.3449, Acc.: 0.8875, Val. Loss: 0.6062, Val. Acc.: 0.8829

Epoch 86
Loss did not decrease. Count = 2
Time:  0.72s, ---- Loss: 0.3429, Acc.: 0.8880, Val. Loss: 0.6076, Val. Acc.: 0.8832

Epoch 87
Loss did not decrease. Count = 3
Time:  0.71s, ---- Loss: 0.3446, Acc.: 0.8881, Val. Loss: 0.6076, Val. Acc.: 0.8834

Epoch 88
Loss did not decrease. Count = 4
Time:  0.71s, ---- Loss: 0.3391, Acc.: 0.8882, Val. Loss: 0.6057, Val. Acc.: 0.8827

Epoch 89
Loss did not decrease. Count = 5
Time:  0.71s, ---- Loss: 0.3431, Acc.: 0.8878, Val. Loss: 0.6074, Val. Acc.: 0.8830

Epoch 90
Loss did not decrease. Count = 6
Time:  0.72s, ---- Loss: 0.3419, Acc.: 0.8884, Val. Loss: 0.6055, Val. Acc.: 0.8838

Epoch 91
Loss did not decrease. Count = 7
Time:  0.72s, ---- Loss: 0.3401, Acc.: 0.8885, Val. Loss: 0.6053, Val. Acc.: 0.8847

Epoch 92
Loss did not decrease. Count = 8
Time:  0.72s, ---- Loss: 0.3456, Acc.: 0.8888, Val. Loss: 0.6066, Val. Acc.: 0.8842

Epoch 93
Loss did not decrease. Count = 9
Time:  0.72s, ---- Loss: 0.3441, Acc.: 0.8879, Val. Loss: 0.6060, Val. Acc.: 0.8832

Epoch 94
Loss did not decrease. Count = 10
Time:  0.72s, ---- Loss: 0.3457, Acc.: 0.8885, Val. Loss: 0.6058, Val. Acc.: 0.8821

Epoch 95
Loss did not decrease. Count = 11
Time:  0.72s, ---- Loss: 0.3444, Acc.: 0.8881, Val. Loss: 0.6060, Val. Acc.: 0.8842

Epoch 96
Loss did not decrease. Count = 12
Time:  0.72s, ---- Loss: 0.3381, Acc.: 0.8878, Val. Loss: 0.6070, Val. Acc.: 0.8825

Epoch 97
Loss did not decrease. Count = 13
Time:  0.72s, ---- Loss: 0.3437, Acc.: 0.8882, Val. Loss: 0.6056, Val. Acc.: 0.8847

Epoch 98
Loss did not decrease. Count = 14
Time:  0.71s, ---- Loss: 0.3431, Acc.: 0.8879, Val. Loss: 0.6060, Val. Acc.: 0.8841

Epoch 99
Validation loss decreased. Saved checkpoint for step 100: models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/tf_ckpts/ckpt-42
Time:  0.82s, ---- Loss: 0.3476, Acc.: 0.8876, Val. Loss: 0.6044, Val. Acc.: 0.8834

Saving at models/fivelabel_5000_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_/hist.png
Done in 1799.25s
