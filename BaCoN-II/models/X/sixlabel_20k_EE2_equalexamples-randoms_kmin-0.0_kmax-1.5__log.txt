2024-04-06 19:04:00.581206: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-06 19:04:00.581249: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-06 19:04:00.582891: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-06 19:04:01.623238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-04-06 19:04:04.051765: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2024-04-06 20:04:08.358439: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5907840000 exceeds 10% of free system memory.
2024-04-06 20:04:12.888162: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5907840000 exceeds 10% of free system memory.
using 1D layers and 4 channels
/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
loc = add_variable_fn(
/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
untransformed_scale = add_variable_fn(
Expected output dimension after layer: conv1d_flipout : 88
Expected output dimension after layer: conv1d_flipout_1 : 41
Expected output dimension after layer: conv1d_flipout_2 : 40
2024-04-06 20:15:24.900096: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5907840000 exceeds 10% of free system memory.
2024-04-06 20:15:36.066343: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5907840000 exceeds 10% of free system memory.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1712434548.705806    1442 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2024-04-06 20:15:56.637341: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 12.10GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-04-06 20:15:56.673228: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 12.10GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-04-06 20:15:57.966817: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 18.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-04-06 20:15:59.588955: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 18.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-04-06 20:16:00.240983: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 12.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-04-06 20:16:00.363559: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 12.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
W0000 00:00:1712434570.798819    1442 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update
W0000 00:00:1712434591.127957    1442 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update
Creating directory models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_
-------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune False
one_vs_all False
c_0 ['lcdm']
c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']
dataset_balanced False
include_last False
log_path models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5__log.txt
restore False
fname sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_
model_name custom
my_path None
DIR data/train
TEST_DIR
models_dir models/
save_ckpt True
out_path_overwrite False
curves_folder data/curve_files_sys/theory_error
save_processed_spectra False
cache_dir False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
norm_data_name /planck_ee2.txt
normalization stdcosmo
sample_pace 1
k_max 1.5
k_min 0.0
i_max None
i_min None
add_noise True
n_noisy_samples 10
add_shot False
add_sys True
add_cosvar True
sigma_sys None
sys_scaled None
sys_factor None
sys_max None
sigma_curves 0.05
sigma_curves_default 0.05
rescale_curves uniform
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable False
unfreeze False
lr 0.01
drop 0.5
n_epochs 100
val_size 0.15
test_size 0.0
batch_size 12000
patience 20
GPU True
TPU False
decay 0.95
BatchNorm True
padding valid
shuffle True
------------ CREATING DATA GENERATORS ------------
labels : ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
Labels encoding:
{'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
n_labels : 6
dgp - 20000 training examples
ds - 20000 training examples
fr - 20000 training examples
lcdm - 20000 training examples
rand - 20000 training examples
wcdm - 20000 training examples
N. of data files: 20000
get_all_indexes labels dict: {'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}
create_generators n_labels: 6
create_generators n_labels_eff: 6
create_generators len_c1: 1
Check for no duplicates in test: (0=ok):
0.0
Check for no duplicates in val: (0=ok):
0
N of indexes in training set: 17000
N of indexes in validation set: 3000
N of indexes in test set: 0
Check - total per class: 20000
--create_generators, train indexes
batch_size: 12000
- Cut sample
bs: 12000
N_labels: 6
N_noise: 10
len_c1: 1
Train index length: 17000
--create_generators, validation indexes
- Cut sample
bs: 12000
N_labels: 6
N_noise: 10
len_c1: 1
Val index length: 3000
len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 17000, 12000, 6, 10
--DataSet Train
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 1.5
Corresponding i_max is 362
Closest k to k_max is 1.500903
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (362, 1)
Final i_max used is 362
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 17000
n_indexes (n of file IDs read for each batch): 200
batch size: 12000
n_batches : 85
For each batch we read 200 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 12000 training examples
Input batch size: 12000
N of batches to cover all file IDs: 85
len(fname_list), batch_size, n_noisy_samples, n_batches: 102000, 12000, 10, 85
--DataSet Validation
DataSet Initialization
Using z bins [0, 1, 2, 3]
Normalisation file is /planck_ee2.txt
Specified k_max is 1.5
Corresponding i_max is 362
Closest k to k_max is 1.500903
Specified k_min is 0.0
Corresponding i_min is 0
Closest k to k_min is 0.01
New data dim: (362, 1)
Final i_max used is 362
Final i_min used is 0
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 6
N. n_classes in output: 6
LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']
list_IDs length: 3000
n_indexes (n of file IDs read for each batch): 200
batch size: 12000
n_batches : 15
For each batch we read 200 file IDs
For each file ID we have 6 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 12000 training examples
Input batch size: 12000
N of batches to cover all file IDs: 15
len(fname_list), batch_size, n_noisy_samples, n_batches: 18000, 12000, 10, 15
------------ DONE ------------
------------ BUILDING MODEL ------------
Input shape (362, 4)
Model: "model"
_________________________________________________________________
Layer (type)                Output Shape              Param #
=================================================================
input_1 (InputLayer)        [(None, 362, 4)]          0
conv1d_flipout (Conv1DFlip  (None, 177, 8)            648
out)
max_pooling1d (MaxPooling1  (None, 88, 8)             0
D)
batch_normalization (Batch  (None, 88, 8)             32
Normalization)
conv1d_flipout_1 (Conv1DFl  (None, 42, 16)            1296
ipout)
max_pooling1d_1 (MaxPoolin  (None, 41, 16)            0
g1D)
batch_normalization_1 (Bat  (None, 41, 16)            64
chNormalization)
conv1d_flipout_2 (Conv1DFl  (None, 40, 32)            2080
ipout)
batch_normalization_2 (Bat  (None, 40, 32)            128
chNormalization)
global_average_pooling1d (  (None, 32)                0
GlobalAveragePooling1D)
dense_flipout (DenseFlipou  (None, 32)                2080
t)
batch_normalization_3 (Bat  (None, 32)                128
chNormalization)
dense_flipout_1 (DenseFlip  (None, 6)                 390
out)
=================================================================
Total params: 6846 (26.74 KB)
Trainable params: 6670 (26.05 KB)
Non-trainable params: 176 (704.00 Byte)
_________________________________________________________________
None
Found GPU at: /device:GPU:0
------------ TRAINING ------------
Features shape: (12000, 362, 4)
Labels shape: (12000, 6)
Initializing checkpoint from scratch.
Epoch 0
Validation loss decreased. Saved checkpoint for step 1: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-1
Time:  56.57s, ---- Loss: 1.0397, Acc.: 0.4730, Val. Loss: 2.3948, Val. Acc.: 0.2423
Epoch 1
Validation loss decreased. Saved checkpoint for step 2: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-2
Time:  4.09s, ---- Loss: 0.8736, Acc.: 0.6097, Val. Loss: 2.0988, Val. Acc.: 0.2357
Epoch 2
Validation loss decreased. Saved checkpoint for step 3: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-3
Time:  4.13s, ---- Loss: 0.7953, Acc.: 0.6597, Val. Loss: 1.6356, Val. Acc.: 0.3737
Epoch 3
Validation loss decreased. Saved checkpoint for step 4: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-4
Time:  4.12s, ---- Loss: 0.7502, Acc.: 0.6819, Val. Loss: 1.5595, Val. Acc.: 0.4272
Epoch 4
Validation loss decreased. Saved checkpoint for step 5: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-5
Time:  4.13s, ---- Loss: 0.7255, Acc.: 0.6943, Val. Loss: 1.4475, Val. Acc.: 0.4662
Epoch 5
Validation loss decreased. Saved checkpoint for step 6: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-6
Time:  4.13s, ---- Loss: 0.7112, Acc.: 0.7032, Val. Loss: 1.3020, Val. Acc.: 0.5134
Epoch 6
Validation loss decreased. Saved checkpoint for step 7: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-7
Time:  4.11s, ---- Loss: 0.7027, Acc.: 0.7104, Val. Loss: 1.1902, Val. Acc.: 0.5567
Epoch 7
Loss did not decrease. Count = 1
Time:  3.96s, ---- Loss: 0.6861, Acc.: 0.7163, Val. Loss: 1.2074, Val. Acc.: 0.5658
Epoch 8
Loss did not decrease. Count = 2
Time:  3.98s, ---- Loss: 0.6686, Acc.: 0.7209, Val. Loss: 1.2587, Val. Acc.: 0.5651
Epoch 9
Loss did not decrease. Count = 3
Time:  3.97s, ---- Loss: 0.6489, Acc.: 0.7269, Val. Loss: 1.3420, Val. Acc.: 0.5501
Epoch 10
Loss did not decrease. Count = 4
Time:  3.95s, ---- Loss: 0.6336, Acc.: 0.7315, Val. Loss: 1.2299, Val. Acc.: 0.5745
Epoch 11
Loss did not decrease. Count = 5
Time:  3.97s, ---- Loss: 0.6235, Acc.: 0.7357, Val. Loss: 1.1984, Val. Acc.: 0.5885
Epoch 12
Validation loss decreased. Saved checkpoint for step 13: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-8
Time:  4.08s, ---- Loss: 0.6121, Acc.: 0.7384, Val. Loss: 1.0250, Val. Acc.: 0.6331
Epoch 13
Validation loss decreased. Saved checkpoint for step 14: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-9
Time:  4.11s, ---- Loss: 0.6046, Acc.: 0.7407, Val. Loss: 0.9753, Val. Acc.: 0.6482
Epoch 14
Validation loss decreased. Saved checkpoint for step 15: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-10
Time:  4.12s, ---- Loss: 0.5999, Acc.: 0.7437, Val. Loss: 0.8845, Val. Acc.: 0.6716
Epoch 15
Validation loss decreased. Saved checkpoint for step 16: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-11
Time:  4.08s, ---- Loss: 0.5941, Acc.: 0.7459, Val. Loss: 0.8341, Val. Acc.: 0.6886
Epoch 16
Validation loss decreased. Saved checkpoint for step 17: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-12
Time:  4.12s, ---- Loss: 0.5882, Acc.: 0.7476, Val. Loss: 0.7853, Val. Acc.: 0.7027
Epoch 17
Validation loss decreased. Saved checkpoint for step 18: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-13
Time:  4.10s, ---- Loss: 0.5863, Acc.: 0.7490, Val. Loss: 0.7630, Val. Acc.: 0.7107
Epoch 18
Validation loss decreased. Saved checkpoint for step 19: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-14
Time:  4.10s, ---- Loss: 0.5893, Acc.: 0.7504, Val. Loss: 0.7304, Val. Acc.: 0.7197
Epoch 19
Loss did not decrease. Count = 1
Time:  3.96s, ---- Loss: 0.5803, Acc.: 0.7516, Val. Loss: 0.7418, Val. Acc.: 0.7170
Epoch 20
Validation loss decreased. Saved checkpoint for step 21: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-15
Time:  4.09s, ---- Loss: 0.5762, Acc.: 0.7532, Val. Loss: 0.7201, Val. Acc.: 0.7240
Epoch 21
Validation loss decreased. Saved checkpoint for step 22: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-16
Time:  4.08s, ---- Loss: 0.5749, Acc.: 0.7539, Val. Loss: 0.6985, Val. Acc.: 0.7332
Epoch 22
Loss did not decrease. Count = 1
Time:  3.96s, ---- Loss: 0.5716, Acc.: 0.7544, Val. Loss: 0.7181, Val. Acc.: 0.7226
Epoch 23
Loss did not decrease. Count = 2
Time:  3.98s, ---- Loss: 0.5713, Acc.: 0.7555, Val. Loss: 0.7044, Val. Acc.: 0.7289
Epoch 24
Validation loss decreased. Saved checkpoint for step 25: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-17
Time:  4.10s, ---- Loss: 0.5735, Acc.: 0.7562, Val. Loss: 0.6922, Val. Acc.: 0.7336
Epoch 25
Validation loss decreased. Saved checkpoint for step 26: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-18
Time:  4.09s, ---- Loss: 0.5654, Acc.: 0.7571, Val. Loss: 0.6883, Val. Acc.: 0.7354
Epoch 26
Validation loss decreased. Saved checkpoint for step 27: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-19
Time:  4.09s, ---- Loss: 0.5661, Acc.: 0.7579, Val. Loss: 0.6855, Val. Acc.: 0.7365
Epoch 27
Loss did not decrease. Count = 1
Time:  3.96s, ---- Loss: 0.5644, Acc.: 0.7589, Val. Loss: 0.6858, Val. Acc.: 0.7377
Epoch 28
Validation loss decreased. Saved checkpoint for step 29: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-20
Time:  4.09s, ---- Loss: 0.5622, Acc.: 0.7593, Val. Loss: 0.6750, Val. Acc.: 0.7413
Epoch 29
Validation loss decreased. Saved checkpoint for step 30: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-21
Time:  4.11s, ---- Loss: 0.5645, Acc.: 0.7601, Val. Loss: 0.6682, Val. Acc.: 0.7441
Epoch 30
Validation loss decreased. Saved checkpoint for step 31: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-22
Time:  4.11s, ---- Loss: 0.5613, Acc.: 0.7607, Val. Loss: 0.6612, Val. Acc.: 0.7481
Epoch 31
Loss did not decrease. Count = 1
Time:  3.96s, ---- Loss: 0.5582, Acc.: 0.7612, Val. Loss: 0.6616, Val. Acc.: 0.7482
Epoch 32
Validation loss decreased. Saved checkpoint for step 33: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-23
Time:  4.09s, ---- Loss: 0.5589, Acc.: 0.7614, Val. Loss: 0.6584, Val. Acc.: 0.7500
Epoch 33
Validation loss decreased. Saved checkpoint for step 34: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-24
Time:  4.07s, ---- Loss: 0.5589, Acc.: 0.7625, Val. Loss: 0.6509, Val. Acc.: 0.7533
Epoch 34
Validation loss decreased. Saved checkpoint for step 35: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-25
Time:  4.03s, ---- Loss: 0.5583, Acc.: 0.7629, Val. Loss: 0.6449, Val. Acc.: 0.7558
Epoch 35
Validation loss decreased. Saved checkpoint for step 36: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-26
Time:  4.05s, ---- Loss: 0.5525, Acc.: 0.7634, Val. Loss: 0.6404, Val. Acc.: 0.7572
Epoch 36
Validation loss decreased. Saved checkpoint for step 37: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-27
Time:  4.08s, ---- Loss: 0.5548, Acc.: 0.7637, Val. Loss: 0.6400, Val. Acc.: 0.7583
Epoch 37
Validation loss decreased. Saved checkpoint for step 38: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-28
Time:  4.08s, ---- Loss: 0.5510, Acc.: 0.7643, Val. Loss: 0.6374, Val. Acc.: 0.7590
Epoch 38
Validation loss decreased. Saved checkpoint for step 39: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-29
Time:  4.05s, ---- Loss: 0.5512, Acc.: 0.7643, Val. Loss: 0.6343, Val. Acc.: 0.7609
Epoch 39
Loss did not decrease. Count = 1
Time:  3.92s, ---- Loss: 0.5526, Acc.: 0.7650, Val. Loss: 0.6349, Val. Acc.: 0.7602
Epoch 40
Validation loss decreased. Saved checkpoint for step 41: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-30
Time:  4.01s, ---- Loss: 0.5493, Acc.: 0.7653, Val. Loss: 0.6313, Val. Acc.: 0.7619
Epoch 41
Validation loss decreased. Saved checkpoint for step 42: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-31
Time:  4.05s, ---- Loss: 0.5484, Acc.: 0.7654, Val. Loss: 0.6307, Val. Acc.: 0.7620
Epoch 42
Loss did not decrease. Count = 1
Time:  3.92s, ---- Loss: 0.5487, Acc.: 0.7661, Val. Loss: 0.6311, Val. Acc.: 0.7615
Epoch 43
Validation loss decreased. Saved checkpoint for step 44: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-32
Time:  4.01s, ---- Loss: 0.5505, Acc.: 0.7665, Val. Loss: 0.6257, Val. Acc.: 0.7646
Epoch 44
Loss did not decrease. Count = 1
Time:  3.93s, ---- Loss: 0.5458, Acc.: 0.7667, Val. Loss: 0.6257, Val. Acc.: 0.7644
Epoch 45
Validation loss decreased. Saved checkpoint for step 46: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-33
Time:  4.03s, ---- Loss: 0.5471, Acc.: 0.7670, Val. Loss: 0.6256, Val. Acc.: 0.7642
Epoch 46
Validation loss decreased. Saved checkpoint for step 47: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-34
Time:  4.02s, ---- Loss: 0.5440, Acc.: 0.7672, Val. Loss: 0.6253, Val. Acc.: 0.7647
Epoch 47
Validation loss decreased. Saved checkpoint for step 48: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-35
Time:  4.06s, ---- Loss: 0.5448, Acc.: 0.7675, Val. Loss: 0.6235, Val. Acc.: 0.7656
Epoch 48
Validation loss decreased. Saved checkpoint for step 49: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-36
Time:  4.03s, ---- Loss: 0.5431, Acc.: 0.7676, Val. Loss: 0.6225, Val. Acc.: 0.7656
Epoch 49
Validation loss decreased. Saved checkpoint for step 50: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-37
Time:  4.01s, ---- Loss: 0.5441, Acc.: 0.7681, Val. Loss: 0.6220, Val. Acc.: 0.7658
Epoch 50
Validation loss decreased. Saved checkpoint for step 51: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-38
Time:  4.07s, ---- Loss: 0.5419, Acc.: 0.7686, Val. Loss: 0.6210, Val. Acc.: 0.7667
Epoch 51
Loss did not decrease. Count = 1
Time:  3.90s, ---- Loss: 0.5443, Acc.: 0.7687, Val. Loss: 0.6210, Val. Acc.: 0.7666
Epoch 52
Validation loss decreased. Saved checkpoint for step 53: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-39
Time:  4.03s, ---- Loss: 0.5413, Acc.: 0.7689, Val. Loss: 0.6196, Val. Acc.: 0.7669
Epoch 53
Validation loss decreased. Saved checkpoint for step 54: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-40
Time:  4.05s, ---- Loss: 0.5451, Acc.: 0.7688, Val. Loss: 0.6192, Val. Acc.: 0.7669
Epoch 54
Validation loss decreased. Saved checkpoint for step 55: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-41
Time:  4.04s, ---- Loss: 0.5425, Acc.: 0.7692, Val. Loss: 0.6187, Val. Acc.: 0.7670
Epoch 55
Loss did not decrease. Count = 1
Time:  3.95s, ---- Loss: 0.5404, Acc.: 0.7694, Val. Loss: 0.6197, Val. Acc.: 0.7672
Epoch 56
Validation loss decreased. Saved checkpoint for step 57: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-42
Time:  4.07s, ---- Loss: 0.5410, Acc.: 0.7694, Val. Loss: 0.6178, Val. Acc.: 0.7682
Epoch 57
Loss did not decrease. Count = 1
Time:  3.90s, ---- Loss: 0.5433, Acc.: 0.7694, Val. Loss: 0.6181, Val. Acc.: 0.7680
Epoch 58
Validation loss decreased. Saved checkpoint for step 59: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-43
Time:  4.03s, ---- Loss: 0.5407, Acc.: 0.7698, Val. Loss: 0.6171, Val. Acc.: 0.7684
Epoch 59
Validation loss decreased. Saved checkpoint for step 60: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-44
Time:  4.06s, ---- Loss: 0.5412, Acc.: 0.7701, Val. Loss: 0.6171, Val. Acc.: 0.7684
Epoch 60
Validation loss decreased. Saved checkpoint for step 61: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-45
Time:  4.03s, ---- Loss: 0.5416, Acc.: 0.7699, Val. Loss: 0.6160, Val. Acc.: 0.7688
Epoch 61
Loss did not decrease. Count = 1
Time:  3.93s, ---- Loss: 0.5383, Acc.: 0.7703, Val. Loss: 0.6161, Val. Acc.: 0.7693
Epoch 62
Validation loss decreased. Saved checkpoint for step 63: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-46
Time:  4.12s, ---- Loss: 0.5403, Acc.: 0.7703, Val. Loss: 0.6158, Val. Acc.: 0.7694
Epoch 63
Validation loss decreased. Saved checkpoint for step 64: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-47
Time:  4.08s, ---- Loss: 0.5398, Acc.: 0.7704, Val. Loss: 0.6154, Val. Acc.: 0.7694
Epoch 64
Validation loss decreased. Saved checkpoint for step 65: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-48
Time:  4.10s, ---- Loss: 0.5379, Acc.: 0.7705, Val. Loss: 0.6150, Val. Acc.: 0.7693
Epoch 65
Validation loss decreased. Saved checkpoint for step 66: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-49
Time:  4.09s, ---- Loss: 0.5374, Acc.: 0.7706, Val. Loss: 0.6143, Val. Acc.: 0.7699
Epoch 66
Loss did not decrease. Count = 1
Time:  3.95s, ---- Loss: 0.5376, Acc.: 0.7708, Val. Loss: 0.6144, Val. Acc.: 0.7693
Epoch 67
Loss did not decrease. Count = 2
Time:  3.98s, ---- Loss: 0.5375, Acc.: 0.7710, Val. Loss: 0.6149, Val. Acc.: 0.7699
Epoch 68
Validation loss decreased. Saved checkpoint for step 69: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-50
Time:  4.10s, ---- Loss: 0.5387, Acc.: 0.7707, Val. Loss: 0.6136, Val. Acc.: 0.7703
Epoch 69
Validation loss decreased. Saved checkpoint for step 70: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-51
Time:  4.09s, ---- Loss: 0.5372, Acc.: 0.7712, Val. Loss: 0.6130, Val. Acc.: 0.7708
Epoch 70
Loss did not decrease. Count = 1
Time:  3.98s, ---- Loss: 0.5382, Acc.: 0.7711, Val. Loss: 0.6138, Val. Acc.: 0.7696
Epoch 71
Loss did not decrease. Count = 2
Time:  3.96s, ---- Loss: 0.5357, Acc.: 0.7713, Val. Loss: 0.6131, Val. Acc.: 0.7702
Epoch 72
Loss did not decrease. Count = 3
Time:  3.95s, ---- Loss: 0.5359, Acc.: 0.7714, Val. Loss: 0.6131, Val. Acc.: 0.7704
Epoch 73
Loss did not decrease. Count = 4
Time:  3.98s, ---- Loss: 0.5359, Acc.: 0.7716, Val. Loss: 0.6134, Val. Acc.: 0.7697
Epoch 74
Validation loss decreased. Saved checkpoint for step 75: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-52
Time:  4.10s, ---- Loss: 0.5364, Acc.: 0.7713, Val. Loss: 0.6128, Val. Acc.: 0.7709
Epoch 75
Loss did not decrease. Count = 1
Time:  3.97s, ---- Loss: 0.5372, Acc.: 0.7715, Val. Loss: 0.6128, Val. Acc.: 0.7707
Epoch 76
Validation loss decreased. Saved checkpoint for step 77: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-53
Time:  4.10s, ---- Loss: 0.5360, Acc.: 0.7714, Val. Loss: 0.6127, Val. Acc.: 0.7703
Epoch 77
Validation loss decreased. Saved checkpoint for step 78: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-54
Time:  4.11s, ---- Loss: 0.5374, Acc.: 0.7716, Val. Loss: 0.6124, Val. Acc.: 0.7703
Epoch 78
Validation loss decreased. Saved checkpoint for step 79: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-55
Time:  4.10s, ---- Loss: 0.5359, Acc.: 0.7718, Val. Loss: 0.6122, Val. Acc.: 0.7706
Epoch 79
Loss did not decrease. Count = 1
Time:  3.98s, ---- Loss: 0.5355, Acc.: 0.7717, Val. Loss: 0.6127, Val. Acc.: 0.7715
Epoch 80
Validation loss decreased. Saved checkpoint for step 81: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-56
Time:  4.08s, ---- Loss: 0.5338, Acc.: 0.7718, Val. Loss: 0.6121, Val. Acc.: 0.7713
Epoch 81
Validation loss decreased. Saved checkpoint for step 82: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-57
Time:  4.09s, ---- Loss: 0.5342, Acc.: 0.7722, Val. Loss: 0.6117, Val. Acc.: 0.7713
Epoch 82
Loss did not decrease. Count = 1
Time:  3.95s, ---- Loss: 0.5376, Acc.: 0.7722, Val. Loss: 0.6122, Val. Acc.: 0.7710
Epoch 83
Validation loss decreased. Saved checkpoint for step 84: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-58
Time:  4.10s, ---- Loss: 0.5356, Acc.: 0.7723, Val. Loss: 0.6116, Val. Acc.: 0.7711
Epoch 84
Validation loss decreased. Saved checkpoint for step 85: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-59
Time:  4.10s, ---- Loss: 0.5373, Acc.: 0.7722, Val. Loss: 0.6116, Val. Acc.: 0.7706
Epoch 85
Loss did not decrease. Count = 1
Time:  3.97s, ---- Loss: 0.5355, Acc.: 0.7720, Val. Loss: 0.6122, Val. Acc.: 0.7710
Epoch 86
Validation loss decreased. Saved checkpoint for step 87: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-60
Time:  4.09s, ---- Loss: 0.5348, Acc.: 0.7723, Val. Loss: 0.6116, Val. Acc.: 0.7712
Epoch 87
Validation loss decreased. Saved checkpoint for step 88: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-61
Time:  4.10s, ---- Loss: 0.5346, Acc.: 0.7724, Val. Loss: 0.6114, Val. Acc.: 0.7706
Epoch 88
Loss did not decrease. Count = 1
Time:  3.97s, ---- Loss: 0.5336, Acc.: 0.7725, Val. Loss: 0.6120, Val. Acc.: 0.7709
Epoch 89
Loss did not decrease. Count = 2
Time:  3.95s, ---- Loss: 0.5361, Acc.: 0.7726, Val. Loss: 0.6115, Val. Acc.: 0.7707
Epoch 90
Validation loss decreased. Saved checkpoint for step 91: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-62
Time:  4.09s, ---- Loss: 0.5355, Acc.: 0.7723, Val. Loss: 0.6109, Val. Acc.: 0.7714
Epoch 91
Loss did not decrease. Count = 1
Time:  3.97s, ---- Loss: 0.5338, Acc.: 0.7723, Val. Loss: 0.6110, Val. Acc.: 0.7710
Epoch 92
Loss did not decrease. Count = 2
Time:  3.96s, ---- Loss: 0.5340, Acc.: 0.7725, Val. Loss: 0.6109, Val. Acc.: 0.7710
Epoch 93
Loss did not decrease. Count = 3
Time:  3.96s, ---- Loss: 0.5347, Acc.: 0.7723, Val. Loss: 0.6113, Val. Acc.: 0.7708
Epoch 94
Validation loss decreased. Saved checkpoint for step 95: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-63
Time:  4.08s, ---- Loss: 0.5360, Acc.: 0.7723, Val. Loss: 0.6106, Val. Acc.: 0.7715
Epoch 95
Loss did not decrease. Count = 1
Time:  3.94s, ---- Loss: 0.5336, Acc.: 0.7723, Val. Loss: 0.6107, Val. Acc.: 0.7713
Epoch 96
Loss did not decrease. Count = 2
Time:  3.96s, ---- Loss: 0.5321, Acc.: 0.7727, Val. Loss: 0.6110, Val. Acc.: 0.7711
Epoch 97
Validation loss decreased. Saved checkpoint for step 98: models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/tf_ckpts/ckpt-64
Time:  4.09s, ---- Loss: 0.5332, Acc.: 0.7726, Val. Loss: 0.6105, Val. Acc.: 0.7717
Epoch 98
Loss did not decrease. Count = 1
Time:  3.96s, ---- Loss: 0.5331, Acc.: 0.7725, Val. Loss: 0.6108, Val. Acc.: 0.7717
Epoch 99
Loss did not decrease. Count = 2
Time:  3.97s, ---- Loss: 0.5353, Acc.: 0.7729, Val. Loss: 0.6110, Val. Acc.: 0.7715
Saving at models/sixlabel_20k_EE2_equalexamples-randoms_kmin-0.0_kmax-1.5_/hist.png
Done in 4748.52s
