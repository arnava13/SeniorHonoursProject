{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2-LDl_8Nai6"
      },
      "source": [
        "# Classification of power spectra with BaCoN\n",
        "\n",
        "Arnav Agarwal\n",
        "\n",
        "Updated training notebook including minimum k_mode parameters, padding parameter, compatibility with TPU execution, github push/pull compatibility and minor improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErLy38KVuVZd"
      },
      "source": [
        "If you are saving models, you will need to use git push within the colab environment, or download the model files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flvg9eICNhqn"
      },
      "source": [
        "## Load the code\n",
        "\n",
        "This notebook is designed to run the training on google colab and testing on local machine (due to issues with colab's latex implementation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTGEv96quTkb"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzX4nFm1BQqd",
        "outputId": "2e2c3892-0648-43ac-90cb-b15a69e381d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SeniorHonoursProject'...\n",
            "remote: Enumerating objects: 293643, done.\u001b[K\n",
            "remote: Counting objects: 100% (16913/16913), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16535/16535), done.\u001b[K\n",
            "remote: Total 293643 (delta 378), reused 16911 (delta 376), pack-reused 276730\u001b[K\n",
            "Receiving objects: 100% (293643/293643), 3.55 GiB | 15.77 MiB/s, done.\n",
            "Resolving deltas: 100% (3570/3570), done.\n",
            "Updating files: 100% (32753/32753), done.\n",
            "/content/SeniorHonoursProject/BaCoN-II\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/arnava13/SeniorHonoursProject\n",
        "%cd SeniorHonoursProject/BaCoN-II"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVBtCPaWw60R"
      },
      "source": [
        "## Run training in shell\n",
        "only do this when not loading trained network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HamIuy_Qx2cf",
        "outputId": "1ffc46ba-371e-42ec-a880-becd17a913af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for interlap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for luigi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "lib5c version 0.6.1\n",
            "Requirement already satisfied: tensorflow==2.15.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-probability==0.23.0 in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (1.25.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (2.2.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (0.5.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (0.1.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q lib5c\n",
        "!lib5c -v\n",
        "!pip install tensorflow==2.15.0\n",
        "!pip install tensorflow-probability==0.23.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gv8_CyoHB0_Y"
      },
      "outputs": [],
      "source": [
        "import os;\n",
        "import subprocess\n",
        "import threading\n",
        "import argparse\n",
        "from test import *\n",
        "from utils import *\n",
        "from models import *\n",
        "from data_generator import *\n",
        "from train import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ex2LIYxl6cjV"
      },
      "outputs": [],
      "source": [
        "from importer import load_model_for_test, my_predict, predict_bayes_label, predict_mean_proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcsxVMqDBpJ_",
        "outputId": "b12c256f-270f-4640-949a-c0eb9fe7bd01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ],
      "source": [
        "print(os.environ['PATH'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Oaw1mzZqTsnu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"font.family\"] = 'serif'\n",
        "plt.rcParams[\"mathtext.fontset\"] = \"cm\"\n",
        "latex_path = '/usr/local/texlive/2024/bin/universal-darwin/'\n",
        "if latex_path not in os.environ['PATH']:\n",
        "    os.environ['PATH'] += os.pathsep + latex_path\n",
        "plt.rcParams[\"text.usetex\"] = True\n",
        "plt.rcParams['text.latex.preamble'] = r'\\usepackage{siunitx}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fZiOh8u5QowF"
      },
      "outputs": [],
      "source": [
        "def handle_output(stream, log_file):\n",
        "    for line in iter(stream.readline, b''):  # This iterates over the output line by line\n",
        "        my_bytes = line.strip()\n",
        "        if my_bytes:\n",
        "            output = f\"{my_bytes.decode('utf-8')}\"\n",
        "            print(output)\n",
        "            log_file.write(output + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXuIOzFXFQL9"
      },
      "source": [
        "### set parameters for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w8B1FUswFQL9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Training options:\n",
        "\n",
        "# ------------------ change the training directory and model name here ------------------\n",
        "\n",
        "# training data directory path\n",
        "DIR='data/train'\n",
        "## (batch size, is a multiple of #classes * noise realisations, e.g. 5 classes, 10 noise --> must be multiple of 50)\n",
        "batch_size='3000'\n",
        "\n",
        "# training the network\n",
        "GPU='True' # Use GPUs in training?\n",
        "TPU='False' # Train with TPUs?\n",
        "# which kind of model\n",
        "bayesian='True' # Bayesian NN or traditional NN (weights single valued or distributions?)\n",
        "n_epochs='100' # How many epochs to train for?\n",
        "\n",
        "\n",
        "# ------------------ other parameters ------------------\n",
        "\n",
        "# Directories\n",
        "mypath=None # Parent directory\n",
        "\n",
        "# Edit this with base model directory\n",
        "mdir='models/'\n",
        "\n",
        "# normalisation file\n",
        "norm_data_name = '/planck_ee2.txt'\n",
        "\n",
        "# scale cut and resolution\n",
        "k_max='2.5' # max of k-modes to include?\n",
        "k_min='0.0' # min of k-modes to include?\n",
        "sample_pace='1'\n",
        "\n",
        "# should the processed curves be saved in an extra file (False recommended, only efficient for n_epochs = 1)\n",
        "save_processed_spectra='False'\n",
        "\n",
        "# fine-tune = only 'LCDM' and 'non-LCDM'\n",
        "fine_tune='False' # Model trained to distinguish between 'LCDM' and 'non-LCDM' classes or between all clases. i.e. binary or multi-classifier\n",
        "\n",
        "# Cache directory, set as False to cache on memory.\n",
        "cache_dir='False'\n",
        "\n",
        "# -------------------- noise model --------------------\n",
        "\n",
        "# noise model - Cosmic variance and shot noise are Euclid-like\n",
        "n_noisy_samples='10' # How many noise examples to train on?\n",
        "add_noise='True'# Add noise?\n",
        "add_shot='False'# Add shot noise term? -- default should be False\n",
        "add_sys='True'# Add systematic error term?\n",
        "add_cosvar='True'# Add cosmic variance term?\n",
        "\n",
        "# path to folder with theory error curves\n",
        "curves_folder = 'data/curve_files_sys/theory_error'; sigma_curves_default = '0.05'\n",
        "# change rescale distribution of theory error curves, uniform recommended\n",
        "rescale_curves = 'uniform' # or gaussian or None\n",
        "\n",
        "# sys error curves - relative scale of curve-amplitude\n",
        "sigma_curves='0.05'\n",
        "\n",
        "# ----------------- Model Name Generation -------------\n",
        "\n",
        "#Two or Six label?\n",
        "\n",
        "if fine_tune != 'True':\n",
        "  two_or_six = 'sixlabel'\n",
        "else:\n",
        "  two_or_six = 'twolabel'\n",
        "\n",
        "\n",
        "#two_or_six = 'fivelabel'\n",
        "\n",
        "#Examples Per Class?\n",
        "examples_per_class = '5k'\n",
        "\n",
        "#Generation code?\n",
        "gen_code = \"EE2\"\n",
        "\n",
        "#LCDM * Filter or Equal Examples from each class * Filter for randoms?\n",
        "randstype = \"equalexamples-randoms\"\n",
        "#randstype = \"LCDM-randoms\"\n",
        "\n",
        "#Additional Notes?\n",
        "fname_notes = \"ds_binC\"\n",
        "\n",
        "#Assemble fname\n",
        "fname = f\"{two_or_six}_{examples_per_class}_{gen_code}_{randstype}_kmin-{k_min}_kmax-{k_max}_{fname_notes}\"\n",
        "\n",
        "# -------------------- additional training settings --------------------\n",
        "\n",
        "\n",
        "# Type of model\n",
        "model_name='custom' # Custom or dummy - dummy has presets as given in models.py\n",
        "\n",
        "#Test mode?\n",
        "test_mode='False' # Network trained on 1 batch of minimal size or not?\n",
        "seed='1312' # Initial seed for test mode batch\n",
        "# Saving or restoring training\n",
        "restore='False' # Restore training from a checkpoint?\n",
        "save_ckpt='True' # Save checkpoints?\n",
        "\n",
        "\n",
        "val_size='0.15' # Validation set % of data set\n",
        "\n",
        "add_FT_dense='False' #if True, adds an additional dense layer before the final 2D one\n",
        "\n",
        "patience='20' # terminate training after 'patience' epochs if no decrease in loss function\n",
        "lr='0.01' # learning rate\n",
        "decay='0.95' #decay rate: If None : Adam(lr),\n",
        "\n",
        "padding='valid'\n",
        "\n",
        "# -------------------- BNN parameters -----------------\n",
        "\n",
        "# Example image details\n",
        "im_depth='500' # Number in z direction (e.g. 500 wave modes for P(k) examples)\n",
        "im_width='1' # Number in y direction (1 is a 2D image : P(k,mu))\n",
        "im_channels='4'  # Number in x direction (e.g. 4 redshifts for P(k,z))\n",
        "swap_axes='True' # Do we swap depth and width axes? True if we have 1D image in 4 channels\n",
        "sort_labels='True' # Sorts labels in ascending/alphabetical order\n",
        "\n",
        "z1='0' # which z-bins to include? Assumes each channel is a new z bin.\n",
        "z2='1' # which z-bins to include? Assumes each channel is a new z bin.\n",
        "z3='2' # which z-bins to include? Assumes each channel is a new z bin.\n",
        "z4='3' # which z-bins to include? Assumes each channel is a new z bin.\n",
        "\n",
        "# Number of layers and kernel sizes\n",
        "k1='10'\n",
        "k2='5'\n",
        "k3='2'\n",
        " # The dimensionality of the output space (i.e. the number of filters in the convolution)\n",
        "f1='8'\n",
        "f2='16'\n",
        "f3='32'\n",
        " # Stride of each layer's kernel\n",
        "s1='2'\n",
        "s2='2'\n",
        "s3='1'\n",
        "# Pooling layer sizes\n",
        "p1='2'\n",
        "p2='2'\n",
        "p3='0'\n",
        "# Strides in Pooling layer\n",
        "sp1='2'\n",
        "sp2='1'\n",
        "sp3='0'\n",
        "\n",
        "n_dense='1' # Number of dense layers\n",
        "\n",
        "# labels of different cosmologies\n",
        "#c0_label = 'lcdm'\n",
        "#c1_label = 'fR dgp wcdm'\n",
        "\n",
        "log_path = mdir + fname + '_log'\n",
        "\n",
        "if fine_tune == \"True\":\n",
        "    log_path += '_FT'\n",
        "\n",
        "log_path += '.txt'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YN5PCcWXWgr"
      },
      "source": [
        "### run the training\n",
        "this may take a while to run, also expect some (harmless) error messages in the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "pdSzp6AFFQL-",
        "outputId": "51ebb0d0-8aeb-4b92-f927-6179db0460ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-08 18:37:07.046686: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-08 18:37:07.046739: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-08 18:37:07.048635: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-08 18:37:08.077720: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-04-08 18:37:10.524542: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-11 (handle_output):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-7-331269d33e17>\", line 7, in handle_output\n",
            "ValueError: I/O operation on closed file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8116102008fc>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mstderr_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mstdout_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mstderr_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# -------------- training loads parameters entered above  --------------\n",
        "proc = subprocess.Popen([\"python3\", \"train.py\", \"--test_mode\" , test_mode, \"--seed\", seed, \\\n",
        "                     \"--bayesian\", bayesian, \"--model_name\", model_name, \\\n",
        "                     \"--fine_tune\", fine_tune, \"--log_path\", log_path,\\\n",
        "                     \"--restore\", restore, \\\n",
        "                     \"--models_dir\", mdir, \\\n",
        "                     \"--cache_dir\", cache_dir, \\\n",
        "                     \"--fname\", fname, \\\n",
        "                     \"--DIR\", DIR, \\\n",
        "                     '--norm_data_name', norm_data_name, \\\n",
        "                     '--curves_folder', curves_folder,\\\n",
        "                     \"--c_0\", 'lcdm', \"--c_1\", 'fR', 'dgp', 'wcdm', 'ds', 'rand', \\\n",
        "                     \"--save_ckpt\", save_ckpt, \\\n",
        "                     \"--im_depth\", im_depth, \"--im_width\", im_width, \"--im_channels\", im_channels, \\\n",
        "                     \"--swap_axes\", swap_axes, \\\n",
        "                     \"--sort_labels\", sort_labels, \\\n",
        "                     \"--add_noise\", add_noise, \"--add_shot\", add_shot, \"--add_sys\", add_sys,\"--add_cosvar\", add_cosvar, \\\n",
        "                     \"--sigma_curves\", sigma_curves, \\\n",
        "                     \"--sigma_curves_default\", sigma_curves_default, \\\n",
        "                     \"--save_processed_spectra\", save_processed_spectra, \\\n",
        "                     \"--sample_pace\", sample_pace,\\\n",
        "                     \"--n_noisy_samples\", n_noisy_samples, \\\n",
        "                     \"--rescale_curves\", rescale_curves, \\\n",
        "                     \"--val_size\", val_size, \\\n",
        "                     \"--z_bins\", z1,z2,z3,z4, \\\n",
        "                     \"--filters\", f1,f2,f3, \"--kernel_sizes\", k1,k2,k3, \"--strides\", s1,s2,s3, \"--pool_sizes\", p1,p2,p3, \"--strides_pooling\", sp1,sp2,sp3, \\\n",
        "                     \"--k_max\", k_max,\\\n",
        "                     \"--k_min\", k_min,\\\n",
        "                     \"--n_dense\", n_dense,\\\n",
        "                     \"--add_FT_dense\", add_FT_dense, \\\n",
        "                     \"--n_epochs\", n_epochs, \"--patience\", patience, \"--batch_size\", batch_size, \"--lr\", lr, \\\n",
        "                     \"--decay\", decay, \\\n",
        "                     \"--GPU\", GPU, \\\n",
        "                     \"--TPU\", TPU, \\\n",
        "                     \"--padding\", padding],\\\n",
        "                     stdout=subprocess.PIPE, \\\n",
        "                     stderr=subprocess.PIPE)\n",
        "\n",
        "with open(log_path, \"w\") as log_file:\n",
        "\n",
        "    stdout_thread = threading.Thread(target=handle_output, args=(proc.stdout, log_file))\n",
        "    stderr_thread = threading.Thread(target=handle_output, args=(proc.stderr, log_file))\n",
        "\n",
        "    stdout_thread.start()\n",
        "    stderr_thread.start()\n",
        "\n",
        "    stdout_thread.join()\n",
        "    stderr_thread.join()\n",
        "\n",
        "    proc.wait()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ."
      ],
      "metadata": {
        "id": "03WidS22CZFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Train bin B\""
      ],
      "metadata": {
        "id": "EUHiMj9-CapA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git push"
      ],
      "metadata": {
        "id": "ZZyqMYbWCchP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6v9Ql7T87Me"
      },
      "outputs": [],
      "source": [
        "ls models/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3sLuZopQ8Dy"
      },
      "source": [
        "## Run the test of the model\n",
        "- insert the path+name of the log file of the model\n",
        "- change the test directory\n",
        "- change the custom name that will be used for the confusion matrix\n",
        "\n",
        "The pdf file with the confusion matrix is saved in the model folder.\n",
        "\n",
        "#### I have been running this locally, whereas the train part is run on colab. I simply push the output models and I save the notebook before switching to local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtOfbiLLBpJ_",
        "outputId": "dff4d9c8-3b65-4f9a-f76f-256a674b463f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/arnav/.pyenv/versions/3.11.8/bin:/opt/homebrew/opt/llvm/bin:/Users/arnav/.pyenv/plugins/pyenv-virtualenv/shims:/Users/arnav/.pyenv/shims:/Users/arnav/.pyenv/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/usr/local/texlive/2024/bin/universal-darwin/\n"
          ]
        }
      ],
      "source": [
        "print(os.environ['PATH'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c98CGa3KJlyf",
        "outputId": "13ca20fa-300e-4114-c356-494c4b2a3399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading log from models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binB/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binB_log.txt \n",
            "\n",
            " -------- Loaded parameters:\n",
            "\n",
            " -------- Loaded parameters:\n",
            "bayesian True\n",
            "test_mode False\n",
            "n_test_idx 2\n",
            "seed 1312\n",
            "fine_tune False\n",
            "one_vs_all False\n",
            "c_0 ['lcdm']\n",
            "c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']\n",
            "dataset_balanced False\n",
            "include_last False\n",
            "log_path \n",
            "restore False\n",
            "fname sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binB\n",
            "model_name custom\n",
            "my_path None\n",
            "DIR data/train\n",
            "TEST_DIR data/test/\n",
            "models_dir models/\n",
            "save_ckpt True\n",
            "out_path_overwrite False\n",
            "curves_folder data/curve_files_sys/theory_error\n",
            "save_processed_spectra False\n",
            "cache_dir False\n",
            "im_depth 500\n",
            "im_width 1\n",
            "im_channels 4\n",
            "swap_axes True\n",
            "sort_labels True\n",
            "norm_data_name /planck_ee2.txt\n",
            "normalization stdcosmo\n",
            "sample_pace 1\n",
            "k_max 2.5\n",
            "k_min 0.0\n",
            "i_max None\n",
            "i_min None\n",
            "add_noise True\n",
            "n_noisy_samples 10\n",
            "add_shot False\n",
            "add_sys True\n",
            "add_cosvar True\n",
            "sigma_sys None\n",
            "sys_scaled None\n",
            "sys_factor None\n",
            "sys_max None\n",
            "sigma_curves 0.05\n",
            "sigma_curves_default 0.05\n",
            "rescale_curves uniform\n",
            "z_bins [0, 1, 2, 3]\n",
            "n_dense 1\n",
            "filters [8, 16, 32]\n",
            "kernel_sizes [10, 5, 2]\n",
            "strides [2, 2, 1]\n",
            "pool_sizes [2, 2, 0]\n",
            "strides_pooling [2, 1, 0]\n",
            "add_FT_dense False\n",
            "trainable False\n",
            "unfreeze False\n",
            "lr 0.01\n",
            "drop 0.5\n",
            "n_epochs 100\n",
            "val_size 0.15\n",
            "test_size 0.0\n",
            "batch_size 3000\n",
            "patience 20\n",
            "GPU True\n",
            "TPU False\n",
            "decay 0.95\n",
            "BatchNorm True\n",
            "padding valid\n",
            "shuffle True\n",
            "group_lab_dict {'True': 'non_lcdm', 'dgp': 'non_lcdm', 'ds': 'non_lcdm', 'fR': 'non_lcdm', 'rand--save_ckpt': 'non_lcdm', 'wcdm': 'non_lcdm', 'lcdm': 'lcdm'}\n",
            "FLAGS.one_vs_all : False\n",
            "TEST_NORM_FILE: FLAGS.norm_data_name from log_path  /planck_ee2.txt\n",
            "Saving cm at confusion\n",
            "Setting save_indexes to False\n",
            "Using data in the directory data/test\n",
            "TEST_NORM_FILE: FLAGS.norm_data_name after args written to FLAGS /planck_ee2.txt\n",
            "-------------------- out_path with FLAGS_fname models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binB\n",
            "\n",
            " -------- Parameters:\n",
            "bayesian True\n",
            "test_mode False\n",
            "n_test_idx 2\n",
            "seed 1312\n",
            "fine_tune False\n",
            "one_vs_all False\n",
            "c_0 ['lcdm']\n",
            "c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']\n",
            "dataset_balanced False\n",
            "include_last False\n",
            "log_path \n",
            "restore False\n",
            "fname sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binB\n",
            "model_name custom\n",
            "my_path None\n",
            "DIR data/train\n",
            "TEST_DIR data/test\n",
            "models_dir models/\n",
            "save_ckpt True\n",
            "out_path_overwrite False\n",
            "curves_folder data/curve_files_sys/theory_error\n",
            "save_processed_spectra False\n",
            "cache_dir False\n",
            "im_depth 500\n",
            "im_width 1\n",
            "im_channels 4\n",
            "swap_axes True\n",
            "sort_labels True\n",
            "norm_data_name /planck_ee2.txt\n",
            "normalization stdcosmo\n",
            "sample_pace 1\n",
            "k_max 2.5\n",
            "k_min 0.0\n",
            "i_max None\n",
            "i_min None\n",
            "add_noise True\n",
            "n_noisy_samples 10\n",
            "add_shot False\n",
            "add_sys True\n",
            "add_cosvar True\n",
            "sigma_sys None\n",
            "sys_scaled None\n",
            "sys_factor None\n",
            "sys_max None\n",
            "sigma_curves 0.05\n",
            "sigma_curves_default 0.05\n",
            "rescale_curves uniform\n",
            "z_bins [0, 1, 2, 3]\n",
            "n_dense 1\n",
            "filters [8, 16, 32]\n",
            "kernel_sizes [10, 5, 2]\n",
            "strides [2, 2, 1]\n",
            "pool_sizes [2, 2, 0]\n",
            "strides_pooling [2, 1, 0]\n",
            "add_FT_dense False\n",
            "trainable False\n",
            "unfreeze False\n",
            "lr 0.01\n",
            "drop 0.5\n",
            "n_epochs 100\n",
            "val_size 0.15\n",
            "test_size 0.0\n",
            "batch_size 600\n",
            "patience 20\n",
            "GPU True\n",
            "TPU False\n",
            "decay 0.95\n",
            "BatchNorm True\n",
            "padding valid\n",
            "shuffle True\n",
            "group_lab_dict {'True': 'non_lcdm', 'dgp': 'non_lcdm', 'ds': 'non_lcdm', 'fR': 'non_lcdm', 'rand--save_ckpt': 'non_lcdm', 'wcdm': 'non_lcdm', 'lcdm': 'lcdm'}\n",
            "save_indexes False\n",
            "n_classes 6\n",
            "------------ CREATING DATASETS ------------\n",
            "\n",
            "labels : ['dgp', 'ds_binA', 'fr', 'lcdm', 'rand', 'wcdm']\n",
            "Labels encoding: \n",
            "{'dgp': 0, 'ds_binA': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}\n",
            "n_labels : 6\n",
            "dgp - 250 training examples\n",
            "ds_binA - 250 training examples\n",
            "fr - 250 training examples\n",
            "lcdm - 250 training examples\n",
            "rand - 250 training examples\n",
            "wcdm - 250 training examples\n",
            "\n",
            "N. of data files: 250\n",
            "get_all_indexes labels dict: {'dgp': 0, 'ds_binA': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}\n",
            "create_generators n_labels_eff: 6\n",
            "create_generators len_c1: 1\n",
            "--Train\n",
            "batch_size: 600\n",
            "- Cut sample\n",
            "bs: 600\n",
            "N_labels: 6\n",
            "N_noise: 10\n",
            "len_c1: 1\n",
            "Indexes length: 250\n",
            "n_keep: 250\n",
            "Sampling\n",
            "New length: 250\n",
            "N batches: 25.0\n",
            " len_C1: 1\n",
            "N indexes: 10.0\n",
            "Ok.\n",
            "N. of test files used: 250\n",
            "DataSet Initialization\n",
            "Using z bins [0, 1, 2, 3]\n",
            "Normalisation file is /planck_ee2.txt\n",
            "Specified k_max is 2.5\n",
            "Corresponding i_max is 399\n",
            "Closest k to k_max is 2.504942\n",
            "Specified k_min is 0.0\n",
            "Corresponding i_min is 0\n",
            "Closest k to k_min is 0.01\n",
            "New data dim: (399, 1)\n",
            "Final i_max used is 399\n",
            "Final i_min used is 0\n",
            "one_vs_all: False\n",
            "dataset_balanced: False\n",
            "base_case_dataset: True\n",
            "N. classes: 6\n",
            "N. n_classes in output: 6\n",
            "LABELS: ['dgp', 'ds_binA', 'fr', 'lcdm', 'rand', 'wcdm']\n",
            "list_IDs length: 250\n",
            "n_indexes (n of file IDs read for each batch): 10\n",
            "batch size: 600\n",
            "n_batches : 25\n",
            "For each batch we read 10 file IDs\n",
            "For each file ID we have 6 labels\n",
            "For each ID, label we have 10 realizations of noise\n",
            "In total, for each batch we have 600 training examples\n",
            "Input batch size: 600\n",
            "N of batches to cover all file IDs: 25\n",
            "len(fname_list), batch_size, n_noisy_samples, n_batches: 1500, 600, 10, 25\n",
            "------------ DONE ------------\n",
            "\n",
            "Input shape (399, 4)\n",
            "------------ BUILDING MODEL ------------\n",
            "\n",
            "Model n_classes : 6 \n",
            "Features shape: (399, 4)\n",
            "Labels shape: (6,)\n",
            "using 1D layers and 4 channels\n",
            "/Users/arnav/.pyenv/versions/3.11.8/lib/python3.11/site-packages/tensorflow_probability/python/layers/util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "  loc = add_variable_fn(\n",
            "/Users/arnav/.pyenv/versions/3.11.8/lib/python3.11/site-packages/tensorflow_probability/python/layers/util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "  untransformed_scale = add_variable_fn(\n",
            "Expected output dimension after layer: conv1d_flipout : 97\n",
            "Expected output dimension after layer: conv1d_flipout_1 : 46\n",
            "Expected output dimension after layer: conv1d_flipout_2 : 45\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 399, 4)]          0         \n",
            "                                                                 \n",
            " conv1d_flipout (Conv1DFlip  (None, 195, 8)            648       \n",
            " out)                                                            \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 97, 8)             0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 97, 8)             32        \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv1d_flipout_1 (Conv1DFl  (None, 47, 16)            1296      \n",
            " ipout)                                                          \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 46, 16)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 46, 16)            64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_flipout_2 (Conv1DFl  (None, 45, 32)            2080      \n",
            " ipout)                                                          \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 45, 32)            128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 32)                0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense_flipout (DenseFlipou  (None, 32)                2080      \n",
            " t)                                                              \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 32)                128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_flipout_1 (DenseFlip  (None, 6)                 390       \n",
            " out)                                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6846 (26.74 KB)\n",
            "Trainable params: 6670 (26.05 KB)\n",
            "Non-trainable params: 176 (704.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Computing loss for randomly initialized model...\n",
            "Loss before loading weights/ 2.338429\n",
            "\n",
            "------------ RESTORING CHECKPOINT ------------\n",
            "\n",
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "Looking for ckpt in models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binB/tf_ckpts/\n",
            "Restoring checkpoint from models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binB/tf_ckpts/ckpt-39\n",
            "Loss after loading weights/ 1.3400866\n",
            "\n",
            "Threshold probability for classification: 0.5 \n",
            "Accuracy on 0 batch using median of sampled probabilities: 0.6716667 %\n",
            "Accuracy on 0 batch using median of sampled probabilities, not considering unclassified examples: 0.81578946 %\n",
            "Accuracy on 1 batch using median of sampled probabilities: 0.69 %\n",
            "Accuracy on 1 batch using median of sampled probabilities, not considering unclassified examples: 0.828 %\n",
            "Accuracy on 2 batch using median of sampled probabilities: 0.69166666 %\n",
            "Accuracy on 2 batch using median of sampled probabilities, not considering unclassified examples: 0.85040987 %\n",
            "Accuracy on 3 batch using median of sampled probabilities: 0.69 %\n",
            "Accuracy on 3 batch using median of sampled probabilities, not considering unclassified examples: 0.82965934 %\n",
            "Accuracy on 4 batch using median of sampled probabilities: 0.665 %\n",
            "Accuracy on 4 batch using median of sampled probabilities, not considering unclassified examples: 0.7995992 %\n",
            "Accuracy on 5 batch using median of sampled probabilities: 0.715 %\n",
            "Accuracy on 5 batch using median of sampled probabilities, not considering unclassified examples: 0.85971946 %\n",
            "Accuracy on 6 batch using median of sampled probabilities: 0.665 %\n",
            "Accuracy on 6 batch using median of sampled probabilities, not considering unclassified examples: 0.798 %\n",
            "Accuracy on 7 batch using median of sampled probabilities: 0.6666667 %\n",
            "Accuracy on 7 batch using median of sampled probabilities, not considering unclassified examples: 0.81466395 %\n",
            "Accuracy on 8 batch using median of sampled probabilities: 0.68 %\n",
            "Accuracy on 8 batch using median of sampled probabilities, not considering unclassified examples: 0.83950615 %\n",
            "Accuracy on 9 batch using median of sampled probabilities: 0.64666665 %\n",
            "Accuracy on 9 batch using median of sampled probabilities, not considering unclassified examples: 0.78701824 %\n",
            "Accuracy on 10 batch using median of sampled probabilities: 0.66 %\n",
            "Accuracy on 10 batch using median of sampled probabilities, not considering unclassified examples: 0.79358715 %\n",
            "Accuracy on 11 batch using median of sampled probabilities: 0.6483333 %\n",
            "Accuracy on 11 batch using median of sampled probabilities, not considering unclassified examples: 0.7842742 %\n",
            "Accuracy on 12 batch using median of sampled probabilities: 0.70666665 %\n",
            "Accuracy on 12 batch using median of sampled probabilities, not considering unclassified examples: 0.83960396 %\n",
            "Accuracy on 13 batch using median of sampled probabilities: 0.70166665 %\n",
            "Accuracy on 13 batch using median of sampled probabilities, not considering unclassified examples: 0.8470825 %\n",
            "Accuracy on 14 batch using median of sampled probabilities: 0.67 %\n",
            "Accuracy on 14 batch using median of sampled probabilities, not considering unclassified examples: 0.8088531 %\n",
            "Accuracy on 15 batch using median of sampled probabilities: 0.6816667 %\n",
            "Accuracy on 15 batch using median of sampled probabilities, not considering unclassified examples: 0.8279352 %\n",
            "Accuracy on 16 batch using median of sampled probabilities: 0.65833336 %\n",
            "Accuracy on 16 batch using median of sampled probabilities, not considering unclassified examples: 0.7915832 %\n",
            "Accuracy on 17 batch using median of sampled probabilities: 0.7183333 %\n",
            "Accuracy on 17 batch using median of sampled probabilities, not considering unclassified examples: 0.8368932 %\n",
            "Accuracy on 18 batch using median of sampled probabilities: 0.69166666 %\n",
            "Accuracy on 18 batch using median of sampled probabilities, not considering unclassified examples: 0.841785 %\n",
            "Accuracy on 19 batch using median of sampled probabilities: 0.69166666 %\n",
            "Accuracy on 19 batch using median of sampled probabilities, not considering unclassified examples: 0.8434959 %\n",
            "Accuracy on 20 batch using median of sampled probabilities: 0.65 %\n",
            "Accuracy on 20 batch using median of sampled probabilities, not considering unclassified examples: 0.7815631 %\n",
            "Accuracy on 21 batch using median of sampled probabilities: 0.69 %\n",
            "Accuracy on 21 batch using median of sampled probabilities, not considering unclassified examples: 0.80859375 %\n",
            "Accuracy on 22 batch using median of sampled probabilities: 0.6533333 %\n",
            "Accuracy on 22 batch using median of sampled probabilities, not considering unclassified examples: 0.8270042 %\n",
            "Accuracy on 23 batch using median of sampled probabilities: 0.69166666 %\n",
            "Accuracy on 23 batch using median of sampled probabilities, not considering unclassified examples: 0.8234127 %\n",
            "Accuracy on 24 batch using median of sampled probabilities: 0.65833336 %\n",
            "Accuracy on 24 batch using median of sampled probabilities, not considering unclassified examples: 0.8127572 %\n",
            "-- Accuracy on test set using median of sampled probabilities: 0.6781333 % \n",
            "\n",
            "-- Accuracy on test set using median of sampled probabilities, not considering unclassified examples: 0.8196315 % \n",
            "\n",
            "Adding Not classified label\n",
            "Saved confusion matrix at models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binB/cm_confusion_frozen_weights.pdf\n",
            "Saved confusion matrix values at models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binB/cm_confusion_frozen_weights_values.txt\n"
          ]
        }
      ],
      "source": [
        "!python test.py --log_path='models/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binB/sixlabel_5k_EE2_equalexamples-randoms_kmin-0.0_kmax-2.5_ds_binB_log.txt' \\\n",
        "  --TEST_DIR='data/test' --cm_name_custom='confusion'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ux1dgMxJXn"
      },
      "source": [
        "## Load modules for classification of a single spectrum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYGq6yP8Nvoq"
      },
      "source": [
        "### Functions to load trained models and data\n",
        "just execute the following cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gazkA01-CMCj"
      },
      "outputs": [],
      "source": [
        "def read_pre_trained_flags(args_dict):\n",
        "  '''\n",
        "  Read model options from logfile and return them in the form of FLAGS (the corresponding class is in utils)\n",
        "  '''\n",
        "  args = DummyFlags(args_dict)\n",
        "  FLAGS = get_flags(args.log_path)\n",
        "  if args.models_dir is not None:\n",
        "        print('Reading model from the directory %s' %args.models_dir)\n",
        "        FLAGS.models_dir = args.models_dir\n",
        "  return FLAGS\n",
        "\n",
        "\n",
        "def get_pre_trained_model(fname, n_classes, input_shape):\n",
        "  '''\n",
        "  returns pre-trained model.\n",
        "  Input:  - fname: string, folder where the trained model is stored\n",
        "          - n_classes: integer, number of output classes of the model\n",
        "          - input_shape:  tuple, input dimensions of the model.\n",
        "                          For pre-trained model in the paper use (100, 4)\n",
        "\n",
        "  '''\n",
        "\n",
        "  log_path = os.path.join('models', fname, fname+'_log.txt' )\n",
        "  models_dir = os.path.join('models', fname)\n",
        "\n",
        "  args_dict= {'log_path':log_path, 'models_dir': models_dir, }\n",
        "  FLAGS=read_pre_trained_flags(args_dict)\n",
        "\n",
        "  print('Input shape %s' %str(input_shape))\n",
        "\n",
        "\n",
        "  model_loaded =  load_model_for_test(FLAGS, input_shape, n_classes=n_classes,\n",
        "                                        generator=None, new_fname='')\n",
        "\n",
        "\n",
        "  return model_loaded\n",
        "\n",
        "\n",
        "\n",
        "def load_X(fname, sample_pace, i_max, norm_data,\n",
        "           add_noise=True, normalise=True, use_sample_pace=False, include_k=True):\n",
        "  '''\n",
        "  Generates one power spectrum X ready for classification.\n",
        "  Options:  fname: path to data\n",
        "            sample_pace: read one point every sample_pace. Useful to reduce data dimension\n",
        "            i_max: Index corresponding to the max k used\n",
        "            norm_data: normalization spectrum\n",
        "\n",
        "            add_noise: whether to add noise or not\n",
        "            normalise: normalise by Planck or not\n",
        "            use_sample_pace\n",
        "            include_k: return k, X or only X\n",
        "\n",
        "\n",
        "\n",
        "  '''\n",
        "\n",
        "  all = np.loadtxt(fname)\n",
        "  print('loaded data shape: %s' %str(all.shape))\n",
        "  if include_k:\n",
        "    k, X = all[:, 0], all[:, 1:]\n",
        "    print('k, X  shape: %s, %s' %(str(k.shape), str(X.shape)))\n",
        "  else:\n",
        "    X = all\n",
        "    k=np.arange(X.shape[0])\n",
        "    print('X  shape : %s' %( str(X.shape)))\n",
        "  if use_sample_pace:\n",
        "    k, X = k[::sample_pace], X[::sample_pace]\n",
        "  print('X  shape after sample pace:  %s' %( str(X.shape)))\n",
        "  k, X  = k[:i_max], X[:i_max]\n",
        "  print('X  shape after k max: %s' %( str(X.shape)))\n",
        "  if add_noise:\n",
        "    noise = np.random.normal(loc=0, scale=generate_noise(k,X, add_sys=True,add_shot=True,sigma_sys=5 ))\n",
        "    X = X+noise\n",
        "  if normalise:\n",
        "    planck = norm_data\n",
        "    print('planck data shape: %s' %str(planck.shape))\n",
        "    X = X/planck-1\n",
        "  if include_k:\n",
        "    return k, X\n",
        "  else:\n",
        "    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho00eqUjDVPp"
      },
      "source": [
        "## Code for getting confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ciV9qDIDXY7"
      },
      "outputs": [],
      "source": [
        "def sample_from_data(model,  my_Xs_norm,  my_min=np.zeros(5), my_max=np.ones(5),\n",
        "                     n_samples=50000, num_monte_carlo=100, th_prob=0.5,\n",
        "                     verbose=True):\n",
        "  '''\n",
        "  Given some normalized features my_Xs_norm and the model, compute mu and Sigma as in the paper eq. 2.8-2.9\n",
        "  and return samples from the probability distribuion eq. 2.10\n",
        "  '''\n",
        "  Al_error, Ep_error, mean = get_mu_Sigma(model, my_Xs_norm , num_monte_carlo=num_monte_carlo, th_prob=th_prob, verbose=verbose)\n",
        "  Sigma = (Al_error+ Ep_error).numpy()\n",
        "  MM = mean.numpy()\n",
        "  if verbose:\n",
        "    print('Sigma')\n",
        "    print(np.round(Sigma, 3))\n",
        "  samples, z_samples = sample_probas(Sigma, MM, my_min=my_min,my_max=my_max, n_samples=n_samples, verbose=verbose)\n",
        "\n",
        "  return samples, z_samples, Sigma, MM\n",
        "\n",
        "\n",
        "\n",
        "def get_mu_Sigma(model, my_Xs_norm, num_monte_carlo=100, th_prob=0.5, verbose=True):\n",
        "  ''''\n",
        "  Given some normalized features my_Xs_norm and the model, compute mu and Sigma as in the paper eq. 2.8-2.9\n",
        "  '''\n",
        "  if verbose:\n",
        "    print('Computing mu and sigma...')\n",
        "  sampled_probas, _, _ = my_predict(tf.expand_dims(my_Xs_norm, axis=0), model, num_monte_carlo=num_monte_carlo, th_prob=th_prob)\n",
        "  mean = tf.reduce_mean(sampled_probas[:,0,:], axis=0)\n",
        "  _, Al_error, Ep_error = get_err_variances(sampled_probas[:,0,:])\n",
        "  return Al_error, Ep_error, mean\n",
        "\n",
        "\n",
        "def sample_probas(Sigma, MM, my_min=np.zeros(5), my_max=np.ones(5), n_samples=50000, verbose=True):\n",
        "  '''\n",
        "  Given mu and Sigma as in the paper eq. 2.8-2.9, computes B, U as described in appendix B of the paper.\n",
        "  Then calls function to sample the prob. distribution (eq. 2.10)\n",
        "  '''\n",
        "  if verbose:\n",
        "    print('Sample probas call')\n",
        "  evals, evecs = np.linalg.eig(Sigma)\n",
        "  B = evecs\n",
        "  U = B.T @ Sigma @  B\n",
        "  if verbose:\n",
        "    print('U')\n",
        "    print(np.round(U, 3))\n",
        "  n_dims=U.shape[0]-1\n",
        "  n_dims=U.shape[0]-1\n",
        "  ind_min = np.argmin(np.diag(U))\n",
        "  if verbose:\n",
        "    print('Index of min eigenvalue is: %s' %ind_min)\n",
        "    print('Min eigenvalue is: %s' %np.min(np.diag(U)))\n",
        "  UU = np.array([np.sqrt(U[i,i])  for i in range(U.shape[0]) if i!=ind_min ] )\n",
        "  if verbose:\n",
        "    print('U has %s elements ' %str(UU.shape[0]))\n",
        "  samples, z_samples = get_samples(UU, B, MM, my_min=my_min, my_max=my_max, ind_min=ind_min, n_dims=n_dims, n_samples=n_samples, verbose=verbose)\n",
        "  return samples, z_samples\n",
        "\n",
        "\n",
        "def get_samples(UU, B, MM, my_min=np.zeros(5), my_max=np.ones(5), ind_min=-1, n_dims=4, n_samples=50000, verbose=True):\n",
        "  '''\n",
        "  samples eq. 2.10 as described in appendix B\n",
        "  '''\n",
        "  if verbose:\n",
        "    print('Sampling...')\n",
        "  samples = np.zeros((0, n_dims+1))\n",
        "  z_samples = np.zeros((0, n_dims))\n",
        "  while samples.shape[0] < n_samples:\n",
        "    s = np.random.multivariate_normal( np.zeros(n_dims), np.diag(UU**2), size=(n_samples,))\n",
        "    accepted = s[(np.min(X_val_batch(s, B, MM, null_ind=ind_min)-my_min, axis=1) >= 0) & (np.max(X_val_batch(s, B, MM, null_ind=ind_min) - my_max, axis=1) <= 0)]\n",
        "    samples = np.concatenate((samples, X_val_batch(accepted, B, MM, null_ind=ind_min)), axis=0)\n",
        "    z_samples = np.concatenate((z_samples, accepted), axis=0)\n",
        "  samples = samples[:n_samples, :]\n",
        "  z_samples = z_samples[:n_samples, :]\n",
        "  if verbose:\n",
        "    print('Done.')\n",
        "  return samples, z_samples\n",
        "\n",
        "\n",
        "\n",
        "def X_val_batch(Z, B, MM, null_ind):\n",
        "  return np.array([ X_val(Z[i], B, MM, null_ind=null_ind,) for i in range(Z.shape[0])])\n",
        "\n",
        "def Z(X, B, MM):\n",
        "  '''  X must be 5-d '''\n",
        "  return B.T @ (X-MM)\n",
        "\n",
        "def X_val(Z, B, MM, null_ind=-1, verbose=False):\n",
        "  '''  Z must be 4-d , MM must be 5-d '''\n",
        "  if verbose:\n",
        "    print(Z)\n",
        "  Z = np.insert(Z, null_ind, 0)\n",
        "  if verbose:\n",
        "    print(Z)\n",
        "  return B @ Z+ MM\n",
        "\n",
        "\n",
        "def get_err_variances(prob_k, AlEp_split=True):\n",
        "  '''\n",
        "  Computes aleatoric and epistemic uncertainty from MC samples from the net. weights\n",
        "  '''\n",
        "  num_samples = prob_k.shape[0]\n",
        "  prob_mean = tf.reduce_mean(prob_k, axis=0)\n",
        "\n",
        "  diag_probs = tf.stack([np.diag(prob_k[i,:]) for i in range(num_samples)], axis=0)  # Diagonal matrix whose diagonal elements are the output of the network for the ith sample\n",
        "  outer_products_Al = tf.stack([tf.tensordot(prob_k[i,:], prob_k[i,:], axes =0) for i in range(num_samples)], axis=0) # Outer self-product of the predictive vectors for the ith sample used in computing Al uncertainty\n",
        "  outer_products_Ep = tf.stack([tf.tensordot(prob_k[i,:] - prob_mean, prob_k[i,:] - prob_mean, axes =0) for i in range(num_samples)], axis=0) # Outer self-product between the difference of the predictive vector for ith sampl and average prediction\n",
        "\n",
        "  Al_error = tf.reduce_mean(diag_probs - outer_products_Al, axis=0) # Average over all the samples to compute aleotoric uncertainty\n",
        "  Ep_error = tf.reduce_mean(outer_products_Ep, axis=0) # Average over all the samples to compute epistemic uncertainty\n",
        "\n",
        "  tot_error = Al_error + Ep_error # Combine to compute total uncertainty\n",
        "  confidences = tf.linalg.diag_part(tot_error).numpy()\n",
        "\n",
        "  if AlEp_split:\n",
        "    return confidences, Al_error, Ep_error\n",
        "  else:\n",
        "    return confidences\n",
        "\n",
        "\n",
        "\n",
        "def get_P_from_samples(samples, th_prob=0.5 ):\n",
        "  '''\n",
        "  Given samples from eq. 2.10, computes the probabilities as in eq. 2.11\n",
        "  '''\n",
        "  Ntot = samples.shape[0] #-unclassified\n",
        "  n_dims  = samples.shape[-1]\n",
        "  all_preds = np.array([predict_bayes_label(sample, th_prob=th_prob) for sample in samples])\n",
        "  unclassified=all_preds[all_preds==99].shape[0]\n",
        "\n",
        "  n_in_class = np.array([all_preds[all_preds==k].shape[0] for k in range(n_dims)])\n",
        "\n",
        "  #print('Ntot: %s  ' %Ntot)\n",
        "  p_uncl=unclassified/Ntot\n",
        "  #print('%s unclassified examples ' %unclassified)\n",
        "  #print('P(unclassified)=%s ' %p_uncl)\n",
        "  pp=n_in_class/Ntot\n",
        "  #print(pp)\n",
        "  #print(p_uncl)\n",
        "\n",
        "  return np.append(pp, p_uncl)\n",
        "\n",
        "\n",
        "def get_all_probas(X, model_loaded,\n",
        "                   n_classes=5,\n",
        "                   num_monte_carlo=100,\n",
        "                   th_prob=0.5,\n",
        "                   n_samples=10000,\n",
        "                   verbose=True):\n",
        "    '''\n",
        "    Given features X, predicts the labels with MC samples from the net,\n",
        "    computes mu and sigma, samples from the prob distribution\n",
        "    and computes P.\n",
        "    Returns result in the form of a dictionary\n",
        "    '''\n",
        "\n",
        "    # Gaussian approx\n",
        "    samples, z_samples, Sigma, MM = sample_from_data(model_loaded,\n",
        "                                                 my_Xs_norm=X,\n",
        "                                                 my_min=np.zeros(n_classes), my_max=np.ones(n_classes),\n",
        "                                                 n_samples=n_samples,\n",
        "                                                 num_monte_carlo=num_monte_carlo,\n",
        "                                                 th_prob=th_prob, verbose=verbose)\n",
        "\n",
        "    P = get_P_from_samples(samples, th_prob=th_prob)\n",
        "    if verbose:\n",
        "      print('P: %s ' %str(P))\n",
        "      print('P sum to %s' %P.sum())\n",
        "    if verbose:\n",
        "      print('mu  sum to %s' %MM.sum())\n",
        "\n",
        "\n",
        "    # Result\n",
        "    res = {'samples':samples, 'P':P, 'z_samples': z_samples, 'Sigma': Sigma, 'MM': MM,}\n",
        "    return res\n",
        "\n",
        "\n",
        "def plot_hist_1D(samples, MM, th_prob=0.5,\n",
        "                  P=None, inv_labels_dict=None,):\n",
        "  '''\n",
        "  Plots histogram of gauss samples for each class\n",
        "  '''\n",
        "\n",
        "  n_dims = samples.shape[-1]-1\n",
        "  fig, axs = plt.subplots(1, n_dims+1, sharey=True, sharex=False,figsize=(25,5))\n",
        "\n",
        "  t_str=''\n",
        "  for k in range(n_dims+1):\n",
        "\n",
        "    axs[k].hist(samples.T[k], bins=15, color = \"lightgray\",lw=0, density=True, label='Gauss samples')\n",
        "    axs[k].text(0.1, 0.9, '$\\mu = $' + '$'+ str(MM[k])+'$', fontsize = 16 , transform=axs[k].transAxes);\n",
        "    if P is  not None:\n",
        "      axs[k].text(0.1, 0.8, '$ P = $' + '$'+ str(P[k])+'$', fontsize = 16 , transform=axs[k].transAxes);\n",
        "      if k==0:\n",
        "        t_str+= 'P unclassified: %s \\n' %str(np.round(P[-1], 2) )\n",
        "    axs[k].set_xlim(0,1)\n",
        "    axs[k].set_ylim(0,10)\n",
        "\n",
        "    if inv_labels_dict is not None:\n",
        "      axs[k].set_title(inv_labels_dict[k])\n",
        "\n",
        "\n",
        "    axs[k].legend(loc='lower right')\n",
        "\n",
        "  axs[0].set_ylabel('P')\n",
        "  fig.suptitle(t_str)\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKo2vnj_Nzo0"
      },
      "source": [
        "## Load models\n",
        "\n",
        "Here we load the pre trained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YdVmL6YItLH"
      },
      "outputs": [],
      "source": [
        "# Encoding of the labels for the two networks. Can be found in the log file of the training.\n",
        "# Here it is easier to define it by hand.\n",
        "\n",
        "inv_labels_dict_6={'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}\n",
        "inv_labels_dict_2={0:'lcdm', 1:'non-LCDM'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9pyiC969KoE"
      },
      "source": [
        "change the model name (first parameter) in the following function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHuW_fpxJmvc"
      },
      "outputs": [],
      "source": [
        "model_6 = get_pre_trained_model('ds_ee2_1k_equalweight_randoms_kmax_5', 6, (100, 4) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WV6117ZOqdF"
      },
      "source": [
        "## Loading a trained network and applying it to a single file\n",
        "\n",
        "Template for loading spectra. Suppose you saved the spectrum in\n",
        "'data/example_spectra/my_ex.txt' . The spectrum is in 4 redshift bins and with 100 points between 0.01 - 2.5 in k . We normalize by the reference planck spectrum and add gaussian noise, then pass it to the five-label network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I0W-Yd0YERw"
      },
      "source": [
        "Load normalization data first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FirJ3GM5YDgl"
      },
      "outputs": [],
      "source": [
        "sample_pace=4 # the planck data were generated with 500 points up to k=10 . We need 1 every 4 points, and to cut to k_max=5.0\n",
        "i_max=200 #for kmax = 5\n",
        "\n",
        "planck = np.loadtxt('data/normalisation/planck_ee2.txt')[:, 1:][::sample_pace][:i_max]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S0HfNLeLsiv"
      },
      "outputs": [],
      "source": [
        "k, X_ds = load_X('data/ds_ee2_test_1k/ds/976.txt',\n",
        "                 sample_pace=sample_pace, i_max=200,\n",
        "                 norm_data=planck,\n",
        "                 add_noise=False, normalise=True, use_sample_pace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_01rSvQTjrT"
      },
      "source": [
        "Let's look at the input features as seen by the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhtjvSiCTiux"
      },
      "outputs": [],
      "source": [
        "plt.plot(k, X_ds);\n",
        "plt.xscale('log');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZgUlHtZrvG7"
      },
      "source": [
        "## Classify your spectra and compute confidence\n",
        "### (for Bayesian networks only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKNL6te3ruT0"
      },
      "outputs": [],
      "source": [
        "res_ds = get_all_probas(X_ds,\n",
        "                        model_5,\n",
        "                        n_classes=6,\n",
        "                        num_monte_carlo=10,\n",
        "                        th_prob=0.5,\n",
        "                        n_samples=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVXx5ejLsG-i"
      },
      "outputs": [],
      "source": [
        "print('P-non-LCDM=%s' %(np.delete(res_ds['P'], [2]).sum()))\n",
        "plot_hist_1D(res_ds['samples'],\n",
        "             res_ds['MM'],\n",
        "             th_prob=0.5,\n",
        "             P=res_ds['P'],\n",
        "             inv_labels_dict=inv_labels_dict_5)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}