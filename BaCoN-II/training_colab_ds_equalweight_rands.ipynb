{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2-LDl_8Nai6"
      },
      "source": [
        "# Classification of power spectra with BaCoN\n",
        "\n",
        "Arnav Agarwal\\\n",
        "S2163065\n",
        "\n",
        "Working on the inclusion of the dark scattering theory into the existing BaCoN-II 5-label model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flvg9eICNhqn"
      },
      "source": [
        "## Load the code\n",
        "\n",
        "Here we import the github repository and import BaCoN.\n",
        "\n",
        "Tensorflow2 and Tensorflow2 Probability should be installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzX4nFm1BQqd",
        "outputId": "af29b149-21db-4000-dab8-16fe588ad9bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SeniorHonoursProject'...\n",
            "remote: Enumerating objects: 215389, done.\u001b[K\n",
            "remote: Counting objects: 100% (2179/2179), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2112/2112), done.\u001b[K\n",
            "remote: Total 215389 (delta 93), reused 2151 (delta 67), pack-reused 213210\u001b[K\n",
            "Receiving objects: 100% (215389/215389), 2.52 GiB | 23.70 MiB/s, done.\n",
            "Resolving deltas: 100% (224/224), done.\n",
            "Updating files: 100% (24597/24597), done.\n",
            "/content/SeniorHonoursProject/BaCoN-II\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/arnava13/SeniorHonoursProject\n",
        "%cd SeniorHonoursProject/BaCoN-II"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVBtCPaWw60R"
      },
      "source": [
        "## Run training in shell\n",
        "only do this when not loading trained network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HamIuy_Qx2cf",
        "outputId": "3ce4c474-cade-48c8-ab6e-e32fb59d1e88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for interlap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for luigi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "lib5c version 0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -q lib5c\n",
        "!lib5c -v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gv8_CyoHB0_Y"
      },
      "outputs": [],
      "source": [
        "import os;\n",
        "import subprocess\n",
        "import threading\n",
        "import argparse\n",
        "from test import *\n",
        "from utils import *\n",
        "from models import *\n",
        "from data_generator import *\n",
        "from train import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ex2LIYxl6cjV"
      },
      "outputs": [],
      "source": [
        "from importer import load_model_for_test, my_predict, predict_bayes_label, predict_mean_proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Oaw1mzZqTsnu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"font.family\"] = 'serif'\n",
        "plt.rcParams[\"mathtext.fontset\"] = \"cm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZiOh8u5QowF",
        "outputId": "b7fed237-cbc9-47d9-baee-a8038c65242c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcurve_files_sys\u001b[0m/         \u001b[01;34mds_ee2_test_1k\u001b[0m/    \u001b[01;34mnormalisation\u001b[0m/\n",
            "delete_files_to_1000.py  \u001b[01;34mds_ee2_train_100\u001b[0m/  \u001b[01;34mtest_5_classes_1k\u001b[0m/\n",
            "delete_files_to_100.py   \u001b[01;34mds_ee2_train_1k\u001b[0m/   \u001b[01;34mtrain_5_classes_1k\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXuIOzFXFQL9"
      },
      "source": [
        "### set parameters for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "w8B1FUswFQL9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Training options:\n",
        "\n",
        "# ------------------ change the training directory and model name here ------------------\n",
        "\n",
        "# training data directory path\n",
        "DIR='data/ds_ee2_train_1k'\n",
        "## (batch size, is a multiple of #classes * noise realisations, e.g. 5 classes, 10 noise --> must be multiple of 50)\n",
        "batch_size='3000'\n",
        "fname = 'ds_ee2_1k_equalweight_randoms_kmax_5'\n",
        "\n",
        "# training the network\n",
        "GPU='True' # Use GPUs in training?\n",
        "# which kind of model\n",
        "bayesian='True' # Bayesian NN or traditional NN (weights single valued or distributions?)\n",
        "n_epochs='50' # How many epochs to train for?\n",
        "\n",
        "\n",
        "# ------------------ ignore all the other parameters for now ------------------\n",
        "\n",
        "# Directories\n",
        "mypath=None # Parent directory\n",
        "\n",
        "# Edit this with base model directory\n",
        "mdir='models/'\n",
        "\n",
        "# normalisation file\n",
        "norm_data_name = '/planck_ee2.txt'\n",
        "\n",
        "# scale cut and resolution\n",
        "k_max='5.0' # which k-modes to include?\n",
        "sample_pace='4'\n",
        "\n",
        "# should the processed curves be saved in an extra file (False recommended, only efficient for n_epochs = 1)\n",
        "save_processed_spectra='False'\n",
        "\n",
        "# -------------------- noise model --------------------\n",
        "\n",
        "# noise model - Cosmic variance and shot noise are Euclid-like\n",
        "n_noisy_samples='10' # How many noise examples to train on?\n",
        "add_noise='True'# Add noise?\n",
        "add_shot='False'# Add shot noise term? -- default should be False\n",
        "add_sys='True'# Add systematic error term?\n",
        "add_cosvar='True'# Add cosmic variance term?\n",
        "\n",
        "# path to folder with theory error curves\n",
        "curves_folder = 'data/curve_files_sys/theory_error'; sigma_curves_default = '0.05'\n",
        "# change rescale distribution of theory error curves, uniform recommended\n",
        "rescale_curves = 'uniform' # or gaussian or None\n",
        "\n",
        "# sys error curves - relative scale of curve-amplitude\n",
        "sigma_curves='0.05'\n",
        "\n",
        "\n",
        "\n",
        "# -------------------- additional training settings --------------------\n",
        "\n",
        "\n",
        "# Type of model\n",
        "model_name='custom' # Custom or dummy - dummy has presets as given in models.py\n",
        "\n",
        "# fine-tune = only 'LCDM' and 'non-LCDM'\n",
        "fine_tune='False' # Model trained to distinguish between 'LCDM' and 'non-LCDM' classes or between all clases. i.e. binary or multi-classifier\n",
        "\n",
        "#Test mode?\n",
        "test_mode='False' # Network trained on 1 batch of minimal size or not?\n",
        "seed='1312' # Initial seed for test mode batch\n",
        "# Saving or restoring training\n",
        "restore='False' # Restore training from a checkpoint?\n",
        "save_ckpt='True' # Save checkpoints?\n",
        "\n",
        "\n",
        "val_size='0.15' # Validation set % of data set\n",
        "\n",
        "add_FT_dense='False' #if True, adds an additional dense layer before the final 2D one\n",
        "\n",
        "patience='50' # terminate training after 'patience' epochs if no decrease in loss function\n",
        "lr='0.01' # learning rate\n",
        "decay='0.95' #decay rate: If None : Adam(lr),\n",
        "\n",
        "\n",
        "\n",
        "# -------------------- BNN parameters -----------------\n",
        "\n",
        "# Example image details\n",
        "im_depth='500' # Number in z direction (e.g. 500 wave modes for P(k) examples)\n",
        "im_width='1' # Number in y direction (1 is a 2D image : P(k,mu))\n",
        "im_channels='4'  # Number in x direction (e.g. 4 redshifts for P(k,z))\n",
        "swap_axes='True' # Do we swap depth and width axes? True if we have 1D image in 4 channels\n",
        "sort_labels='True' # Sorts labels in ascending/alphabetical order\n",
        "\n",
        "z1='0' # which z-bins to include? Assumes each channel is a new z bin.\n",
        "z2='1' # which z-bins to include? Assumes each channel is a new z bin.\n",
        "z3='2' # which z-bins to include? Assumes each channel is a new z bin.\n",
        "z4='3' # which z-bins to include? Assumes each channel is a new z bin.\n",
        "\n",
        "# Number of layers and kernel sizes\n",
        "k1='10'\n",
        "k2='5'\n",
        "k3='2'\n",
        " # The dimensionality of the output space (i.e. the number of filters in the convolution)\n",
        "f1='8'\n",
        "f2='16'\n",
        "f3='32'\n",
        " # Stride of each layer's kernel\n",
        "s1='2'\n",
        "s2='2'\n",
        "s3='1'\n",
        "# Pooling layer sizes\n",
        "p1='2'\n",
        "p2='2'\n",
        "p3='0'\n",
        "# Strides in Pooling layer\n",
        "sp1='2'\n",
        "sp2='1'\n",
        "sp3='0'\n",
        "\n",
        "n_dense='1' # Number of dense layers\n",
        "\n",
        "# labels of different cosmologies\n",
        "#c0_label = 'lcdm'\n",
        "#c1_label = 'fR dgp wcdm'\n",
        "\n",
        "log_path = mdir + fname + '_log'\n",
        "\n",
        "if fine_tune == \"True\":\n",
        "    log_path += '_FT'\n",
        "\n",
        "log_path += '.txt'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subprocess output management function"
      ],
      "metadata": {
        "id": "xJEzUe5_Op3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_output(stream, prefix, log_file):\n",
        "    for line in iter(stream.readline, b''):  # This iterates over the output line by line\n",
        "        my_bytes = line.strip()\n",
        "        if my_bytes:\n",
        "            output = f\"{prefix}: {my_bytes.decode('utf-8')}\"\n",
        "            print(output)\n",
        "            log_file.write(output + \"\\n\")"
      ],
      "metadata": {
        "id": "getcbGEOOtb2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YN5PCcWXWgr"
      },
      "source": [
        "### run the training\n",
        "this may take a while to run, also expect some (harmless) error messages in the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdSzp6AFFQL-",
        "outputId": "7a048f84-16de-4f47-eda1-596fd2ab0565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "err: 2024-02-21 15:53:55.831068: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "err: 2024-02-21 15:53:55.831137: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "err: 2024-02-21 15:53:55.832377: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "err: 2024-02-21 15:53:56.808944: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "err: /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "err: loc = add_variable_fn(\n",
            "err: 2024-02-21 15:53:59.309683: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "err: /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "err: untransformed_scale = add_variable_fn(\n",
            "err: WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "err: I0000 00:00:1708530857.187287   18048 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "out: Directory models/ds_ee2_1k_equalweight_randoms_kmax_5 not created\n",
            "out: Logger creating log file: models/ds_ee2_1k_equalweight_randoms_kmax_5/ds_ee2_1k_equalweight_randoms_kmax_5_log.txt\n",
            "out: -------- Parameters:\n",
            "out: bayesian True\n",
            "out: test_mode False\n",
            "out: n_test_idx 2\n",
            "out: seed 1312\n",
            "out: fine_tune False\n",
            "out: one_vs_all False\n",
            "out: c_0 ['lcdm']\n",
            "out: c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']\n",
            "out: dataset_balanced False\n",
            "out: include_last False\n",
            "out: log_path models/ds_ee2_1k_equalweight_randoms_kmax_5_log.txt\n",
            "out: restore False\n",
            "out: fname ds_ee2_1k_equalweight_randoms_kmax_5\n",
            "out: model_name custom\n",
            "out: my_path None\n",
            "out: DIR data/ds_ee2_train_1k\n",
            "out: TEST_DIR data/test_data/\n",
            "out: models_dir models/\n",
            "out: save_ckpt True\n",
            "out: out_path_overwrite False\n",
            "out: curves_folder data/curve_files_sys/theory_error\n",
            "out: save_processed_spectra False\n",
            "out: im_depth 500\n",
            "out: im_width 1\n",
            "out: im_channels 4\n",
            "out: swap_axes True\n",
            "out: sort_labels True\n",
            "out: norm_data_name /planck_ee2.txt\n",
            "out: normalization stdcosmo\n",
            "out: sample_pace 4\n",
            "out: k_max 5.0\n",
            "out: i_max None\n",
            "out: add_noise True\n",
            "out: n_noisy_samples 10\n",
            "out: add_shot False\n",
            "out: add_sys True\n",
            "out: add_cosvar True\n",
            "out: sigma_sys None\n",
            "out: sys_scaled None\n",
            "out: sys_factor None\n",
            "out: sys_max None\n",
            "out: sigma_curves 0.05\n",
            "out: sigma_curves_default 0.05\n",
            "out: rescale_curves uniform\n",
            "out: z_bins [0, 1, 2, 3]\n",
            "out: n_dense 1\n",
            "out: filters [8, 16, 32]\n",
            "out: kernel_sizes [10, 5, 2]\n",
            "out: strides [2, 2, 1]\n",
            "out: pool_sizes [2, 2, 0]\n",
            "out: strides_pooling [2, 1, 0]\n",
            "out: add_FT_dense False\n",
            "out: trainable False\n",
            "out: unfreeze False\n",
            "out: lr 0.01\n",
            "out: drop 0.5\n",
            "out: n_epochs 50\n",
            "out: val_size 0.15\n",
            "out: test_size 0.0\n",
            "out: batch_size 3000\n",
            "out: patience 50\n",
            "out: GPU True\n",
            "out: decay 0.95\n",
            "out: BatchNorm True\n",
            "out: ------------ CREATING DATA GENERATORS ------------\n",
            "out: labels : ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']\n",
            "out: Labels encoding:\n",
            "out: {'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}\n",
            "out: n_labels : 6\n",
            "out: dgp - 1000 training examples\n",
            "out: ds - 1000 training examples\n",
            "out: fr - 1000 training examples\n",
            "out: lcdm - 1000 training examples\n",
            "out: rand - 1000 training examples\n",
            "out: wcdm - 1000 training examples\n",
            "out: N. of data files: 1000\n",
            "out: get_all_indexes labels dict: {'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}\n",
            "out: create_generators n_labels: 6\n",
            "out: create_generators n_labels_eff: 6\n",
            "out: create_generators len_c1: 1\n",
            "out: Check for no duplicates in test: (0=ok):\n",
            "out: 0.0\n",
            "out: Check for no duplicates in val: (0=ok):\n",
            "out: 0\n",
            "out: N of files in training set: 850\n",
            "out: N of files in validation set: 150\n",
            "out: N of files in test set: 0\n",
            "out: Check - total: 1000\n",
            "out: --create_generators, train indexes\n",
            "out: batch_size: 3000\n",
            "out: - Cut sample\n",
            "out: bs: 3000\n",
            "out: N_labels: 6\n",
            "out: N_noise: 10\n",
            "out: len_c1: 1\n",
            "out: Train index length: 850\n",
            "out: --create_generators, validation indexes\n",
            "out: - Cut sample\n",
            "out: bs: 3000\n",
            "out: N_labels: 6\n",
            "out: N_noise: 10\n",
            "out: len_c1: 1\n",
            "out: Val index length: 150\n",
            "out: len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 850, 3000, 6, 10\n",
            "out: --DataGenerator Train\n",
            "out: Data Generator Initialization\n",
            "out: Using z bins [0, 1, 2, 3]\n",
            "out: Normalisation file is /planck_ee2.txt\n",
            "out: Specified k_max is 5.0\n",
            "out: Corresponding i_max is 112\n",
            "out: Closest k to k_max is 4.936132\n",
            "out: New data dim: (112, 1)\n",
            "out: Final i_max used is 112\n",
            "out: one_vs_all: False\n",
            "out: dataset_balanced: False\n",
            "out: base_case_dataset: True\n",
            "out: N. classes: 6\n",
            "out: N. n_classes in output: 6\n",
            "out: LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']\n",
            "out: list_IDs length: 850\n",
            "out: n_indexes (n of file IDs read for each batch): 50\n",
            "out: batch size: 3000\n",
            "out: n_batches : 17\n",
            "out: For each batch we read 50 file IDs\n",
            "out: For each file ID we have 6 labels\n",
            "out: For each ID, label we have 10 realizations of noise\n",
            "out: In total, for each batch we have 3000 training examples\n",
            "out: Input batch size: 3000\n",
            "out: N of batches to cover all file IDs: 17\n",
            "out: --DataGenerator Validation\n",
            "out: Data Generator Initialization\n",
            "out: Using z bins [0, 1, 2, 3]\n",
            "out: Normalisation file is /planck_ee2.txt\n",
            "out: Specified k_max is 5.0\n",
            "out: Corresponding i_max is 112\n",
            "out: Closest k to k_max is 4.936132\n",
            "out: New data dim: (112, 1)\n",
            "out: Final i_max used is 112\n",
            "out: one_vs_all: False\n",
            "out: dataset_balanced: False\n",
            "out: base_case_dataset: True\n",
            "out: N. classes: 6\n",
            "out: N. n_classes in output: 6\n",
            "out: LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']\n",
            "out: list_IDs length: 150\n",
            "out: n_indexes (n of file IDs read for each batch): 50\n",
            "out: batch size: 3000\n",
            "out: n_batches : 3\n",
            "out: For each batch we read 50 file IDs\n",
            "out: For each file ID we have 6 labels\n",
            "out: For each ID, label we have 10 realizations of noise\n",
            "out: In total, for each batch we have 3000 training examples\n",
            "out: Input batch size: 3000\n",
            "out: N of batches to cover all file IDs: 3\n",
            "out: ------------ DONE ------------\n",
            "out: ------------ BUILDING MODEL ------------\n",
            "out: Input shape (112, 4)\n",
            "out: using 1D layers and 4 channels\n",
            "out: Expected output dimension of layer conv1d_flipout: 52.0\n",
            "out: Expected output dimension of layer max_pooling1d: 26.0\n",
            "out: Expected output dimension of layer conv1d_flipout_1: 11.5\n",
            "out: Expected output dimension of layer max_pooling1d_1: 10.5\n",
            "out: Expected output dimension of layer conv1d_flipout_2: 9.5\n",
            "out: Model: \"model\"\n",
            "out: _________________________________________________________________\n",
            "out: Layer (type)                Output Shape              Param #\n",
            "out: =================================================================\n",
            "out: input_1 (InputLayer)        [(None, 112, 4)]          0\n",
            "out: conv1d_flipout (Conv1DFlip  (None, 52, 8)             648\n",
            "out: out)\n",
            "out: max_pooling1d (MaxPooling1  (None, 26, 8)             0\n",
            "out: D)\n",
            "out: batch_normalization (Batch  (None, 26, 8)             32\n",
            "out: Normalization)\n",
            "out: conv1d_flipout_1 (Conv1DFl  (None, 11, 16)            1296\n",
            "out: ipout)\n",
            "out: max_pooling1d_1 (MaxPoolin  (None, 10, 16)            0\n",
            "out: g1D)\n",
            "out: batch_normalization_1 (Bat  (None, 10, 16)            64\n",
            "out: chNormalization)\n",
            "out: conv1d_flipout_2 (Conv1DFl  (None, 9, 32)             2080\n",
            "out: ipout)\n",
            "out: batch_normalization_2 (Bat  (None, 9, 32)             128\n",
            "out: chNormalization)\n",
            "out: global_average_pooling1d (  (None, 32)                0\n",
            "out: GlobalAveragePooling1D)\n",
            "out: dense_flipout (DenseFlipou  (None, 32)                2080\n",
            "out: t)\n",
            "out: batch_normalization_3 (Bat  (None, 32)                128\n",
            "out: chNormalization)\n",
            "out: dense_flipout_1 (DenseFlip  (None, 6)                 390\n",
            "out: out)\n",
            "out: =================================================================\n",
            "out: Total params: 6846 (26.74 KB)\n",
            "out: Trainable params: 6670 (26.05 KB)\n",
            "out: Non-trainable params: 176 (704.00 Byte)\n",
            "out: _________________________________________________________________\n",
            "out: None\n",
            "out: Found GPU at: /device:GPU:0\n",
            "out: ------------ TRAINING ------------\n",
            "out: Features shape: (3000, 112, 4)\n",
            "out: Labels shape: (3000, 6)\n",
            "out: Initializing checkpoint from scratch.\n",
            "out: Epoch 0\n",
            "out: Validation loss decreased. Saved checkpoint for step 1: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-1\n",
            "out: Time:  87.03s, ---- Loss: 1.7528, Acc.: 0.2543, Val. Loss: 2.6896, Val. Acc.: 0.1722\n",
            "out: Epoch 1\n",
            "out: Loss did not decrease. Count = 1\n",
            "out: Time:  78.80s, ---- Loss: 1.4807, Acc.: 0.3926, Val. Loss: 2.7202, Val. Acc.: 0.1766\n",
            "out: Epoch 2\n",
            "out: Loss did not decrease. Count = 2\n",
            "out: Time:  78.29s, ---- Loss: 1.3407, Acc.: 0.5011, Val. Loss: 2.9207, Val. Acc.: 0.1724\n",
            "out: Epoch 3\n",
            "out: Loss did not decrease. Count = 3\n",
            "out: Time:  78.69s, ---- Loss: 1.2714, Acc.: 0.5370, Val. Loss: 2.7673, Val. Acc.: 0.1770\n",
            "out: Epoch 4\n",
            "out: Validation loss decreased. Saved checkpoint for step 5: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-2\n",
            "out: Time:  78.31s, ---- Loss: 1.2491, Acc.: 0.5565, Val. Loss: 2.6091, Val. Acc.: 0.2117\n",
            "out: Epoch 5\n",
            "out: Validation loss decreased. Saved checkpoint for step 6: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-3\n",
            "out: Time:  78.81s, ---- Loss: 1.2255, Acc.: 0.5678, Val. Loss: 2.5802, Val. Acc.: 0.2307\n",
            "out: Epoch 6\n",
            "out: Loss did not decrease. Count = 1\n",
            "out: Time:  78.46s, ---- Loss: 1.2186, Acc.: 0.5769, Val. Loss: 2.6528, Val. Acc.: 0.2263\n",
            "out: Epoch 7\n",
            "out: Loss did not decrease. Count = 2\n",
            "out: Time:  78.38s, ---- Loss: 1.2085, Acc.: 0.5855, Val. Loss: 2.6613, Val. Acc.: 0.2600\n",
            "out: Epoch 8\n",
            "out: Loss did not decrease. Count = 3\n",
            "out: Time:  78.51s, ---- Loss: 1.1876, Acc.: 0.5899, Val. Loss: 2.6446, Val. Acc.: 0.2713\n",
            "out: Epoch 9\n",
            "out: Validation loss decreased. Saved checkpoint for step 10: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-4\n",
            "out: Time:  78.22s, ---- Loss: 1.1829, Acc.: 0.5931, Val. Loss: 2.5708, Val. Acc.: 0.3210\n",
            "out: Epoch 10\n",
            "out: Loss did not decrease. Count = 1\n",
            "out: Time:  78.39s, ---- Loss: 1.1554, Acc.: 0.5968, Val. Loss: 2.5861, Val. Acc.: 0.3508\n",
            "out: Epoch 11\n",
            "out: Loss did not decrease. Count = 2\n",
            "out: Time:  78.30s, ---- Loss: 1.1310, Acc.: 0.6027, Val. Loss: 2.5846, Val. Acc.: 0.3441\n",
            "out: Epoch 12\n",
            "out: Loss did not decrease. Count = 3\n",
            "out: Time:  78.15s, ---- Loss: 1.1447, Acc.: 0.6055, Val. Loss: 2.5982, Val. Acc.: 0.3591\n",
            "out: Epoch 13\n",
            "out: Validation loss decreased. Saved checkpoint for step 14: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-5\n",
            "out: Time:  77.94s, ---- Loss: 1.1188, Acc.: 0.6072, Val. Loss: 2.5044, Val. Acc.: 0.3797\n",
            "out: Epoch 14\n",
            "out: Validation loss decreased. Saved checkpoint for step 15: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-6\n",
            "out: Time:  78.49s, ---- Loss: 1.0942, Acc.: 0.6097, Val. Loss: 2.3823, Val. Acc.: 0.4098\n",
            "out: Epoch 15\n",
            "out: Validation loss decreased. Saved checkpoint for step 16: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-7\n",
            "out: Time:  77.92s, ---- Loss: 1.0763, Acc.: 0.6167, Val. Loss: 2.3532, Val. Acc.: 0.4204\n",
            "out: Epoch 16\n",
            "out: Validation loss decreased. Saved checkpoint for step 17: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-8\n",
            "out: Time:  78.68s, ---- Loss: 1.1048, Acc.: 0.6180, Val. Loss: 2.3121, Val. Acc.: 0.4509\n",
            "out: Epoch 17\n",
            "out: Validation loss decreased. Saved checkpoint for step 18: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-9\n",
            "out: Time:  78.15s, ---- Loss: 1.0721, Acc.: 0.6213, Val. Loss: 2.2384, Val. Acc.: 0.4681\n",
            "out: Epoch 18\n",
            "out: Validation loss decreased. Saved checkpoint for step 19: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-10\n",
            "out: Time:  77.65s, ---- Loss: 1.0669, Acc.: 0.6211, Val. Loss: 2.2196, Val. Acc.: 0.4877\n",
            "out: Epoch 19\n",
            "out: Validation loss decreased. Saved checkpoint for step 20: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-11\n",
            "out: Time:  77.85s, ---- Loss: 1.0430, Acc.: 0.6274, Val. Loss: 2.1509, Val. Acc.: 0.5073\n",
            "out: Epoch 20\n",
            "out: Validation loss decreased. Saved checkpoint for step 21: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-12\n",
            "out: Time:  77.76s, ---- Loss: 1.0687, Acc.: 0.6306, Val. Loss: 2.1001, Val. Acc.: 0.5242\n",
            "out: Epoch 21\n",
            "out: Validation loss decreased. Saved checkpoint for step 22: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-13\n",
            "out: Time:  77.38s, ---- Loss: 1.0803, Acc.: 0.6315, Val. Loss: 2.0462, Val. Acc.: 0.5464\n",
            "out: Epoch 22\n",
            "out: Validation loss decreased. Saved checkpoint for step 23: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-14\n",
            "out: Time:  77.11s, ---- Loss: 1.0469, Acc.: 0.6371, Val. Loss: 2.0013, Val. Acc.: 0.5601\n",
            "out: Epoch 23\n",
            "out: Validation loss decreased. Saved checkpoint for step 24: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-15\n",
            "out: Time:  77.46s, ---- Loss: 1.0642, Acc.: 0.6357, Val. Loss: 1.9667, Val. Acc.: 0.5788\n",
            "out: Epoch 24\n",
            "out: Validation loss decreased. Saved checkpoint for step 25: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-16\n",
            "out: Time:  76.90s, ---- Loss: 1.0552, Acc.: 0.6396, Val. Loss: 1.9509, Val. Acc.: 0.5847\n",
            "out: Epoch 25\n",
            "out: Validation loss decreased. Saved checkpoint for step 26: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-17\n",
            "out: Time:  76.14s, ---- Loss: 1.0404, Acc.: 0.6416, Val. Loss: 1.9269, Val. Acc.: 0.5836\n",
            "out: Epoch 26\n",
            "out: Validation loss decreased. Saved checkpoint for step 27: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-18\n",
            "out: Time:  77.62s, ---- Loss: 1.0286, Acc.: 0.6415, Val. Loss: 1.9189, Val. Acc.: 0.5898\n",
            "out: Epoch 27\n",
            "out: Loss did not decrease. Count = 1\n",
            "out: Time:  77.61s, ---- Loss: 1.0478, Acc.: 0.6488, Val. Loss: 1.9406, Val. Acc.: 0.5729\n",
            "out: Epoch 28\n",
            "out: Validation loss decreased. Saved checkpoint for step 29: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-19\n",
            "out: Time:  77.31s, ---- Loss: 1.0283, Acc.: 0.6501, Val. Loss: 1.8964, Val. Acc.: 0.5922\n",
            "out: Epoch 29\n",
            "out: Validation loss decreased. Saved checkpoint for step 30: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-20\n",
            "out: Time:  77.41s, ---- Loss: 1.0225, Acc.: 0.6492, Val. Loss: 1.8899, Val. Acc.: 0.5962\n",
            "out: Epoch 30\n",
            "out: Validation loss decreased. Saved checkpoint for step 31: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-21\n",
            "out: Time:  77.04s, ---- Loss: 1.0092, Acc.: 0.6527, Val. Loss: 1.8655, Val. Acc.: 0.5987\n",
            "out: Epoch 31\n",
            "out: Validation loss decreased. Saved checkpoint for step 32: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-22\n",
            "out: Time:  77.45s, ---- Loss: 1.0291, Acc.: 0.6531, Val. Loss: 1.8476, Val. Acc.: 0.6074\n",
            "out: Epoch 32\n",
            "out: Validation loss decreased. Saved checkpoint for step 33: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-23\n",
            "out: Time:  76.96s, ---- Loss: 1.0044, Acc.: 0.6562, Val. Loss: 1.8456, Val. Acc.: 0.6012\n",
            "out: Epoch 33\n",
            "out: Validation loss decreased. Saved checkpoint for step 34: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-24\n",
            "out: Time:  77.94s, ---- Loss: 1.0366, Acc.: 0.6580, Val. Loss: 1.8353, Val. Acc.: 0.6033\n",
            "out: Epoch 34\n",
            "out: Loss did not decrease. Count = 1\n",
            "out: Time:  76.71s, ---- Loss: 0.9960, Acc.: 0.6587, Val. Loss: 1.8625, Val. Acc.: 0.5918\n",
            "out: Epoch 35\n",
            "out: Loss did not decrease. Count = 2\n",
            "out: Time:  76.48s, ---- Loss: 0.9958, Acc.: 0.6573, Val. Loss: 1.8558, Val. Acc.: 0.5878\n",
            "out: Epoch 36\n",
            "out: Loss did not decrease. Count = 3\n",
            "out: Time:  76.33s, ---- Loss: 0.9834, Acc.: 0.6611, Val. Loss: 1.8668, Val. Acc.: 0.5912\n",
            "out: Epoch 37\n",
            "out: Validation loss decreased. Saved checkpoint for step 38: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-25\n",
            "out: Time:  75.55s, ---- Loss: 1.0009, Acc.: 0.6614, Val. Loss: 1.8219, Val. Acc.: 0.6074\n",
            "out: Epoch 38\n",
            "out: Loss did not decrease. Count = 1\n",
            "out: Time:  75.56s, ---- Loss: 0.9839, Acc.: 0.6619, Val. Loss: 1.8296, Val. Acc.: 0.5984\n",
            "out: Epoch 39\n",
            "out: Validation loss decreased. Saved checkpoint for step 40: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-26\n",
            "out: Time:  76.79s, ---- Loss: 0.9996, Acc.: 0.6640, Val. Loss: 1.8103, Val. Acc.: 0.6084\n",
            "out: Epoch 40\n",
            "out: Loss did not decrease. Count = 1\n",
            "out: Time:  76.26s, ---- Loss: 0.9711, Acc.: 0.6634, Val. Loss: 1.8154, Val. Acc.: 0.6079\n",
            "out: Epoch 41\n",
            "out: Validation loss decreased. Saved checkpoint for step 42: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-27\n",
            "out: Time:  76.43s, ---- Loss: 0.9918, Acc.: 0.6647, Val. Loss: 1.7868, Val. Acc.: 0.6226\n",
            "out: Epoch 42\n",
            "out: Validation loss decreased. Saved checkpoint for step 43: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-28\n",
            "out: Time:  76.47s, ---- Loss: 0.9882, Acc.: 0.6653, Val. Loss: 1.7780, Val. Acc.: 0.6264\n",
            "out: Epoch 43\n",
            "out: Loss did not decrease. Count = 1\n",
            "out: Time:  76.85s, ---- Loss: 0.9544, Acc.: 0.6707, Val. Loss: 1.7831, Val. Acc.: 0.6287\n",
            "out: Epoch 44\n",
            "out: Validation loss decreased. Saved checkpoint for step 45: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-29\n",
            "out: Time:  76.09s, ---- Loss: 0.9642, Acc.: 0.6671, Val. Loss: 1.7533, Val. Acc.: 0.6369\n",
            "out: Epoch 45\n",
            "out: Validation loss decreased. Saved checkpoint for step 46: models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-30\n",
            "out: Time:  76.54s, ---- Loss: 0.9896, Acc.: 0.6655, Val. Loss: 1.7450, Val. Acc.: 0.6412\n",
            "out: Epoch 46\n",
            "out: Loss did not decrease. Count = 1\n",
            "out: Time:  76.15s, ---- Loss: 0.9762, Acc.: 0.6688, Val. Loss: 1.7605, Val. Acc.: 0.6322\n",
            "out: Epoch 47\n",
            "out: Loss did not decrease. Count = 2\n",
            "out: Time:  76.48s, ---- Loss: 0.9500, Acc.: 0.6716, Val. Loss: 1.7539, Val. Acc.: 0.6380\n",
            "out: Epoch 48\n",
            "out: Loss did not decrease. Count = 3\n",
            "out: Time:  76.19s, ---- Loss: 0.9826, Acc.: 0.6707, Val. Loss: 1.7600, Val. Acc.: 0.6343\n",
            "out: Epoch 49\n",
            "out: Loss did not decrease. Count = 4\n",
            "out: Time:  76.74s, ---- Loss: 0.9611, Acc.: 0.6695, Val. Loss: 1.7571, Val. Acc.: 0.6391\n",
            "out: Saving at models/ds_ee2_1k_equalweight_randoms_kmax_5/hist.png\n",
            "out: Done in 3888.34s\n"
          ]
        }
      ],
      "source": [
        "# -------------- training loads parameters entered above  --------------\n",
        "proc = subprocess.Popen([\"python3\", \"train.py\", \"--test_mode\" , test_mode, \"--seed\", seed, \\\n",
        "                     \"--bayesian\", bayesian, \"--model_name\", model_name, \\\n",
        "                     \"--fine_tune\", fine_tune, \"--log_path\", log_path,\\\n",
        "                     \"--restore\", restore, \\\n",
        "                     \"--models_dir\", mdir, \\\n",
        "                     \"--fname\", fname, \\\n",
        "                     \"--DIR\", DIR, \\\n",
        "                     '--norm_data_name', norm_data_name, \\\n",
        "                     '--curves_folder', curves_folder,\\\n",
        "                     \"--c_0\", 'lcdm', \\\n",
        "                     \"--c_1\", 'fR', 'dgp', 'wcdm', 'ds', 'rand'\\\n",
        "                     \"--save_ckpt\", save_ckpt, \\\n",
        "                     \"--im_depth\", im_depth, \"--im_width\", im_width, \"--im_channels\", im_channels, \\\n",
        "                     \"--swap_axes\", swap_axes, \\\n",
        "                     \"--sort_labels\", sort_labels, \\\n",
        "                     \"--add_noise\", add_noise, \"--add_shot\", add_shot, \"--add_sys\", add_sys,\"--add_cosvar\", add_cosvar, \\\n",
        "                     \"--sigma_curves\", sigma_curves, \\\n",
        "                     \"--sigma_curves_default\", sigma_curves_default, \\\n",
        "                     \"--save_processed_spectra\", save_processed_spectra, \\\n",
        "                     \"--sample_pace\", sample_pace,\\\n",
        "                     \"--n_noisy_samples\", n_noisy_samples, \\\n",
        "                     \"--rescale_curves\", rescale_curves, \\\n",
        "                     \"--val_size\", val_size, \\\n",
        "                     \"--z_bins\", z1,z2,z3,z4, \\\n",
        "                     \"--filters\", f1,f2,f3, \"--kernel_sizes\", k1,k2,k3, \"--strides\", s1,s2,s3, \"--pool_sizes\", p1,p2,p3, \"--strides_pooling\", sp1,sp2,sp3, \\\n",
        "                     \"--k_max\", k_max,\\\n",
        "                     \"--n_dense\", n_dense,\\\n",
        "                     \"--add_FT_dense\", add_FT_dense, \\\n",
        "                     \"--n_epochs\", n_epochs, \"--patience\", patience, \"--batch_size\", batch_size, \"--lr\", lr, \\\n",
        "                     \"--decay\", decay, \\\n",
        "                     \"--GPU\", GPU],\\\n",
        "                     stdout=subprocess.PIPE, \\\n",
        "                     stderr=subprocess.PIPE)\n",
        "\n",
        "with open(log_path, \"w\") as log_file:\n",
        "\n",
        "    stdout_thread = threading.Thread(target=handle_output, args=(proc.stdout, \"out\", log_file))\n",
        "    stderr_thread = threading.Thread(target=handle_output, args=(proc.stderr, \"err\", log_file))\n",
        "\n",
        "    stdout_thread.start()\n",
        "    stderr_thread.start()\n",
        "\n",
        "    stdout_thread.join()\n",
        "    stderr_thread.join()\n",
        "\n",
        "    proc.wait()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6v9Ql7T87Me",
        "outputId": "2efb9d8d-4c64-4ecd-c1f9-4597f885dee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mEE2_train20k_baseline\u001b[0m/                    \u001b[01;34mmodel_withBayes_GPU_train_1k\u001b[0m/\n",
            "\u001b[01;34mmodel_withBayes_GPU_train_100\u001b[0m/            model_withBayes_GPU_train_1k_log_FT.txt\n",
            "model_withBayes_GPU_train_100_log_FT.txt\n"
          ]
        }
      ],
      "source": [
        "ls models/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3sLuZopQ8Dy"
      },
      "source": [
        "## Run the test of the model\n",
        "- insert the path+name of the log file of the model\n",
        "- change the test directory\n",
        "- change the custom name that will be used for the confusion matrix\n",
        "\n",
        "The pdf file with the confusion matrix is saved in the model folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c98CGa3KJlyf",
        "outputId": "2d10880c-82d2-49ac-f7f0-f6b5171bfffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-21 17:34:49.240436: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-21 17:34:49.240492: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-21 17:34:49.241751: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-21 17:34:50.237196: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Reading log from models/ds_ee2_1k_equalweight_randoms_kmax_5/ds_ee2_1k_equalweight_randoms_kmax_5_log.txt \n",
            "\n",
            " -------- Loaded parameters:\n",
            "bayesian True\n",
            "test_mode False\n",
            "n_test_idx 2\n",
            "seed 1312\n",
            "fine_tune False\n",
            "one_vs_all False\n",
            "c_0 ['lcdm']\n",
            "c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']\n",
            "dataset_balanced False\n",
            "include_last False\n",
            "log_path \n",
            "restore False\n",
            "fname ds_ee2_1k_equalweight_randoms_kmax_5\n",
            "model_name custom\n",
            "my_path None\n",
            "DIR data/ds_ee2_train_1k\n",
            "TEST_DIR data/test_data/\n",
            "models_dir models/\n",
            "save_ckpt True\n",
            "out_path_overwrite False\n",
            "curves_folder data/curve_files_sys/theory_error\n",
            "save_processed_spectra False\n",
            "im_depth 500\n",
            "im_width 1\n",
            "im_channels 4\n",
            "swap_axes True\n",
            "sort_labels True\n",
            "norm_data_name /planck_ee2.txt\n",
            "normalization stdcosmo\n",
            "sample_pace 4\n",
            "k_max 5.0\n",
            "i_max None\n",
            "add_noise True\n",
            "n_noisy_samples 10\n",
            "add_shot False\n",
            "add_sys True\n",
            "add_cosvar True\n",
            "sigma_sys None\n",
            "sys_scaled None\n",
            "sys_factor None\n",
            "sys_max None\n",
            "sigma_curves 0.05\n",
            "sigma_curves_default 0.05\n",
            "rescale_curves uniform\n",
            "z_bins [0, 1, 2, 3]\n",
            "n_dense 1\n",
            "filters [8, 16, 32]\n",
            "kernel_sizes [10, 5, 2]\n",
            "strides [2, 2, 1]\n",
            "pool_sizes [2, 2, 0]\n",
            "strides_pooling [2, 1, 0]\n",
            "add_FT_dense False\n",
            "trainable False\n",
            "unfreeze False\n",
            "lr 0.01\n",
            "drop 0.5\n",
            "n_epochs 50\n",
            "val_size 0.15\n",
            "test_size 0.0\n",
            "batch_size 3000\n",
            "patience 50\n",
            "GPU True\n",
            "decay 0.95\n",
            "BatchNorm True\n",
            "group_lab_dict {'True': 'non_lcdm', 'dgp': 'non_lcdm', 'ds': 'non_lcdm', 'fR': 'non_lcdm', 'rand--save_ckpt': 'non_lcdm', 'wcdm': 'non_lcdm', 'lcdm': 'lcdm'}\n",
            "FLAGS.one_vs_all : False\n",
            "TEST_NORM_FILE: FLAGS.norm_data_name from log_path  /planck_ee2.txt\n",
            "Saving cm at DS_EE2_1K_EQUALWEIGHT_RAND_kmax_5\n",
            "Setting save_indexes to False\n",
            "Using data in the directory data/ds_ee2_test_1k\n",
            "TEST_NORM_FILE: FLAGS.norm_data_name after args written to FLAGS /planck_ee2.txt\n",
            "-------------------- out_path with FLAGS_fname models/ds_ee2_1k_equalweight_randoms_kmax_5\n",
            "Logger creating log file: models/ds_ee2_1k_equalweight_randoms_kmax_5/DS_EE2_1K_EQUALWEIGHT_RAND_kmax_5_log.txt\n",
            "\n",
            " -------- Parameters:\n",
            "bayesian True\n",
            "test_mode False\n",
            "n_test_idx 2\n",
            "seed 1312\n",
            "fine_tune False\n",
            "one_vs_all False\n",
            "c_0 ['lcdm']\n",
            "c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']\n",
            "dataset_balanced False\n",
            "include_last False\n",
            "log_path \n",
            "restore False\n",
            "fname ds_ee2_1k_equalweight_randoms_kmax_5\n",
            "model_name custom\n",
            "my_path None\n",
            "DIR data/ds_ee2_train_1k\n",
            "TEST_DIR data/ds_ee2_test_1k\n",
            "models_dir models/\n",
            "save_ckpt True\n",
            "out_path_overwrite False\n",
            "curves_folder data/curve_files_sys/theory_error\n",
            "save_processed_spectra False\n",
            "im_depth 500\n",
            "im_width 1\n",
            "im_channels 4\n",
            "swap_axes True\n",
            "sort_labels True\n",
            "norm_data_name /planck_ee2.txt\n",
            "normalization stdcosmo\n",
            "sample_pace 4\n",
            "k_max 5.0\n",
            "i_max None\n",
            "add_noise True\n",
            "n_noisy_samples 10\n",
            "add_shot False\n",
            "add_sys True\n",
            "add_cosvar True\n",
            "sigma_sys None\n",
            "sys_scaled None\n",
            "sys_factor None\n",
            "sys_max None\n",
            "sigma_curves 0.05\n",
            "sigma_curves_default 0.05\n",
            "rescale_curves uniform\n",
            "z_bins [0, 1, 2, 3]\n",
            "n_dense 1\n",
            "filters [8, 16, 32]\n",
            "kernel_sizes [10, 5, 2]\n",
            "strides [2, 2, 1]\n",
            "pool_sizes [2, 2, 0]\n",
            "strides_pooling [2, 1, 0]\n",
            "add_FT_dense False\n",
            "trainable False\n",
            "unfreeze False\n",
            "lr 0.01\n",
            "drop 0.5\n",
            "n_epochs 50\n",
            "val_size 0.15\n",
            "test_size 0.0\n",
            "batch_size 3000\n",
            "patience 50\n",
            "GPU True\n",
            "decay 0.95\n",
            "BatchNorm True\n",
            "group_lab_dict {'True': 'non_lcdm', 'dgp': 'non_lcdm', 'ds': 'non_lcdm', 'fR': 'non_lcdm', 'rand--save_ckpt': 'non_lcdm', 'wcdm': 'non_lcdm', 'lcdm': 'lcdm'}\n",
            "save_indexes False\n",
            "------------ CREATING DATA GENERATORS ------------\n",
            "\n",
            "labels : ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']\n",
            "Labels encoding: \n",
            "{'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}\n",
            "n_labels : 6\n",
            "dgp - 1000 training examples\n",
            "ds - 1000 training examples\n",
            "fr - 1000 training examples\n",
            "lcdm - 1000 training examples\n",
            "rand - 1000 training examples\n",
            "wcdm - 1000 training examples\n",
            "\n",
            "N. of data files: 1000\n",
            "get_all_indexes labels dict: {'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}\n",
            "create_generators n_labels_eff: 6\n",
            "create_generators len_c1: 1\n",
            "--Train\n",
            "batch_size: 3000\n",
            "- Cut sample\n",
            "bs: 3000\n",
            "N_labels: 6\n",
            "N_noise: 10\n",
            "len_c1: 1\n",
            "Indexes length: 1000\n",
            "n_keep: 1000\n",
            "Sampling\n",
            "New length: 1000\n",
            "N batches: 20.0\n",
            " len_C1: 1\n",
            "N indexes: 50.0\n",
            "Ok.\n",
            "N. of test files used: 1000\n",
            "Data Generator Initialization\n",
            "Using z bins [0, 1, 2, 3]\n",
            "Normalisation file is /planck_ee2.txt\n",
            "Specified k_max is 5.0\n",
            "Corresponding i_max is 112\n",
            "Closest k to k_max is 4.936132\n",
            "New data dim: (112, 1)\n",
            "Final i_max used is 112\n",
            "one_vs_all: False\n",
            "dataset_balanced: False\n",
            "base_case_dataset: True\n",
            "N. classes: 6\n",
            "N. n_classes in output: 6\n",
            "LABELS: ['dgp', 'ds', 'fr', 'lcdm', 'rand', 'wcdm']\n",
            "list_IDs length: 1000\n",
            "n_indexes (n of file IDs read for each batch): 50\n",
            "batch size: 3000\n",
            "n_batches : 20\n",
            "For each batch we read 50 file IDs\n",
            "For each file ID we have 6 labels\n",
            "For each ID, label we have 10 realizations of noise\n",
            "In total, for each batch we have 3000 training examples\n",
            "Input batch size: 3000\n",
            "N of batches to cover all file IDs: 20\n",
            "------------ DONE ------------\n",
            "\n",
            "Input shape (112, 4)\n",
            "------------ BUILDING MODEL ------------\n",
            "\n",
            "Model n_classes : 6 \n",
            "Features shape: (3000, 112, 4)\n",
            "Labels shape: (3000, 6)\n",
            "using 1D layers and 4 channels\n",
            "Expected output dimension of layer conv1d_flipout: 52.0\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "  loc = add_variable_fn(\n",
            "2024-02-21 17:35:00.380688: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "  untransformed_scale = add_variable_fn(\n",
            "Expected output dimension of layer max_pooling1d: 26.0\n",
            "Expected output dimension of layer conv1d_flipout_1: 11.5\n",
            "Expected output dimension of layer max_pooling1d_1: 10.5\n",
            "Expected output dimension of layer conv1d_flipout_2: 9.5\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 112, 4)]          0         \n",
            "                                                                 \n",
            " conv1d_flipout (Conv1DFlip  (None, 52, 8)             648       \n",
            " out)                                                            \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 26, 8)             0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 26, 8)             32        \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv1d_flipout_1 (Conv1DFl  (None, 11, 16)            1296      \n",
            " ipout)                                                          \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 10, 16)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 10, 16)            64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_flipout_2 (Conv1DFl  (None, 9, 32)             2080      \n",
            " ipout)                                                          \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 9, 32)             128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 32)                0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense_flipout (DenseFlipou  (None, 32)                2080      \n",
            " t)                                                              \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 32)                128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_flipout_1 (DenseFlip  (None, 6)                 390       \n",
            " out)                                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6846 (26.74 KB)\n",
            "Trainable params: 6670 (26.05 KB)\n",
            "Non-trainable params: 176 (704.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Computing loss for randomly initialized model...\n",
            "Loss before loading weights/ 1.9353333\n",
            "\n",
            "------------ RESTORING CHECKPOINT ------------\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "Looking for ckpt in models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/\n",
            "Restoring checkpoint from models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-30\n",
            "Loss after loading weights/ 1.0179489\n",
            "\n",
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).step\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._iterations\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._current_learning_rate\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.13\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.14\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.15\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.16\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.17\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.18\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.21\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.22\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.23\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.24\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.25\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.26\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.27\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.28\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.29\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.30\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.31\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.32\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.33\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.34\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.35\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.36\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.37\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.38\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.39\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.40\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.41\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.42\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.43\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.44\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.45\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.46\n",
            "Threshold probability for classification: 0.5 \n",
            "Accuracy on 0 batch using median of sampled probabilities: 0.536 %\n",
            "Accuracy on 0 batch using median of sampled probabilities, not considering unclassified examples: 0.7928994 %\n",
            "Accuracy on 1 batch using median of sampled probabilities: 0.53033334 %\n",
            "Accuracy on 1 batch using median of sampled probabilities, not considering unclassified examples: 0.7745862 %\n",
            "Accuracy on 2 batch using median of sampled probabilities: 0.5453333 %\n",
            "Accuracy on 2 batch using median of sampled probabilities, not considering unclassified examples: 0.7731569 %\n",
            "Accuracy on 3 batch using median of sampled probabilities: 0.5093333 %\n",
            "Accuracy on 3 batch using median of sampled probabilities, not considering unclassified examples: 0.75419545 %\n",
            "Accuracy on 4 batch using median of sampled probabilities: 0.5593333 %\n",
            "Accuracy on 4 batch using median of sampled probabilities, not considering unclassified examples: 0.78338003 %\n",
            "Accuracy on 5 batch using median of sampled probabilities: 0.5326667 %\n",
            "Accuracy on 5 batch using median of sampled probabilities, not considering unclassified examples: 0.78256613 %\n",
            "Accuracy on 6 batch using median of sampled probabilities: 0.52033335 %\n",
            "Accuracy on 6 batch using median of sampled probabilities, not considering unclassified examples: 0.75776696 %\n",
            "Accuracy on 7 batch using median of sampled probabilities: 0.5366667 %\n",
            "Accuracy on 7 batch using median of sampled probabilities, not considering unclassified examples: 0.7781537 %\n",
            "Accuracy on 8 batch using median of sampled probabilities: 0.525 %\n",
            "Accuracy on 8 batch using median of sampled probabilities, not considering unclassified examples: 0.7594021 %\n",
            "Accuracy on 9 batch using median of sampled probabilities: 0.52933335 %\n",
            "Accuracy on 9 batch using median of sampled probabilities, not considering unclassified examples: 0.74519 %\n",
            "Accuracy on 10 batch using median of sampled probabilities: 0.546 %\n",
            "Accuracy on 10 batch using median of sampled probabilities, not considering unclassified examples: 0.78 %\n",
            "Accuracy on 11 batch using median of sampled probabilities: 0.5313333 %\n",
            "Accuracy on 11 batch using median of sampled probabilities, not considering unclassified examples: 0.76561 %\n",
            "Accuracy on 12 batch using median of sampled probabilities: 0.53866667 %\n",
            "Accuracy on 12 batch using median of sampled probabilities, not considering unclassified examples: 0.7944936 %\n",
            "Accuracy on 13 batch using median of sampled probabilities: 0.53333336 %\n",
            "Accuracy on 13 batch using median of sampled probabilities, not considering unclassified examples: 0.7718283 %\n",
            "Accuracy on 14 batch using median of sampled probabilities: 0.5513333 %\n",
            "Accuracy on 14 batch using median of sampled probabilities, not considering unclassified examples: 0.7739822 %\n",
            "Accuracy on 15 batch using median of sampled probabilities: 0.54733336 %\n",
            "Accuracy on 15 batch using median of sampled probabilities, not considering unclassified examples: 0.7800475 %\n",
            "Accuracy on 16 batch using median of sampled probabilities: 0.531 %\n",
            "Accuracy on 16 batch using median of sampled probabilities, not considering unclassified examples: 0.75070685 %\n",
            "Accuracy on 17 batch using median of sampled probabilities: 0.54966664 %\n",
            "Accuracy on 17 batch using median of sampled probabilities, not considering unclassified examples: 0.76662016 %\n",
            "Accuracy on 18 batch using median of sampled probabilities: 0.5086667 %\n",
            "Accuracy on 18 batch using median of sampled probabilities, not considering unclassified examples: 0.75098425 %\n",
            "Accuracy on 19 batch using median of sampled probabilities: 0.54433334 %\n",
            "Accuracy on 19 batch using median of sampled probabilities, not considering unclassified examples: 0.7839654 %\n",
            "-- Accuracy on test set using median of sampled probabilities: 0.5353 % \n",
            "\n",
            "-- Accuracy on test set using median of sampled probabilities, not considering unclassified examples: 0.7709768 % \n",
            "\n",
            "Adding Not classified label\n",
            "Saved confusion matrix at models/ds_ee2_1k_equalweight_randoms_kmax_5/cm_DS_EE2_1K_EQUALWEIGHT_RAND_kmax_5_frozen_weights.pdf\n",
            "Saved confusion matrix values at models/ds_ee2_1k_equalweight_randoms_kmax_5/cm_DS_EE2_1K_EQUALWEIGHT_RAND_kmax_5_frozen_weights_values.txt\n"
          ]
        }
      ],
      "source": [
        "!python test.py --log_path='models/ds_ee2_1k_equalweight_randoms_kmax_5/ds_ee2_1k_equalweight_randoms_kmax_5_log.txt' \\\n",
        "  --TEST_DIR='data/ds_ee2_test_1k' --cm_name_custom='DS_EE2_1K_EQUALWEIGHT_RAND_kmax_5'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ux1dgMxJXn"
      },
      "source": [
        "## Load modules for classification of a single spectrum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYGq6yP8Nvoq"
      },
      "source": [
        "### Functions to load trained models and data\n",
        "just execute the following cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gazkA01-CMCj"
      },
      "outputs": [],
      "source": [
        "def read_pre_trained_flags(args_dict):\n",
        "  '''\n",
        "  Read model options from logfile and return them in the form of FLAGS (the corresponding class is in utils)\n",
        "  '''\n",
        "  args = DummyFlags(args_dict)\n",
        "  FLAGS = get_flags(args.log_path)\n",
        "  if args.models_dir is not None:\n",
        "        print('Reading model from the directory %s' %args.models_dir)\n",
        "        FLAGS.models_dir = args.models_dir\n",
        "  return FLAGS\n",
        "\n",
        "\n",
        "def get_pre_trained_model(fname, n_classes, input_shape):\n",
        "  '''\n",
        "  returns pre-trained model.\n",
        "  Input:  - fname: string, folder where the trained model is stored\n",
        "          - n_classes: integer, number of output classes of the model\n",
        "          - input_shape:  tuple, input dimensions of the model.\n",
        "                          For pre-trained model in the paper use (100, 4)\n",
        "\n",
        "  '''\n",
        "\n",
        "  log_path = os.path.join('models', fname, fname+'_log.txt' )\n",
        "  models_dir = os.path.join('models', fname)\n",
        "\n",
        "  args_dict= {'log_path':log_path, 'models_dir': models_dir, }\n",
        "  FLAGS=read_pre_trained_flags(args_dict)\n",
        "\n",
        "  print('Input shape %s' %str(input_shape))\n",
        "\n",
        "\n",
        "  model_loaded =  load_model_for_test(FLAGS, input_shape, n_classes=n_classes,\n",
        "                                        generator=None, new_fname='')\n",
        "\n",
        "\n",
        "  return model_loaded\n",
        "\n",
        "\n",
        "\n",
        "def load_X(fname, sample_pace, i_max, norm_data,\n",
        "           add_noise=True, normalise=True, use_sample_pace=False, include_k=True):\n",
        "  '''\n",
        "  Generates one power spectrum X ready for classification.\n",
        "  Options:  fname: path to data\n",
        "            sample_pace: read one point every sample_pace. Useful to reduce data dimension\n",
        "            i_max: Index corresponding to the max k used\n",
        "            norm_data: normalization spectrum\n",
        "\n",
        "            add_noise: whether to add noise or not\n",
        "            normalise: normalise by Planck or not\n",
        "            use_sample_pace\n",
        "            include_k: return k, X or only X\n",
        "\n",
        "\n",
        "\n",
        "  '''\n",
        "\n",
        "  all = np.loadtxt(fname)\n",
        "  print('loaded data shape: %s' %str(all.shape))\n",
        "  if include_k:\n",
        "    k, X = all[:, 0], all[:, 1:]\n",
        "    print('k, X  shape: %s, %s' %(str(k.shape), str(X.shape)))\n",
        "  else:\n",
        "    X = all\n",
        "    k=np.arange(X.shape[0])\n",
        "    print('X  shape : %s' %( str(X.shape)))\n",
        "  if use_sample_pace:\n",
        "    k, X = k[::sample_pace], X[::sample_pace]\n",
        "  print('X  shape after sample pace:  %s' %( str(X.shape)))\n",
        "  k, X  = k[:i_max], X[:i_max]\n",
        "  print('X  shape after k max: %s' %( str(X.shape)))\n",
        "  if add_noise:\n",
        "    noise = np.random.normal(loc=0, scale=generate_noise(k,X, add_sys=True,add_shot=True,sigma_sys=5 ))\n",
        "    X = X+noise\n",
        "  if normalise:\n",
        "    planck = norm_data\n",
        "    print('planck data shape: %s' %str(planck.shape))\n",
        "    X = X/planck-1\n",
        "  if include_k:\n",
        "    return k, X\n",
        "  else:\n",
        "    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho00eqUjDVPp"
      },
      "source": [
        "## Code for getting confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3ciV9qDIDXY7"
      },
      "outputs": [],
      "source": [
        "def sample_from_data(model,  my_Xs_norm,  my_min=np.zeros(5), my_max=np.ones(5),\n",
        "                     n_samples=50000, num_monte_carlo=100, th_prob=0.5,\n",
        "                     verbose=True):\n",
        "  '''\n",
        "  Given some normalized features my_Xs_norm and the model, compute mu and Sigma as in the paper eq. 2.8-2.9\n",
        "  and return samples from the probability distribuion eq. 2.10\n",
        "  '''\n",
        "  Al_error, Ep_error, mean = get_mu_Sigma(model, my_Xs_norm , num_monte_carlo=num_monte_carlo, th_prob=th_prob, verbose=verbose)\n",
        "  Sigma = (Al_error+ Ep_error).numpy()\n",
        "  MM = mean.numpy()\n",
        "  if verbose:\n",
        "    print('Sigma')\n",
        "    print(np.round(Sigma, 3))\n",
        "  samples, z_samples = sample_probas(Sigma, MM, my_min=my_min,my_max=my_max, n_samples=n_samples, verbose=verbose)\n",
        "\n",
        "  return samples, z_samples, Sigma, MM\n",
        "\n",
        "\n",
        "\n",
        "def get_mu_Sigma(model, my_Xs_norm, num_monte_carlo=100, th_prob=0.5, verbose=True):\n",
        "  ''''\n",
        "  Given some normalized features my_Xs_norm and the model, compute mu and Sigma as in the paper eq. 2.8-2.9\n",
        "  '''\n",
        "  if verbose:\n",
        "    print('Computing mu and sigma...')\n",
        "  sampled_probas, _, _ = my_predict(tf.expand_dims(my_Xs_norm, axis=0), model, num_monte_carlo=num_monte_carlo, th_prob=th_prob)\n",
        "  mean = tf.reduce_mean(sampled_probas[:,0,:], axis=0)\n",
        "  _, Al_error, Ep_error = get_err_variances(sampled_probas[:,0,:])\n",
        "  return Al_error, Ep_error, mean\n",
        "\n",
        "\n",
        "def sample_probas(Sigma, MM, my_min=np.zeros(5), my_max=np.ones(5), n_samples=50000, verbose=True):\n",
        "  '''\n",
        "  Given mu and Sigma as in the paper eq. 2.8-2.9, computes B, U as described in appendix B of the paper.\n",
        "  Then calls function to sample the prob. distribution (eq. 2.10)\n",
        "  '''\n",
        "  if verbose:\n",
        "    print('Sample probas call')\n",
        "  evals, evecs = np.linalg.eig(Sigma)\n",
        "  B = evecs\n",
        "  U = B.T @ Sigma @  B\n",
        "  if verbose:\n",
        "    print('U')\n",
        "    print(np.round(U, 3))\n",
        "  n_dims=U.shape[0]-1\n",
        "  n_dims=U.shape[0]-1\n",
        "  ind_min = np.argmin(np.diag(U))\n",
        "  if verbose:\n",
        "    print('Index of min eigenvalue is: %s' %ind_min)\n",
        "    print('Min eigenvalue is: %s' %np.min(np.diag(U)))\n",
        "  UU = np.array([np.sqrt(U[i,i])  for i in range(U.shape[0]) if i!=ind_min ] )\n",
        "  if verbose:\n",
        "    print('U has %s elements ' %str(UU.shape[0]))\n",
        "  samples, z_samples = get_samples(UU, B, MM, my_min=my_min, my_max=my_max, ind_min=ind_min, n_dims=n_dims, n_samples=n_samples, verbose=verbose)\n",
        "  return samples, z_samples\n",
        "\n",
        "\n",
        "def get_samples(UU, B, MM, my_min=np.zeros(5), my_max=np.ones(5), ind_min=-1, n_dims=4, n_samples=50000, verbose=True):\n",
        "  '''\n",
        "  samples eq. 2.10 as described in appendix B\n",
        "  '''\n",
        "  if verbose:\n",
        "    print('Sampling...')\n",
        "  samples = np.zeros((0, n_dims+1))\n",
        "  z_samples = np.zeros((0, n_dims))\n",
        "  while samples.shape[0] < n_samples:\n",
        "    s = np.random.multivariate_normal( np.zeros(n_dims), np.diag(UU**2), size=(n_samples,))\n",
        "    accepted = s[(np.min(X_val_batch(s, B, MM, null_ind=ind_min)-my_min, axis=1) >= 0) & (np.max(X_val_batch(s, B, MM, null_ind=ind_min) - my_max, axis=1) <= 0)]\n",
        "    samples = np.concatenate((samples, X_val_batch(accepted, B, MM, null_ind=ind_min)), axis=0)\n",
        "    z_samples = np.concatenate((z_samples, accepted), axis=0)\n",
        "  samples = samples[:n_samples, :]\n",
        "  z_samples = z_samples[:n_samples, :]\n",
        "  if verbose:\n",
        "    print('Done.')\n",
        "  return samples, z_samples\n",
        "\n",
        "\n",
        "\n",
        "def X_val_batch(Z, B, MM, null_ind):\n",
        "  return np.array([ X_val(Z[i], B, MM, null_ind=null_ind,) for i in range(Z.shape[0])])\n",
        "\n",
        "def Z(X, B, MM):\n",
        "  '''  X must be 5-d '''\n",
        "  return B.T @ (X-MM)\n",
        "\n",
        "def X_val(Z, B, MM, null_ind=-1, verbose=False):\n",
        "  '''  Z must be 4-d , MM must be 5-d '''\n",
        "  if verbose:\n",
        "    print(Z)\n",
        "  Z = np.insert(Z, null_ind, 0)\n",
        "  if verbose:\n",
        "    print(Z)\n",
        "  return B @ Z+ MM\n",
        "\n",
        "\n",
        "def get_err_variances(prob_k, AlEp_split=True):\n",
        "  '''\n",
        "  Computes aleatoric and epistemic uncertainty from MC samples from the net. weights\n",
        "  '''\n",
        "  num_samples = prob_k.shape[0]\n",
        "  prob_mean = tf.reduce_mean(prob_k, axis=0)\n",
        "\n",
        "  diag_probs = tf.stack([np.diag(prob_k[i,:]) for i in range(num_samples)], axis=0)  # Diagonal matrix whose diagonal elements are the output of the network for the ith sample\n",
        "  outer_products_Al = tf.stack([tf.tensordot(prob_k[i,:], prob_k[i,:], axes =0) for i in range(num_samples)], axis=0) # Outer self-product of the predictive vectors for the ith sample used in computing Al uncertainty\n",
        "  outer_products_Ep = tf.stack([tf.tensordot(prob_k[i,:] - prob_mean, prob_k[i,:] - prob_mean, axes =0) for i in range(num_samples)], axis=0) # Outer self-product between the difference of the predictive vector for ith sampl and average prediction\n",
        "\n",
        "  Al_error = tf.reduce_mean(diag_probs - outer_products_Al, axis=0) # Average over all the samples to compute aleotoric uncertainty\n",
        "  Ep_error = tf.reduce_mean(outer_products_Ep, axis=0) # Average over all the samples to compute epistemic uncertainty\n",
        "\n",
        "  tot_error = Al_error + Ep_error # Combine to compute total uncertainty\n",
        "  confidences = tf.linalg.diag_part(tot_error).numpy()\n",
        "\n",
        "  if AlEp_split:\n",
        "    return confidences, Al_error, Ep_error\n",
        "  else:\n",
        "    return confidences\n",
        "\n",
        "\n",
        "\n",
        "def get_P_from_samples(samples, th_prob=0.5 ):\n",
        "  '''\n",
        "  Given samples from eq. 2.10, computes the probabilities as in eq. 2.11\n",
        "  '''\n",
        "  Ntot = samples.shape[0] #-unclassified\n",
        "  n_dims  = samples.shape[-1]\n",
        "  all_preds = np.array([predict_bayes_label(sample, th_prob=th_prob) for sample in samples])\n",
        "  unclassified=all_preds[all_preds==99].shape[0]\n",
        "\n",
        "  n_in_class = np.array([all_preds[all_preds==k].shape[0] for k in range(n_dims)])\n",
        "\n",
        "  #print('Ntot: %s  ' %Ntot)\n",
        "  p_uncl=unclassified/Ntot\n",
        "  #print('%s unclassified examples ' %unclassified)\n",
        "  #print('P(unclassified)=%s ' %p_uncl)\n",
        "  pp=n_in_class/Ntot\n",
        "  #print(pp)\n",
        "  #print(p_uncl)\n",
        "\n",
        "  return np.append(pp, p_uncl)\n",
        "\n",
        "\n",
        "def get_all_probas(X, model_loaded,\n",
        "                   n_classes=5,\n",
        "                   num_monte_carlo=100,\n",
        "                   th_prob=0.5,\n",
        "                   n_samples=10000,\n",
        "                   verbose=True):\n",
        "    '''\n",
        "    Given features X, predicts the labels with MC samples from the net,\n",
        "    computes mu and sigma, samples from the prob distribution\n",
        "    and computes P.\n",
        "    Returns result in the form of a dictionary\n",
        "    '''\n",
        "\n",
        "    # Gaussian approx\n",
        "    samples, z_samples, Sigma, MM = sample_from_data(model_loaded,\n",
        "                                                 my_Xs_norm=X,\n",
        "                                                 my_min=np.zeros(n_classes), my_max=np.ones(n_classes),\n",
        "                                                 n_samples=n_samples,\n",
        "                                                 num_monte_carlo=num_monte_carlo,\n",
        "                                                 th_prob=th_prob, verbose=verbose)\n",
        "\n",
        "    P = get_P_from_samples(samples, th_prob=th_prob)\n",
        "    if verbose:\n",
        "      print('P: %s ' %str(P))\n",
        "      print('P sum to %s' %P.sum())\n",
        "    if verbose:\n",
        "      print('mu  sum to %s' %MM.sum())\n",
        "\n",
        "\n",
        "    # Result\n",
        "    res = {'samples':samples, 'P':P, 'z_samples': z_samples, 'Sigma': Sigma, 'MM': MM,}\n",
        "    return res\n",
        "\n",
        "\n",
        "def plot_hist_1D(samples, MM, th_prob=0.5,\n",
        "                  P=None, inv_labels_dict=None,):\n",
        "  '''\n",
        "  Plots histogram of gauss samples for each class\n",
        "  '''\n",
        "\n",
        "  n_dims = samples.shape[-1]-1\n",
        "  fig, axs = plt.subplots(1, n_dims+1, sharey=True, sharex=False,figsize=(25,5))\n",
        "\n",
        "  t_str=''\n",
        "  for k in range(n_dims+1):\n",
        "\n",
        "    axs[k].hist(samples.T[k], bins=15, color = \"lightgray\",lw=0, density=True, label='Gauss samples')\n",
        "    axs[k].text(0.1, 0.9, '$\\mu = $' + '$'+ str(MM[k])+'$', fontsize = 16 , transform=axs[k].transAxes);\n",
        "    if P is  not None:\n",
        "      axs[k].text(0.1, 0.8, '$ P = $' + '$'+ str(P[k])+'$', fontsize = 16 , transform=axs[k].transAxes);\n",
        "      if k==0:\n",
        "        t_str+= 'P unclassified: %s \\n' %str(np.round(P[-1], 2) )\n",
        "    axs[k].set_xlim(0,1)\n",
        "    axs[k].set_ylim(0,10)\n",
        "\n",
        "    if inv_labels_dict is not None:\n",
        "      axs[k].set_title(inv_labels_dict[k])\n",
        "\n",
        "\n",
        "    axs[k].legend(loc='lower right')\n",
        "\n",
        "  axs[0].set_ylabel('P')\n",
        "  fig.suptitle(t_str)\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKo2vnj_Nzo0"
      },
      "source": [
        "## Load models\n",
        "\n",
        "Here we load the pre trained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2YdVmL6YItLH"
      },
      "outputs": [],
      "source": [
        "# Encoding of the labels for the two networks. Can be found in the log file of the training.\n",
        "# Here it is easier to define it by hand.\n",
        "\n",
        "inv_labels_dict_5={'dgp': 0, 'ds': 1, 'fr': 2, 'lcdm': 3, 'rand': 4, 'wcdm': 5}\n",
        "inv_labels_dict_2={0:'lcdm', 1:'non-LCDM'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9pyiC969KoE"
      },
      "source": [
        "change the model name (first parameter) in the following function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rHuW_fpxJmvc",
        "outputId": "79003bcb-0d48-49ba-ed67-d249dbe64a86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " -------- Loaded parameters:\n",
            "bayesian True\n",
            "test_mode False\n",
            "n_test_idx 2\n",
            "seed 1312\n",
            "fine_tune False\n",
            "one_vs_all False\n",
            "c_0 ['lcdm']\n",
            "c_1 ['True', 'dgp', 'ds', 'fR', 'rand--save_ckpt', 'wcdm']\n",
            "dataset_balanced False\n",
            "include_last False\n",
            "log_path \n",
            "restore False\n",
            "fname ds_ee2_1k_equalweight_randoms_kmax_5\n",
            "model_name custom\n",
            "my_path None\n",
            "DIR data/ds_ee2_train_1k\n",
            "TEST_DIR data/test_data/\n",
            "models_dir models/\n",
            "save_ckpt True\n",
            "out_path_overwrite False\n",
            "curves_folder data/curve_files_sys/theory_error\n",
            "save_processed_spectra False\n",
            "im_depth 500\n",
            "im_width 1\n",
            "im_channels 4\n",
            "swap_axes True\n",
            "sort_labels True\n",
            "norm_data_name /planck_ee2.txt\n",
            "normalization stdcosmo\n",
            "sample_pace 4\n",
            "k_max 5.0\n",
            "i_max None\n",
            "add_noise True\n",
            "n_noisy_samples 10\n",
            "add_shot False\n",
            "add_sys True\n",
            "add_cosvar True\n",
            "sigma_sys None\n",
            "sys_scaled None\n",
            "sys_factor None\n",
            "sys_max None\n",
            "sigma_curves 0.05\n",
            "sigma_curves_default 0.05\n",
            "rescale_curves uniform\n",
            "z_bins [0, 1, 2, 3]\n",
            "n_dense 1\n",
            "filters [8, 16, 32]\n",
            "kernel_sizes [10, 5, 2]\n",
            "strides [2, 2, 1]\n",
            "pool_sizes [2, 2, 0]\n",
            "strides_pooling [2, 1, 0]\n",
            "add_FT_dense False\n",
            "trainable False\n",
            "unfreeze False\n",
            "lr 0.01\n",
            "drop 0.5\n",
            "n_epochs 50\n",
            "val_size 0.15\n",
            "test_size 0.0\n",
            "batch_size 3000\n",
            "patience 50\n",
            "GPU True\n",
            "decay 0.95\n",
            "BatchNorm True\n",
            "group_lab_dict {'True': 'non_lcdm', 'dgp': 'non_lcdm', 'ds': 'non_lcdm', 'fR': 'non_lcdm', 'rand--save_ckpt': 'non_lcdm', 'wcdm': 'non_lcdm', 'lcdm': 'lcdm'}\n",
            "FLAGS.one_vs_all : False\n",
            "Reading model from the directory models/ds_ee2_1k_equalweight_randoms_kmax_5\n",
            "Input shape (100, 4)\n",
            "------------ BUILDING MODEL ------------\n",
            "\n",
            "Model n_classes : 5 \n",
            "using 1D layers and 4 channels\n",
            "Expected output dimension of layer conv1d_flipout: 46.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "  loc = add_variable_fn(\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "  untransformed_scale = add_variable_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected output dimension of layer max_pooling1d: 23.0\n",
            "Expected output dimension of layer conv1d_flipout_1: 10.0\n",
            "Expected output dimension of layer max_pooling1d_1: 9.0\n",
            "Expected output dimension of layer conv1d_flipout_2: 8.0\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 100, 4)]          0         \n",
            "                                                                 \n",
            " conv1d_flipout (Conv1DFlip  (None, 46, 8)             648       \n",
            " out)                                                            \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 23, 8)             0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 23, 8)             32        \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv1d_flipout_1 (Conv1DFl  (None, 10, 16)            1296      \n",
            " ipout)                                                          \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 9, 16)             0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 9, 16)             64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_flipout_2 (Conv1DFl  (None, 8, 32)             2080      \n",
            " ipout)                                                          \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 8, 32)             128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 32)                0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense_flipout (DenseFlipou  (None, 32)                2080      \n",
            " t)                                                              \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 32)                128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_flipout_1 (DenseFlip  (None, 5)                 325       \n",
            " out)                                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6781 (26.49 KB)\n",
            "Trainable params: 6605 (25.80 KB)\n",
            "Non-trainable params: 176 (704.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "------------ RESTORING CHECKPOINT ------------\n",
            "\n",
            "Looking for ckpt in models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/\n",
            "Restoring checkpoint from models/ds_ee2_1k_equalweight_randoms_kmax_5/tf_ckpts/ckpt-30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Received incompatible tensor with shape (6,) when attempting to restore variable with shape (5,) and name dense_flipout_1/bias_posterior_loc:0.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_restore_from_tensors\u001b[0;34m(self, restored_tensors)\u001b[0m\n\u001b[1;32m    781\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m         assigned_variable = shape_safe_assign_variable_handle(\n\u001b[0m\u001b[1;32m    783\u001b[0m             self.handle, self.shape, restored_tensor)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mshape_safe_assign_variable_handle\u001b[0;34m(handle, shape, value, name)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mvalue_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m   \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m   return gen_resource_variable_ops.assign_variable_op(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Shapes (5,) and (6,) are incompatible",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b57867b25286>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pre_trained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ds_ee2_1k_equalweight_randoms_kmax_5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-47e7885a0cf6>\u001b[0m in \u001b[0;36mget_pre_trained_model\u001b[0;34m(fname, n_classes, input_shape)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   model_loaded =  load_model_for_test(FLAGS, input_shape, n_classes=n_classes,\n\u001b[0m\u001b[1;32m     33\u001b[0m                                         generator=None, new_fname='')\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/SeniorHonoursProject/BaCoN-II/importer.py\u001b[0m in \u001b[0;36mload_model_for_test\u001b[0;34m(FLAGS, input_shape, n_classes, generator, FLAGS_ORIGINAL, new_fname)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Checkpoint not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m#print('Checkpoint not found')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/checkpoint.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2706\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2707\u001b[0;31m       \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2708\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2709\u001b[0m         \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure restore operations have completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/checkpoint.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2568\u001b[0m       \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2569\u001b[0m     \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcheckpoint_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpointOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2570\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2571\u001b[0m     metrics.AddCheckpointReadDuration(\n\u001b[1;32m   2572\u001b[0m         \u001b[0mapi_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_CHECKPOINT_V2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/checkpoint.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   1477\u001b[0m         saveables_cache=self._saveables_cache)\n\u001b[1;32m   1478\u001b[0m     restore_lib.CheckpointPosition(\n\u001b[0;32m-> 1479\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1480\u001b[0m                                                    reader)\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/restore.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, trackable, reader)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# This object's correspondence with a checkpointed object is new, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# process deferred restorations for it and its dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_descendants\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/restore.py\u001b[0m in \u001b[0;36m_restore_descendants\u001b[0;34m(self, reader)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     restore_ops.extend(\n\u001b[0;32m--> 463\u001b[0;31m         current_position.checkpoint.restore_saveables(\n\u001b[0m\u001b[1;32m    464\u001b[0m             \u001b[0mtensor_saveables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mpython_positions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/checkpoint.py\u001b[0m in \u001b[0;36mrestore_saveables\u001b[0;34m(self, tensor_saveables, python_positions, registered_savers, reader)\u001b[0m\n\u001b[1;32m    377\u001b[0m       new_restore_ops = functional_saver.MultiDeviceSaver.from_saveables(\n\u001b[1;32m    378\u001b[0m           \u001b[0mflat_saveables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m           registered_savers).restore(self.save_path_tensor, self.options)\n\u001b[0m\u001b[1;32m    380\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_op\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_function_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m       \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/functional_saver.py\u001b[0m in \u001b[0;36mrestore_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m                   restored_tensors[trackable_utils.extract_local_name(\n\u001b[1;32m    466\u001b[0m                       ckpt_key)] = tensor\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestored_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m                   \u001b[0mrestore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36m_restore_from_tensors\u001b[0;34m(self, restored_tensors)\u001b[0m\n\u001b[1;32m    745\u001b[0m                        f\"\\n\\tGot: {list(restored_tensors.keys())}\")\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msaveable_object_to_restore_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestored_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36m_restore_from_tensors\u001b[0;34m(restored_tensors)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0msaveable_restored_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice_spec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m       restore_ops[saveable.name] = saveable.restore(\n\u001b[0m\u001b[1;32m    785\u001b[0m           saveable_restored_tensors, restored_shapes=None)\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    600\u001b[0m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_with_mapped_captures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrestored_tensor_dict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestored_tensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_restore_from_tensors\u001b[0;34m(self, restored_tensors)\u001b[0m\n\u001b[1;32m    783\u001b[0m             self.handle, self.shape, restored_tensor)\n\u001b[1;32m    784\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    786\u001b[0m             \u001b[0;34mf\"Received incompatible tensor with shape {restored_tensor.shape} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0;34mf\"when attempting to restore variable with shape {self.shape} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Received incompatible tensor with shape (6,) when attempting to restore variable with shape (5,) and name dense_flipout_1/bias_posterior_loc:0."
          ]
        }
      ],
      "source": [
        "model_5 = get_pre_trained_model('ds_ee2_1k_equalweight_randoms_kmax_5', 5, (100, 4) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WV6117ZOqdF"
      },
      "source": [
        "## Loading a trained network and applying it to a single file\n",
        "\n",
        "Template for loading spectra. Suppose you saved the spectrum in\n",
        "'data/example_spectra/my_ex.txt' . The spectrum is in 4 redshift bins and with 100 points between 0.01 - 2.5 in k . We normalize by the reference planck spectrum and add gaussian noise, then pass it to the five-label network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I0W-Yd0YERw"
      },
      "source": [
        "Load normalization data first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FirJ3GM5YDgl"
      },
      "outputs": [],
      "source": [
        "sample_pace=4 # the planck data were generated with 500 points up to k=10 . We need 1 every 4 points, and to cut to k_max=2.5\n",
        "i_max=100\n",
        "\n",
        "planck = np.loadtxt('data/normalisation/planck_ee2.txt')[:, 1:][::sample_pace][:i_max]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S0HfNLeLsiv"
      },
      "outputs": [],
      "source": [
        "k, X_ds = load_X('data/ds_ee2_test_1k/ds/976.txt',\n",
        "                 sample_pace=sample_pace, i_max=100,\n",
        "                 norm_data=planck,\n",
        "                 add_noise=False, normalise=True, use_sample_pace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_01rSvQTjrT"
      },
      "source": [
        "Let's look at the input features as seen by the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhtjvSiCTiux"
      },
      "outputs": [],
      "source": [
        "plt.plot(k, X_ds);\n",
        "plt.xscale('log');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZgUlHtZrvG7"
      },
      "source": [
        "## Classify your spectra and compute confidence\n",
        "### (for Bayesian networks only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKNL6te3ruT0"
      },
      "outputs": [],
      "source": [
        "res_ds = get_all_probas(X_ds,\n",
        "                        model_5,\n",
        "                        n_classes=5,\n",
        "                        num_monte_carlo=5,\n",
        "                        th_prob=0.5,\n",
        "                        n_samples=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVXx5ejLsG-i"
      },
      "outputs": [],
      "source": [
        "print('P-non-LCDM=%s' %(np.delete(res_ds['P'], [2]).sum()))\n",
        "plot_hist_1D(res_ds['samples'],\n",
        "             res_ds['MM'],\n",
        "             th_prob=0.5,\n",
        "             P=res_ds['P'],\n",
        "             inv_labels_dict=inv_labels_dict_5)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}